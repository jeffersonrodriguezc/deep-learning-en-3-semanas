<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.29">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Explicando la Mejora de los Transformers sobre las RNNs – Deep Learning Practico en 3 Semanas</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-0815c480559380816a4d1ea211a47e91.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-748b535e376f14d4692bf2b2e5fd6380.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-0815c480559380816a4d1ea211a47e91.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-dfb324f25d9b1687192fa8be62ac8f9c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-08799928d293638fe9563d5c5a2794e5.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-dfb324f25d9b1687192fa8be62ac8f9c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Deep Learning Practico en 3 Semanas</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Inicio</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../semana_1/index.html"> 
<span class="menu-text">Semana 1</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../semana_2/index.html"> 
<span class="menu-text">Semana 2</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../semana_3/index.html"> 
<span class="menu-text">Semana 3</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/jeffersonrodriguezc/deep-learning-en-3-semanas/"> <i class="bi bi-github" role="img" aria-label="GitHub">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#comparativa-rnns-vs-transformers" id="toc-comparativa-rnns-vs-transformers" class="nav-link active" data-scroll-target="#comparativa-rnns-vs-transformers">Comparativa RNNs vs Transformers</a></li>
  <li><a href="#codificación-y-decodificación-del-transformer-en-traducción" id="toc-codificación-y-decodificación-del-transformer-en-traducción" class="nav-link" data-scroll-target="#codificación-y-decodificación-del-transformer-en-traducción">Codificación y decodificación del Transformer en traducción</a></li>
  <li><a href="#esquema-de-trabajo" id="toc-esquema-de-trabajo" class="nav-link" data-scroll-target="#esquema-de-trabajo">Esquema de trabajo</a>
  <ul class="collapse">
  <li><a href="#carga-y-preprocesamiento-de-datos" id="toc-carga-y-preprocesamiento-de-datos" class="nav-link" data-scroll-target="#carga-y-preprocesamiento-de-datos">Carga y Preprocesamiento de Datos</a></li>
  <li><a href="#definir-los-componentes-del-transformer" id="toc-definir-los-componentes-del-transformer" class="nav-link" data-scroll-target="#definir-los-componentes-del-transformer">Definir los componentes del Transformer</a></li>
  </ul></li>
  <li><a href="#el-transformer" id="toc-el-transformer" class="nav-link" data-scroll-target="#el-transformer">El Transformer</a>
  <ul class="collapse">
  <li><a href="#hiperparámetros" id="toc-hiperparámetros" class="nav-link" data-scroll-target="#hiperparámetros">Hiperparámetros</a></li>
  <li><a href="#probemos-el-transformer" id="toc-probemos-el-transformer" class="nav-link" data-scroll-target="#probemos-el-transformer">Probemos el transformer</a></li>
  </ul></li>
  <li><a href="#training" id="toc-training" class="nav-link" data-scroll-target="#training">Training</a>
  <ul class="collapse">
  <li><a href="#configurar-el-optimizador" id="toc-configurar-el-optimizador" class="nav-link" data-scroll-target="#configurar-el-optimizador">Configurar el optimizador</a></li>
  <li><a href="#explicación-esquema-de-learning-rate-del-transformer" id="toc-explicación-esquema-de-learning-rate-del-transformer" class="nav-link" data-scroll-target="#explicación-esquema-de-learning-rate-del-transformer">Explicación esquema de Learning Rate del Transformer</a></li>
  <li><a href="#ajustar-función-de-pérdida-y-métricas" id="toc-ajustar-función-de-pérdida-y-métricas" class="nav-link" data-scroll-target="#ajustar-función-de-pérdida-y-métricas">Ajustar función de pérdida y métricas</a></li>
  <li><a href="#entrenar-el-modelo" id="toc-entrenar-el-modelo" class="nav-link" data-scroll-target="#entrenar-el-modelo">Entrenar el modelo</a></li>
  </ul></li>
  <li><a href="#ejecutar-inferencia" id="toc-ejecutar-inferencia" class="nav-link" data-scroll-target="#ejecutar-inferencia">Ejecutar inferencia</a></li>
  <li><a href="#crear-los-plots-de-atención" id="toc-crear-los-plots-de-atención" class="nav-link" data-scroll-target="#crear-los-plots-de-atención">Crear los plots de atención</a></li>
  <li><a href="#exportar-el-modelo" id="toc-exportar-el-modelo" class="nav-link" data-scroll-target="#exportar-el-modelo">Exportar el modelo</a></li>
  <li><a href="#conclusión" id="toc-conclusión" class="nav-link" data-scroll-target="#conclusión">Conclusión</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/jeffersonrodriguezc/deep-learning-en-3-semanas/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Explicando la Mejora de los Transformers sobre las RNNs</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p>Última actualización 07/05/2025</p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://colab.research.google.com/github/jeffersonrodriguezc/deep-learning-en-3-semanas/blob/main/semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" class="img-fluid figure-img"></a></p>
<figcaption>Open In Colab</figcaption>
</figure>
</div>
<div id="cell-3" class="cell" data-cellview="form" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#@title Importar librerías</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co">#importar librerías necesarias</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow_datasets <span class="im">as</span> tfds</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow_text</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">'ignore'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-4" class="cell" data-cellview="form" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">#@title Funciones complementarias</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_similaridad_positional_encodings(pos_encoding):</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># normalización de los vectores a 1</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>  pos_encoding<span class="op">/=</span>tf.norm(pos_encoding, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># seleccionamos el vector de la posición 1000</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>  p <span class="op">=</span> pos_encoding[<span class="dv">1000</span>]</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># cálculo de la similitud del producto punto</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>  dots <span class="op">=</span> tf.einsum(<span class="st">'pd,d -&gt; p'</span>, pos_encoding, p)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># visualización de la relación de los vectores con sus palabras</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># vecinas, por definición tendran mucha similaridad.</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>  plt.subplot(<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>  plt.plot(dots)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>  plt.ylim([<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>  plt.plot([<span class="dv">950</span>, <span class="dv">950</span>, <span class="bu">float</span>(<span class="st">'nan'</span>), <span class="dv">1050</span>, <span class="dv">1050</span>],</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>          [<span class="dv">0</span>,<span class="dv">1</span>,<span class="bu">float</span>(<span class="st">'nan'</span>),<span class="dv">0</span>,<span class="dv">1</span>], color<span class="op">=</span><span class="st">'k'</span>, label<span class="op">=</span><span class="st">'Zoom'</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>  plt.legend()</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>  plt.subplot(<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>  plt.plot(dots)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>  plt.xlim([<span class="dv">950</span>, <span class="dv">1050</span>])</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>  plt.ylim([<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_distribucion_longitudes_tokens(all_lengths):</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>  plt.hist(all_lengths, np.linspace(<span class="dv">0</span>, <span class="dv">500</span>, <span class="dv">101</span>))</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>  plt.ylim(plt.ylim())</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>  max_length <span class="op">=</span> <span class="bu">max</span>(all_lengths)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>  plt.plot([max_length, max_length], plt.ylim())</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>  plt.title(<span class="ss">f'Número máximo de tokens por muestra: </span><span class="sc">{</span>max_length<span class="sc">}</span><span class="ss">'</span>)<span class="op">;</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_positional_encodings(pos_encoding):</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Gráficar las dimensiones</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>  plt.pcolormesh(pos_encoding.numpy().T, cmap<span class="op">=</span><span class="st">'RdBu'</span>)</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>  plt.ylabel(<span class="st">'Profundidad'</span>)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>  plt.xlabel(<span class="st">'Posición'</span>)</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>  plt.colorbar()</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>  plt.show()</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_lr_planificador(learning_rate):</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>  plt.plot(learning_rate(tf.<span class="bu">range</span>(<span class="dv">40000</span>, dtype<span class="op">=</span>tf.float32)))</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>  plt.ylabel(<span class="st">'Learning Rate'</span>)<span class="op">;</span></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>  plt.xlabel(<span class="st">'Paso de entrenamiento'</span>)<span class="op">;</span></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_attention_head(in_tokens, translated_tokens, attention):</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Saltar el token start.</span></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>  translated_tokens <span class="op">=</span> translated_tokens[<span class="dv">1</span>:]</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>  ax <span class="op">=</span> plt.gca()</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>  ax.matshow(attention)</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>  ax.set_xticks(<span class="bu">range</span>(<span class="bu">len</span>(in_tokens)))</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>  ax.set_yticks(<span class="bu">range</span>(<span class="bu">len</span>(translated_tokens)))</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>  labels <span class="op">=</span> [label.decode(<span class="st">'utf-8'</span>) <span class="cf">for</span> label <span class="kw">in</span> in_tokens.numpy()]</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>  ax.set_xticklabels(</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>      labels, rotation<span class="op">=</span><span class="dv">90</span>)</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>  labels <span class="op">=</span> [label.decode(<span class="st">'utf-8'</span>) <span class="cf">for</span> label <span class="kw">in</span> translated_tokens.numpy()]</span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>  ax.set_yticklabels(labels)</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_attention_weights(sentence, translated_tokens, attention_heads):</span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a>  in_tokens <span class="op">=</span> tf.convert_to_tensor([sentence])</span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>  in_tokens <span class="op">=</span> tokenizers.pt.tokenize(in_tokens).to_tensor()</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a>  in_tokens <span class="op">=</span> tokenizers.pt.lookup(in_tokens)[<span class="dv">0</span>]</span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a>  fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">15</span>))</span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> h, head <span class="kw">in</span> <span class="bu">enumerate</span>(attention_heads):</span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> fig.add_subplot(<span class="dv">2</span>, <span class="dv">4</span>, h<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a>    plot_attention_head(in_tokens, translated_tokens, head)</span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="ss">f'Head </span><span class="sc">{</span>h<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a>  plt.tight_layout()</span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a>  plt.show()</span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_traduccion(sentence, tokens, ground_truth):</span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span><span class="st">"Input:"</span><span class="sc">:15s}</span><span class="ss">: </span><span class="sc">{</span>sentence<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span><span class="st">"Predicción"</span><span class="sc">:15s}</span><span class="ss">: </span><span class="sc">{</span>tokens<span class="sc">.</span>numpy()<span class="sc">.</span>decode(<span class="st">"utf-8"</span>)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span><span class="st">"Ground truth"</span><span class="sc">:15s}</span><span class="ss">: </span><span class="sc">{</span>ground_truth<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Los Transformers: Una Nueva Era en el Procesamiento de Secuencias</strong></p>
<p>Las Redes Neuronales Recurrentes (RNNs) han sido una herramienta fundamental para el procesamiento de datos secuenciales, como el lenguaje natural. Sin embargo, los Transformers, propuestos en el artículo <a href="https://arxiv.org/abs/1706.03762">“Attention is all you need”</a>, han revolucionado este campo, ofreciendo ventajas significativas sobre las RNNs. <strong>Los Transformers son redes neuronales profundas que reemplazan las CNNs y las RNNs. Estos introducen la auto-atención (self-attention) permite a los Transformers transmitir información fácilmente a través de las secuencias de entrada.</strong></p>
<p><strong>¿Por qué los Transformers son importantes?</strong></p>
<ul>
<li><strong>Paralelización:</strong> A diferencia de las RNNs, los Transformers pueden procesar todos los elementos de una secuencia en paralelo, lo que permite un entrenamiento mucho más rápido en hardware moderno como GPUs y TPUs.</li>
<li><strong>Captura de dependencias temporales de larga duración:</strong> Los Transformers utilizan mecanismos de atención que permiten a cada posición de la secuencia prestar atención a todas las demás posiciones, facilitando el aprendizaje de relaciones entre elementos distantes. Las RNNs tienen dificultades para esto debido a la necesidad de procesar la secuencia paso a paso.</li>
<li><strong>Flexibilidad:</strong> Los Transformers no hacen suposiciones sobre las relaciones temporales o espaciales en los datos, lo que los hace aplicables a una variedad de tareas más allá del lenguaje, como el procesamiento de imágenes o el análisis de juegos.</li>
</ul>
<p>En este notebook, exploraremos en profundidad los componentes clave de los Transformers y compararemos su funcionamiento con el de las RNNs, destacando las razones de su superioridad.</p>
<section id="comparativa-rnns-vs-transformers" class="level3">
<h3 class="anchored" data-anchor-id="comparativa-rnns-vs-transformers">Comparativa RNNs vs Transformers</h3>
<table>
<tbody><tr>
<th>
<a href="https://www.tensorflow.org/text/tutorials/nmt_with_attention">RNN+Modelo de Atención</a>
</th>
<th>
Transformer de 1 capa
</th>
</tr>
<tr>
<td>
<img width="411" src="https://www.tensorflow.org/images/tutorials/transformer/RNN+attention-words.png">
</td>
<td>
<img width="400" src="https://www.tensorflow.org/images/tutorials/transformer/Transformer-1layer-words.png">
</td>
</tr>
</tbody></table>
</section>
<section id="codificación-y-decodificación-del-transformer-en-traducción" class="level3">
<h3 class="anchored" data-anchor-id="codificación-y-decodificación-del-transformer-en-traducción">Codificación y decodificación del Transformer en traducción</h3>
<p><img src="https://www.tensorflow.org/images/tutorials/transformer/apply_the_transformer_to_machine_translation.gif" alt="Funcionamiento transformers"></p>
<p>Funcionamiento de los Transformers en la traducción de texto. Fuente: <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">Blog Google AI</a>.</p>
</section>
<section id="esquema-de-trabajo" class="level2">
<h2 class="anchored" data-anchor-id="esquema-de-trabajo">Esquema de trabajo</h2>
<p>Con lo anterior en mente, y una vez visto un poco las diferencias entre RNNs y Transformers, vamos a abordar los siguientes contenidos de manera detallada:</p>
<ol type="1">
<li>Prepararás los datos.</li>
<li>Implementarás los componentes necesarios:
<ul>
<li>Embeddings posicionales.</li>
<li>Capas de atención.</li>
<li>El codificador y el decodificador.</li>
</ul></li>
<li>Construirás y entrenarás el Transformer.</li>
<li>Generarás traducciones.</li>
<li>Exportarás el modelo.</li>
</ol>
<section id="carga-y-preprocesamiento-de-datos" class="level3">
<h3 class="anchored" data-anchor-id="carga-y-preprocesamiento-de-datos">Carga y Preprocesamiento de Datos</h3>
<p>Cargaremos un conjunto de datos de traducción Portugués-Inglés y utilizaremos un tokenizador para preparar el texto para el modelo. Este conjunto de datos contiene aproximadamente 52.000 ejemplos de entrenamiento, 1.200 de validación y 1.800 de prueba.</p>
<div id="cell-10" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>examples, metadata <span class="op">=</span> tfds.load(<span class="st">'ted_hrlr_translate/pt_to_en'</span>,</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>                               with_info<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>                               as_supervised<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>train_examples, val_examples <span class="op">=</span> examples[<span class="st">'train'</span>], examples[<span class="st">'validation'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-11" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="c1f6baff-7041-48bf-addc-d7fa4da95c2e" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Mostrar algunos ejemplos de parejas de oraciones</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> pt_examples, en_examples <span class="kw">in</span> train_examples.batch(<span class="dv">3</span>).take(<span class="dv">1</span>):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">'-&gt; Ejemplos en portugués:'</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> pt <span class="kw">in</span> pt_examples.numpy():</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(pt.decode(<span class="st">'utf-8'</span>))</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>()</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">'-&gt; Traducción al inglés:'</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> en <span class="kw">in</span> en_examples.numpy():</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(en.decode(<span class="st">'utf-8'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>-&gt; Ejemplos en portugués:
e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .
mas e se estes fatores fossem ativos ?
mas eles não tinham a curiosidade de me testar .

-&gt; Traducción al inglés:
and when you improve searchability , you actually take away the one advantage of print , which is serendipity .
but what if it were active ?
but they did n't test for curiosity .</code></pre>
</div>
</div>
<section id="tokenizadores" class="level4">
<h4 class="anchored" data-anchor-id="tokenizadores">Tokenizadores</h4>
<p>En nuestro ejemplo vamos a usar los tokenizadores construidos en el tutorial <a href="https://www.tensorflow.org/text/guide/subwords_tokenizer">subword tokenizer</a>. Ese tutorial optimiza dos objetos <code>text.BertTokenizer</code> (uno para inglés, otro para portugués) para <strong>este conjunto de datos</strong> y los exporta en formato <code>saved_model</code> de TensorFlow.</p>
<div id="cell-13" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:36}}" data-outputid="91b9a5b2-c0e5-445b-bbac-51a9c23c646b" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">'ted_hrlr_translate_pt_en_converter'</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>tf.keras.utils.get_file(</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f'</span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss">.zip'</span>,</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f'https://storage.googleapis.com/download.tensorflow.org/models/</span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss">.zip'</span>,</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    cache_dir<span class="op">=</span><span class="st">'.'</span>, cache_subdir<span class="op">=</span><span class="st">''</span>, extract<span class="op">=</span><span class="va">True</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>'./ted_hrlr_translate_pt_en_converter_extracted'</code></pre>
</div>
</div>
<div id="cell-14" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># cargar los tokenizadores</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>tokenizers <span class="op">=</span> tf.saved_model.load(<span class="ss">f'</span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss">_extracted/</span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-15" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="0b6b462f-9167-458e-c681-e7ec4e21597a" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ambos tokenizadores tienen los mismo métodos</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>[item <span class="cf">for</span> item <span class="kw">in</span> <span class="bu">dir</span>(tokenizers.en) <span class="cf">if</span> <span class="kw">not</span> item.startswith(<span class="st">'_'</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>['detokenize',
 'get_reserved_tokens',
 'get_vocab_path',
 'get_vocab_size',
 'lookup',
 'tokenize',
 'tokenizer',
 'vocab']</code></pre>
</div>
</div>
<div id="cell-16" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="f4fb09f0-830a-4b0d-f7eb-6ca5834fc9ce" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># por ejemplo revisemos los vocab</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>tokenizers.en.vocab.shape, tokenizers.pt.vocab.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>(TensorShape([7010]), TensorShape([7765]))</code></pre>
</div>
</div>
<p>El método <code>tokenize</code> convierte un grupo de oraciones en un grupo de identificadores de tokens de una misma longitud (padding). Este método separa los signos de puntuación, las minúsculas y normaliza el texto de entrada antes de la tokenización. Esta normalización no es visible aquí porque los datos de entrada ya están normalizados.</p>
<div id="cell-18" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="69e8aa12-dcbd-4c08-98bb-7aad21f7f5b7" data-execution_count="9">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'-&gt; Esto es un grupo de cadenas:'</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> en <span class="kw">in</span> en_examples.numpy():</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(en.decode(<span class="st">'utf-8'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>-&gt; Esto es un grupo de cadenas:
and when you improve searchability , you actually take away the one advantage of print , which is serendipity .
but what if it were active ?
but they did n't test for curiosity .</code></pre>
</div>
</div>
<div id="cell-19" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="04184a3f-7407-4df8-91c9-b72ed66e56c2" data-execution_count="10">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>encoded <span class="op">=</span> tokenizers.en.tokenize(en_examples)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'-&gt; Este es el grupo de cedena de ID de tokens (con padding)'</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> row <span class="kw">in</span> encoded.to_list():</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(row)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>-&gt; Este es el grupo de cedena de ID de tokens (con padding)
[2, 72, 117, 79, 1259, 1491, 2362, 13, 79, 150, 184, 311, 71, 103, 2308, 74, 2679, 13, 148, 80, 55, 4840, 1434, 2423, 540, 15, 3]
[2, 87, 90, 107, 76, 129, 1852, 30, 3]
[2, 87, 83, 149, 50, 9, 56, 664, 85, 2512, 15, 3]</code></pre>
</div>
</div>
<p>El método <code>detokenize</code> convierte los ID de token de nuevo en texto legible normal:</p>
<div id="cell-21" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="5719e114-7bf4-471b-f7a7-2f7ac6b3e2e6" data-execution_count="11">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>round_trip <span class="op">=</span> tokenizers.en.detokenize(encoded)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'-&gt; Correspondiente texto:'</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> line <span class="kw">in</span> round_trip.numpy():</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(line.decode(<span class="st">'utf-8'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>-&gt; Correspondiente texto:
and when you improve searchability , you actually take away the one advantage of print , which is serendipity .
but what if it were active ?
but they did n ' t test for curiosity .</code></pre>
</div>
</div>
<p>El método <code>lookup</code> convierte de token-IDs a token-texto:</p>
<div id="cell-23" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="ea2499bb-b49f-42fe-94c4-d5698023c3a7" data-execution_count="12">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'-&gt; Este es el texto dividido en tokens:'</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> tokenizers.en.lookup(encoded)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>tokens</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>-&gt; Este es el texto dividido en tokens:</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>&lt;tf.RaggedTensor [[b'[START]', b'and', b'when', b'you', b'improve', b'search', b'##ability',
  b',', b'you', b'actually', b'take', b'away', b'the', b'one', b'advantage',
  b'of', b'print', b',', b'which', b'is', b's', b'##ere', b'##nd', b'##ip',
  b'##ity', b'.', b'[END]']                                                 ,
 [b'[START]', b'but', b'what', b'if', b'it', b'were', b'active', b'?',
  b'[END]']                                                           ,
 [b'[START]', b'but', b'they', b'did', b'n', b"'", b't', b'test', b'for',
  b'curiosity', b'.', b'[END]']                                          ]&gt;</code></pre>
</div>
</div>
<p>La salida demuestra el aspecto de <code>"subword"</code> de la tokenización de subpalabras.</p>
<p>Por ejemplo, la palabra <code>'searchability'</code> se descompone en <code>'search'</code> y <code>'##ability'</code>, y la palabra <code>'serendipity'</code> en <code>'s'</code>, <code>'##ere'</code>, <code>'##nd'</code>, <code>'##ip'</code> e <code>'##ity'</code>.</p>
<p>Ten en cuenta que el texto tokenizado incluye los tokens <code>'[START]'</code> y <code>'[END]'</code>.</p>
<p>La distribución de tokens por ejemplo en el conjunto de datos es la siguiente:</p>
<div id="cell-25" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="7e7a42b6-e2f1-4489-ca09-e1f9de91d2b4" data-execution_count="13">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>lengths <span class="op">=</span> []</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> pt_examples, en_examples <span class="kw">in</span> train_examples.batch(<span class="dv">1024</span>):</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>  pt_tokens <span class="op">=</span> tokenizers.pt.tokenize(pt_examples)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>  lengths.append(pt_tokens.row_lengths())</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>  en_tokens <span class="op">=</span> tokenizers.en.tokenize(en_examples)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>  lengths.append(en_tokens.row_lengths())</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">'.'</span>, end<span class="op">=</span><span class="st">''</span>, flush<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>...................................................</code></pre>
</div>
</div>
<div id="cell-26" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:453}}" data-outputid="938c846b-2729-4a39-fa2d-433a3ac26e68" data-execution_count="14">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>all_lengths <span class="op">=</span> np.concatenate(lengths)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>plot_distribucion_longitudes_tokens(all_lengths)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Nb_2c_Traduccion_usando_transformers_keras_files/figure-html/cell-15-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="configuración-del-pipeline-de-datos-usando-tf.data" class="level4">
<h4 class="anchored" data-anchor-id="configuración-del-pipeline-de-datos-usando-tf.data">Configuración del pipeline de datos usando <code>tf.data</code></h4>
<p>La siguiente función toma batches de texto como entrada y los convierte a un formato adecuado para el entrenamiento.</p>
<ol type="1">
<li>Los tokeniza en lotes de diferentes dimensiones (ragged).</li>
<li>Recorta cada uno para que no tenga más de <code>MAX_TOKENS</code>.</li>
<li>Divide los tokens objetivo (inglés) en entradas y etiquetas. Estos se desplazan un paso de modo que en cada ubicación de entrada, la <code>etiqueta</code> es el ID del siguiente token.</li>
<li>Convierte los <code>RaggedTensor</code> en <code>Tensor</code> densos con padding.</li>
<li>Devuelve un par <code>(entradas, etiquetas)</code>.</li>
</ol>
<div id="cell-29" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>MAX_TOKENS<span class="op">=</span><span class="dv">64</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> prepare_batch(pt, en):</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    pt <span class="op">=</span> tokenizers.pt.tokenize(pt) <span class="co"># la sálida tiene diferentes longitudes</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    pt <span class="op">=</span> pt[:, :MAX_TOKENS]    <span class="co"># Truncar al max # de tokens</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    pt <span class="op">=</span> pt.to_tensor()  <span class="co"># Convertir a un tensor denso sin padding</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>    en <span class="op">=</span> tokenizers.en.tokenize(en)</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>    en <span class="op">=</span> en[:, :(MAX_TOKENS<span class="op">+</span><span class="dv">1</span>)] <span class="co"># para poder hacer el shift</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>    en_inputs <span class="op">=</span> en[:, :<span class="op">-</span><span class="dv">1</span>].to_tensor()  <span class="co"># Elimina los tokens [</span><span class="re">END</span><span class="co">]</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>    en_labels <span class="op">=</span> en[:, <span class="dv">1</span>:].to_tensor()   <span class="co"># Eliminar los tokens [START]</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (pt, en_inputs), en_labels</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>La siguiente función convierte un conjunto de datos de ejemplos de texto en datos de lotes para el entrenamiento.</p>
<ol type="1">
<li>Tokeniza el texto y filtra las secuencias que son demasiado largas.</li>
<li>El método <code>cache</code> asegura que ese trabajo solo se ejecute una vez.</li>
<li>Luego, <code>shuffle</code> y preparar el batch.</li>
<li>Finalmente, <code>prefetch</code> ejecuta el conjunto de datos en paralelo con el modelo para asegurar que los datos estén disponibles cuando se necesiten. Consulta <a href="https://www.tensorflow.org/guide/data_performance.ipynb">Mejorar rendimiento con <code>tf.data</code></a> para más detalles.</li>
</ol>
<div id="cell-31" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>BUFFER_SIZE <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">64</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-32" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_batches(ds):</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> (</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>      ds</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>      .shuffle(BUFFER_SIZE)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>      .batch(BATCH_SIZE)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>      .<span class="bu">map</span>(prepare_batch, tf.data.AUTOTUNE)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>      .prefetch(buffer_size<span class="op">=</span>tf.data.AUTOTUNE))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="probar-nuestro-pipeline-de-datos" class="level4">
<h4 class="anchored" data-anchor-id="probar-nuestro-pipeline-de-datos">Probar nuestro pipeline de datos</h4>
<div id="cell-34" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Crear los conjuntos de entrenamiento y validación</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>train_batches <span class="op">=</span> make_batches(train_examples)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>val_batches <span class="op">=</span> make_batches(val_examples)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Como se verían las entradas y sálidas en nuestro pipeline de datos?</p>
<table>
<tbody><tr>
<th>
Inputs en la parte inferior, labels en la parte superior.
</th>
</tr>
<tr>
<td>
<img width="400" src="https://www.tensorflow.org/images/tutorials/transformer/Transformer-1layer-words.png">
</td>
</tr>
</tbody></table>
<p>Esta configuración se llama “<strong>teacher forcing</strong>” porque, independientemente de la salida del modelo en cada paso de tiempo, recibe el valor verdadero como entrada para el siguiente paso de tiempo. Esta es una forma simple y eficiente de entrenar un modelo de generación de texto. Es eficiente porque no necesitas ejecutar el modelo secuencialmente; las salidas en las diferentes ubicaciones de la secuencia se pueden calcular en paralelo.</p>
<div id="cell-38" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="1b645a21-aadf-4167-dec5-b70f7a9ff417" data-execution_count="19">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># visualizar un ejemplo de nuestros datos para entrenar</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (pt, en), en_labels <span class="kw">in</span> train_batches.take(<span class="dv">1</span>):</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">break</span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pt.shape)</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(en.shape)</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(en_labels.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(64, 64)
(64, 64)
(64, 64)</code></pre>
</div>
</div>
<p>Las etiquetas <code>en</code> y <code>en_labels</code> son las mismas, sólo que desplazadas en 1:</p>
<div id="cell-40" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="da38b3e1-7f43-442e-8c89-e55c8e33477c" data-execution_count="20">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(en[<span class="dv">0</span>][:<span class="dv">10</span>])</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(en_labels[<span class="dv">0</span>][:<span class="dv">10</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tf.Tensor([ 2 90 80 81 85 30  0  0  0  0], shape=(10,), dtype=int64)
tf.Tensor([90 80 81 85 30  3  0  0  0  0], shape=(10,), dtype=int64)</code></pre>
</div>
</div>
</section>
</section>
<section id="definir-los-componentes-del-transformer" class="level3">
<h3 class="anchored" data-anchor-id="definir-los-componentes-del-transformer">Definir los componentes del Transformer</h3>
<p>Dentro de un Transformer pasan muchas cosas. Las cosas importantes que hay que recordar son:</p>
<ul>
<li>Sigue el mismo patrón general que un modelo estándar secuencia-a-secuencia con un codificador y un decodificador.</li>
<li>Si trabajas paso a paso, todo tendrá sentido.</li>
</ul>
<table>
<tbody><tr>
<th colspan="1">
Diagrama original del Transformer
</th>
<th colspan="1">
Representación de un Transformer de 4 capas
</th>
</tr>
<tr>
<td>
<img width="400" src="https://www.tensorflow.org/images/tutorials/transformer/transformer.png">
</td>
<td>
<img width="307" src="https://www.tensorflow.org/images/tutorials/transformer/Transformer-4layer-compact.png">
</td>
</tr>
</tbody></table>
<section id="la-capa-para-la-codificación-de-la-posición" class="level4">
<h4 class="anchored" data-anchor-id="la-capa-para-la-codificación-de-la-posición">La capa para la codificación de la posición</h4>
<p>Las entradas tanto del codificador como del descodificador utilizan la misma lógica de incrustación y codificación posicional</p>
<table>
<tbody><tr>
<th colspan="1">
The embedding and positional encoding layer
</th>
</tr><tr>
</tr><tr>
<td>
<img src="https://www.tensorflow.org/images/tutorials/transformer/PositionalEmbedding.png">
</td>
</tr>
</tbody></table>
<p>Dada una secuencia de tokens, tanto los tokens de entrada (portugués) como los tokens objetivo (inglés) deben convertirse en vectores utilizando una capa <code>tf.keras.layers.Embedding</code>.</p>
<p>Las capas de atención utilizadas en todo el modelo ven su entrada como un conjunto de vectores, sin ningún orden. Dado que el modelo no contiene ninguna capa recurrente o convolucional, necesita alguna forma de identificar el orden de las palabras; de lo contrario, vería la secuencia de entrada como una instancia de <a href="https://developers.google.com/machine-learning/glossary#bag-of-words">bolsa de palabras</a>, <code>cómo estás</code>, <code>cómo tú estás</code>, <code>tú cómo estás</code>, y así sucesivamente, son indistinguibles.</p>
<p>Por lo tanto, el Transformer agrega una “Codificación Posicional” a los vectores de embedding. Utiliza un conjunto de senos y cosenos en diferentes frecuencias (a lo largo de la secuencia). Por definición, los elementos cercanos tendrán codificaciones posicionales similares.</p>
<p>El paper original usa la siguiente formúla para la codificación posicional:</p>
<p><span class="math display">\[\Large{PE_{(pos, 2i)} = \sin(pos / 10000^{2i / d_{model}})} \]</span> <span class="math display">\[\Large{PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i / d_{model}})} \]</span></p>
<p>Nota: El código a continuación lo implementa, pero en lugar de intercalar los senos y cosenos, los vectores de senos y cosenos simplemente se concatenan. Permutar los canales de esta manera es funcionalmente equivalente, y un poco más fácil de implementar y mostrar en las gráficas siguientes.</p>
<p><strong>Explicación Paso a Paso:</strong> Imaginemos que tenemos una frase: “<strong>El gato está en la alfombra</strong>”.</p>
<p>Y supongamos que d_model = 4. Esto significa que cada posición (cada palabra) se representará con un vector de 4 números.</p>
<ol type="1">
<li>Posiciones:</li>
</ol>
<ul>
<li>“El” está en la posición 0.</li>
<li>“gato” está en la posición 1.</li>
<li>“está” está en la posición 2.</li>
<li>“en” está en la posición 3.</li>
<li>“la” está en la posición 4.</li>
<li>“alfombra” está en la posición 5.</li>
</ul>
<ol start="2" type="1">
<li>Dimensiones:</li>
</ol>
<p>Como d_model = 4, tendremos dimensiones 0, 1, 2, y 3 en nuestro vector de codificación posicional.</p>
<ol start="3" type="1">
<li>Aplicando la Fórmula (Ejemplo: la palabra “gato” en la posición 1):</li>
</ol>
<ul>
<li>Para la dimensión 0 (2i = 0, entonces i = 0):</li>
</ul>
<pre><code>PE(1, 0) = sin(1 / 10000^(2*0 / 4)) = sin(1 / 10000^0) = sin(1 / 1) = sin(1) ≈ 0.841</code></pre>
<ul>
<li>Para la dimensión 1 (2i + 1 = 1, entonces i = 0):</li>
</ul>
<pre><code>PE(1, 1) = cos(1 / 10000^(2*0 / 4)) = cos(1 / 10000^0) = cos(1 / 1) = cos(1) ≈ 0.540</code></pre>
<ul>
<li>Para la dimensión 2 (2i = 2, entonces i = 1):</li>
</ul>
<pre><code>PE(1, 2) = sin(1 / 10000^(2*1 / 4)) = sin(1 / 10000^(1/2)) = sin(1 / 100) = sin(0.01) ≈ 0.01</code></pre>
<ul>
<li>Para la dimensión 3 (2i + 1 = 3, entonces i = 1):</li>
</ul>
<pre><code>PE(1, 3) = cos(1 / 10000^(2*1 / 4)) = cos(1 / 10000^(1/2)) = cos(1 / 100) = cos(0.01) ≈ 0.99995</code></pre>
<p>Entonces, la codificación posicional para la palabra “gato” (posición 1) sería aproximadamente el vector: [0.841, 0.540, 0.01, 0.99995].</p>
<div id="cell-49" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> positional_encoding(length, depth):</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>  depth <span class="op">=</span> depth<span class="op">/</span><span class="dv">2</span> <span class="co"># mitad de las dimensiones para cada función</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>  positions <span class="op">=</span> np.arange(length)[:, np.newaxis]     <span class="co"># shape = (seq, 1)</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>  depths <span class="op">=</span> np.arange(depth)[np.newaxis, :]<span class="op">/</span>depth   <span class="co"># shape = (1, depth)</span></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>  angle_rates <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">10000</span><span class="op">**</span>depths)         <span class="co"># (1, depth)</span></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>  angle_rads <span class="op">=</span> positions <span class="op">*</span> angle_rates      <span class="co"># (pos, depth)</span></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>  pos_encoding <span class="op">=</span> np.concatenate(</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>      [np.sin(angle_rads), np.cos(angle_rads)],</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>      axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> tf.cast(pos_encoding, dtype<span class="op">=</span>tf.float32)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>La función de codificación de posición es una pila de senos y cosenos que vibran a distintas frecuencias según su ubicación a lo largo de la profundidad del vector de incrustación. Vibran a través del eje de posición.</p>
<div id="cell-51" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="7ddc76d9-1585-4a8c-da6d-0c270edd72e5" data-execution_count="22">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># para el ejemplo de nuestra frase: El gato esta en la alfombra</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="co"># en este caso se concatenaron las funciones</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>positional_encoding(length<span class="op">=</span><span class="dv">6</span>, depth<span class="op">=</span><span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>&lt;tf.Tensor: shape=(6, 4), dtype=float32, numpy=
array([[ 0.        ,  0.        ,  1.        ,  1.        ],
       [ 0.84147096,  0.00999983,  0.5403023 ,  0.99995   ],
       [ 0.9092974 ,  0.01999867, -0.41614684,  0.9998    ],
       [ 0.14112   ,  0.0299955 , -0.9899925 ,  0.99955004],
       [-0.7568025 ,  0.03998933, -0.6536436 ,  0.9992001 ],
       [-0.9589243 ,  0.04997917,  0.2836622 ,  0.99875027]],
      dtype=float32)&gt;</code></pre>
</div>
</div>
<div id="cell-52" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:472}}" data-outputid="d7a7faee-b156-4fad-af17-41e7cfa76045" data-execution_count="23">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>pos_encoding <span class="op">=</span> positional_encoding(length<span class="op">=</span><span class="dv">2048</span>, depth<span class="op">=</span><span class="dv">512</span>)</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Revisar la dimensión</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pos_encoding.shape)</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>plot_positional_encodings(pos_encoding)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(2048, 512)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Nb_2c_Traduccion_usando_transformers_keras_files/figure-html/cell-24-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Por definición, estos vectores se alinean bien con los vectores cercanos a lo largo del eje de posición. A continuación, los vectores de codificación de posición se normalizan y el vector de la posición <code>1000</code> se compara, por producto punto, con todos los demás:</p>
<div id="cell-54" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:435}}" data-outputid="5a4e4e72-104c-4070-b4a8-aa3631e08fe9" data-execution_count="24">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>plot_similaridad_positional_encodings(pos_encoding)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Nb_2c_Traduccion_usando_transformers_keras_files/figure-html/cell-25-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Ahora creemos la capa: <code>PositionEmbedding</code></p>
<div id="cell-56" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PositionalEmbedding(tf.keras.layers.Layer):</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, d_model):</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.d_model <span class="op">=</span> d_model</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.embedding <span class="op">=</span> tf.keras.layers.Embedding(vocab_size, d_model, mask_zero<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.pos_encoding <span class="op">=</span> positional_encoding(length<span class="op">=</span><span class="dv">2048</span>, depth<span class="op">=</span>d_model)</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> compute_mask(<span class="va">self</span>, <span class="op">*</span>args, <span class="op">**</span>kwargs):</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">self</span>.embedding.compute_mask(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> call(<span class="va">self</span>, x):</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>    length <span class="op">=</span> tf.shape(x)[<span class="dv">1</span>]</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.embedding(x)</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Este factor establece la escala relativa de la incrustación y la codificación_positonal.</span></span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Es decir asegurar que tengan escalas similares</span></span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a>    x <span class="op">*=</span> tf.math.sqrt(tf.cast(<span class="va">self</span>.d_model, tf.float32))</span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Se suma las posiciones a los embeddings de los tokens</span></span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.pos_encoding[tf.newaxis, :length, :]</span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-57" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>embed_pt <span class="op">=</span> PositionalEmbedding(vocab_size<span class="op">=</span>tokenizers.pt.get_vocab_size().numpy(), d_model<span class="op">=</span><span class="dv">512</span>)</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>embed_en <span class="op">=</span> PositionalEmbedding(vocab_size<span class="op">=</span>tokenizers.en.get_vocab_size().numpy(), d_model<span class="op">=</span><span class="dv">512</span>)</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>pt_emb <span class="op">=</span> embed_pt(pt)</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>en_emb <span class="op">=</span> embed_en(en)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-58" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="fc44d343-b3ca-4578-d343-962d14fe2bfb" data-execution_count="27">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># la máscara de cada oración, recordar que las oraciones no tiene la misma</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="co"># longitud, asi que se aplica la máscara</span></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>en_emb._keras_mask</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>&lt;tf.Tensor: shape=(64, 64), dtype=bool, numpy=
array([[ True,  True,  True, ..., False, False, False],
       [ True,  True,  True, ..., False, False, False],
       [ True,  True,  True, ..., False, False, False],
       ...,
       [ True,  True,  True, ..., False, False, False],
       [ True,  True,  True, ..., False, False, False],
       [ True,  True,  True, ..., False, False, False]])&gt;</code></pre>
</div>
</div>
<div id="cell-59" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="c17f885f-f5b1-4542-d187-09b3c71b4712" data-execution_count="28">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>pt_emb</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>&lt;tf.Tensor: shape=(64, 64, 512), dtype=float32, numpy=
array([[[ 0.81105447, -0.7717942 ,  0.5229695 , ...,  0.02146667,
          0.24027777,  0.26449662],
        [ 1.8670975 ,  1.6279981 ,  0.97901094, ...,  1.3392618 ,
          0.8188071 ,  0.21744752],
        [-0.11729413,  0.5008898 ,  0.32620972, ...,  0.27159345,
          1.1697826 ,  1.4658518 ],
        ...,
        [-1.3773707 ,  0.633837  ,  1.1719698 , ...,  1.6278234 ,
          0.05931401,  0.90110606],
        [-1.1504335 , -0.23321328,  1.8688756 , ...,  1.6278226 ,
          0.0593133 ,  0.90110534],
        [-0.24389721, -0.9982976 ,  1.8318357 , ...,  1.6278219 ,
          0.05931258,  0.9011047 ]],

       [[ 0.81105447, -0.7717942 ,  0.5229695 , ...,  0.02146667,
          0.24027777,  0.26449662],
        [-0.03075731,  0.79514253,  1.7891788 , ...,  0.0534128 ,
          0.01318026,  0.86922586],
        [ 1.0960544 ,  0.75714344,  0.7072108 , ...,  0.3786983 ,
          1.0271425 ,  1.3102653 ],
        ...,
        [-1.3773707 ,  0.633837  ,  1.1719698 , ...,  1.6278234 ,
          0.05931401,  0.90110606],
        [-1.1504335 , -0.23321328,  1.8688756 , ...,  1.6278226 ,
          0.0593133 ,  0.90110534],
        [-0.24389721, -0.9982976 ,  1.8318357 , ...,  1.6278219 ,
          0.05931258,  0.9011047 ]],

       [[ 0.81105447, -0.7717942 ,  0.5229695 , ...,  0.02146667,
          0.24027777,  0.26449662],
        [ 1.7584805 ,  1.4909694 , -0.1760881 , ...,  2.1180818 ,
          0.4953122 ,  0.23008263],
        [ 0.55767405,  1.4934946 ,  0.02770001, ...,  0.84077555,
          0.9217277 ,  1.9566159 ],
        ...,
        [-1.3773707 ,  0.633837  ,  1.1719698 , ...,  1.6278234 ,
          0.05931401,  0.90110606],
        [-1.1504335 , -0.23321328,  1.8688756 , ...,  1.6278226 ,
          0.0593133 ,  0.90110534],
        [-0.24389721, -0.9982976 ,  1.8318357 , ...,  1.6278219 ,
          0.05931258,  0.9011047 ]],

       ...,

       [[ 0.81105447, -0.7717942 ,  0.5229695 , ...,  0.02146667,
          0.24027777,  0.26449662],
        [ 0.7150625 ,  1.6491615 ,  1.213108  , ...,  1.0735046 ,
          0.3283956 ,  0.01745564],
        [ 1.1696244 ,  1.2925339 ,  1.6480486 , ...,  0.24500364,
          1.4749924 ,  1.9941585 ],
        ...,
        [-1.3773707 ,  0.633837  ,  1.1719698 , ...,  1.6278234 ,
          0.05931401,  0.90110606],
        [-1.1504335 , -0.23321328,  1.8688756 , ...,  1.6278226 ,
          0.0593133 ,  0.90110534],
        [-0.24389721, -0.9982976 ,  1.8318357 , ...,  1.6278219 ,
          0.05931258,  0.9011047 ]],

       [[ 0.81105447, -0.7717942 ,  0.5229695 , ...,  0.02146667,
          0.24027777,  0.26449662],
        [-0.23110986,  0.5054298 , -0.25015187, ...,  1.1486163 ,
          1.1958522 ,  1.1460018 ],
        [ 1.4417007 , -0.06407106,  0.7872464 , ...,  0.72261333,
          0.1937148 ,  2.0049632 ],
        ...,
        [-1.3773707 ,  0.633837  ,  1.1719698 , ...,  1.6278234 ,
          0.05931401,  0.90110606],
        [-1.1504335 , -0.23321328,  1.8688756 , ...,  1.6278226 ,
          0.0593133 ,  0.90110534],
        [-0.24389721, -0.9982976 ,  1.8318357 , ...,  1.6278219 ,
          0.05931258,  0.9011047 ]],

       [[ 0.81105447, -0.7717942 ,  0.5229695 , ...,  0.02146667,
          0.24027777,  0.26449662],
        [ 1.7306108 ,  1.4167533 ,  1.2637417 , ..., -0.04352736,
          0.04924804,  0.62587   ],
        [ 0.03706914,  0.90970105,  1.9453614 , ...,  0.0534128 ,
          0.01318026,  0.86922586],
        ...,
        [-1.3773707 ,  0.633837  ,  1.1719698 , ...,  1.6278234 ,
          0.05931401,  0.90110606],
        [-1.1504335 , -0.23321328,  1.8688756 , ...,  1.6278226 ,
          0.0593133 ,  0.90110534],
        [-0.24389721, -0.9982976 ,  1.8318357 , ...,  1.6278219 ,
          0.05931258,  0.9011047 ]]], dtype=float32)&gt;</code></pre>
</div>
</div>
</section>
<section id="capas-de-adición-y-normalización" class="level4">
<h4 class="anchored" data-anchor-id="capas-de-adición-y-normalización">Capas de Adición y normalización</h4>
<table>
<tbody><tr>
<th colspan="2">
Add y normalize
</th>
</tr><tr>
</tr><tr>
<td>
<img src="https://www.tensorflow.org/images/tutorials/transformer/Add+Norm.png">
</td>
</tr>
</tbody></table>
<p>Estos bloques de “Add &amp; Norm” se encuentran distribuidos a lo largo de todo el modelo Transformer. Cada uno combina una conexión residual y pasa el resultado a través de una capa de <code>LayerNormalization</code>.</p>
<p>La manera más sencilla de organizar el código es estructurándolo alrededor de estos bloques residuales. En las siguientes secciones, definiremos clases de capas personalizadas para cada uno de ellos.</p>
<p>Los bloques residuales “Add &amp; Norm” se incluyen para que el entrenamiento sea eficiente. La conexión residual proporciona una ruta directa para el gradiente (y asegura que los vectores sean <strong>actualizados</strong> por las capas de atención en lugar de ser <strong>reemplazados</strong>), mientras que la normalización mantiene una escala razonable para las salidas.</p>
<p><strong>Nota</strong>: Las implementaciones que se muestran a continuación utilizan la capa <code>Add</code> para asegurar que las máscaras de Keras se propaguen correctamente (el operador <code>+</code> no lo hace).</p>
</section>
<section id="bases-de-la-capa-de-atención" class="level4">
<h4 class="anchored" data-anchor-id="bases-de-la-capa-de-atención">Bases de la capa de atención</h4>
<p>Las capas de atención se utilizan a lo largo de todo el modelo Transformer. Todas ellas son idénticas, excepto por cómo se configura la atención. Cada una contiene una capa <code>layers.MultiHeadAttention</code>, una capa <code>layers.LayerNormalization</code> y una capa <code>layers.Add</code>.</p>
<table>
<tbody><tr>
<th colspan="2">
Capa de atención básica
</th>
</tr><tr>
</tr><tr>
<td>
<img src="https://www.tensorflow.org/images/tutorials/transformer/BaseAttention.png">
</td>
</tr>
</tbody></table>
<p>Para implementar estas capas de atención, comenzaremos con una clase base simple que sólo contenga definidos los componentes. Cada caso de uso se implementará como una subclase (framework).</p>
<div id="cell-65" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BaseAttention(tf.keras.layers.Layer):</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, <span class="op">**</span>kwargs):</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.mha <span class="op">=</span> tf.keras.layers.MultiHeadAttention(<span class="op">**</span>kwargs)</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.layernorm <span class="op">=</span> tf.keras.layers.LayerNormalization()</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.add <span class="op">=</span> tf.keras.layers.Add()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="atención-cómo-funciona" class="level5">
<h5 class="anchored" data-anchor-id="atención-cómo-funciona"><strong>Atención, cómo funciona?</strong></h5>
<p>Antes de entrar en los detalles de cada uso, aquí tienes un breve repaso de cómo funciona la atención:</p>
<table>
<tbody><tr>
<th colspan="1">
Capa de atención básica
</th>
</tr>
<tr>
<td>
<img width="430" src="https://www.tensorflow.org/images/tutorials/transformer/BaseAttention-new.png">
</td>
</tr>
</tbody></table>
<p>Hay dos entradas: 1. La secuencia de consulta (<em>query sequence</em>): la secuencia que se está procesando; la secuencia que “atiende” (abajo). 2. La secuencia de contexto (<em>context sequence</em>): la secuencia a la que se está “atendiendo” (izquierda).</p>
<p>La salida tiene la misma forma que la secuencia de consulta.</p>
<p>Una analogía común es que esta operación se asemeja a una búsqueda en un diccionario. Una búsqueda en un diccionario <strong>difuso</strong>, <strong>diferenciable</strong> y <strong>vectorizado</strong>.</p>
<p>Aquí tienes un diccionario regular de Python, con 3 claves y 3 valores al que se le pasa una única consulta.</p>
<pre><code>d = {'color': 'azul', 'edad': 22, 'tipo': 'pickup'}
result = d['color']</code></pre>
<ul>
<li>El <code>query</code> es lo que se esta tratando de encontrar.</li>
<li>The <code>key</code> es el tipo de información que tiene el diccionario</li>
<li>The <code>value</code> es la información</li>
</ul>
<p>Cuando buscas una <code>query</code> (consulta) en un diccionario normal, el diccionario encuentra la <code>key</code> (clave) coincidente y devuelve su <code>value</code> (valor) asociado. La <code>query</code> tiene una <code>key</code> coincidente o no la tiene.</p>
<p>Puedes imaginar un diccionario <strong>difuso</strong> donde las claves no tienen que coincidir perfectamente. Si buscaras <code>d["especie"]</code> en el diccionario de arriba, quizás querrías que devolviera <code>"pickup"</code> ya que es la mejor coincidencia para la consulta.</p>
<p>Una capa de atención realiza una búsqueda difusa como esta, pero no solo busca la mejor clave. Combina los <code>values</code> basándose en qué tan bien la <code>query</code> coincide con cada <code>key</code>.</p>
<p>¿Cómo funciona esto? En una capa de atención, la <code>query</code>, la <code>key</code> y el <code>value</code> son cada uno vectores. En lugar de realizar una búsqueda de asignación, la capa de atención combina los vectores de la <code>query</code> y la <code>key</code> para determinar qué tan bien coinciden, obteniendo una “puntuación de atención” (<em>attention score</em>). La capa devuelve el promedio de todos los <code>values</code>, ponderado por las “puntuaciones de atención”.</p>
<p>Cada posición en la secuencia de consulta (<em>query sequence</em>) proporciona un vector de <code>query</code>. La secuencia de contexto (<em>context sequence</em>) actúa como el diccionario. Cada posición en la secuencia de contexto proporciona un vector de <code>key</code> y un vector de <code>value</code>.</p>
<p>Los vectores de entrada no se utilizan directamente; la capa <code>layers.MultiHeadAttention</code> incluye capas <code>layers.Dense</code> para proyectar los vectores de entrada antes de utilizarlos.</p>
</section>
<section id="capa-de-atención-cruzada" class="level5">
<h5 class="anchored" data-anchor-id="capa-de-atención-cruzada"><strong>Capa de atención cruzada</strong></h5>
<p>En el centro literal del Transformer está la capa de atención cruzada. Esta capa conecta el codificador y el decodificador. Esta capa es el uso más directo de la atención en el modelo, realiza la misma tarea que el bloque de atención en el <a href="https://www.tensorflow.org/text/tutorials/nmt_with_attention">Tutorial NMT con atención</a>.</p>
<table>
<tbody><tr>
<th colspan="1">
Atención cruzada
</th>
</tr><tr>
</tr><tr>
<td>
<img src="https://www.tensorflow.org/images/tutorials/transformer/CrossAttention.png">
</td>
</tr>
</tbody></table>
<p>Para implementar esto, pasas la secuencia objetivo <code>x</code> como la <code>query</code> (consulta) y la secuencia de <code>context</code> (contexto) como la <code>key/value</code> (clave/valor) al llamar a la capa <code>mha</code>:</p>
<div id="cell-72" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CrossAttention(BaseAttention):</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> call(<span class="va">self</span>, x, context):</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>    attn_output, attn_scores <span class="op">=</span> <span class="va">self</span>.mha(</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>        query<span class="op">=</span>x,</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>        key<span class="op">=</span>context,</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>        value<span class="op">=</span>context,</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>        return_attention_scores<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Almacenar los scores de atención para</span></span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># visualizarlos más adelante</span></span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.last_attn_scores <span class="op">=</span> attn_scores</span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.add([x, attn_output])</span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.layernorm(x)</span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-16"><a href="#cb51-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>La siguiente caricatura muestra cómo fluye la información a través de esta capa. Las columnas representan la suma ponderada de la secuencia contextual.</p>
<p>Para simplificar, no se muestran las conexiones residuales.</p>
<table>
<tbody><tr>
<th>
Atención cruzada
</th>
</tr>
<tr>
<td>
<img width="430" src="https://www.tensorflow.org/images/tutorials/transformer/CrossAttention-new-full.png">
</td>
</tr>
</tbody></table>
<p>La longitud de la salida es la longitud de la secuencia de <code>query</code> (consulta), y no la longitud de la secuencia de <code>key/value</code> (clave/valor) del contexto.</p>
<p>El diagrama se simplifica aún más a continuación. No es necesario dibujar la matriz completa de “pesos de atención”.</p>
<p>El punto clave es que cada ubicación en la <code>query</code> puede “ver” todos los pares de <code>key/value</code> en el contexto, pero no se intercambia información entre las diferentes consultas.</p>
<table>
<tbody><tr>
<th>
Cada elemento del query puede ver todo el contexto.
</th>
</tr>
<tr>
<td>
<img width="430" src="https://www.tensorflow.org/images/tutorials/transformer/CrossAttention-new.png">
</td>
</tr>
</tbody></table>
<p>Ejemplo:</p>
<div id="cell-78" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="5e1de892-1052-40fa-e9f9-f73e7b2297dd" data-execution_count="31">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>sample_ca <span class="op">=</span> CrossAttention(num_heads<span class="op">=</span><span class="dv">2</span>, key_dim<span class="op">=</span><span class="dv">512</span>)</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pt_emb.shape)</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(en_emb.shape)</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a><span class="co"># cada token de la frase en inglés</span></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a><span class="co"># será operados con todos los tokens de</span></span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a><span class="co"># la oración en portugués, esto pasa así</span></span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a><span class="co"># solo en entrenamiento</span></span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sample_ca(en_emb, pt_emb).shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(64, 64, 512)
(64, 64, 512)
(64, 64, 512)</code></pre>
</div>
</div>
</section>
<section id="capa-global-de-auto-atención" class="level5">
<h5 class="anchored" data-anchor-id="capa-global-de-auto-atención"><strong>Capa global de auto-atención</strong></h5>
<p>Esta capa se encarga de procesar la secuencia contextual y de propagar la información a lo largo de la misma:</p>
<table>
<tbody><tr>
<th colspan="1">
La capa global de auto-atención
</th>
</tr><tr>
</tr><tr>
<td>
<img src="https://www.tensorflow.org/images/tutorials/transformer/SelfAttention.png">
</td>
</tr>
</tbody></table>
<p>Dado que la secuencia contextual es fija mientras se genera la traducción (entrenamiento), se permite que la información fluya en ambas direcciones.</p>
<p>Antes de los Transformers y la autoatención, los modelos solían utilizar RNNs o CNNs para realizar esta tarea (bidireccionalidad):</p>
<table>
<tbody><tr>
<th colspan="1">
RNNs y CNNs bidireccionales
</th>
</tr>
<tr>
<td>
<img width="500" src="https://www.tensorflow.org/images/tutorials/transformer/RNN-bidirectional.png">
</td>
</tr>
<tr>
<td>
<img width="500" src="https://www.tensorflow.org/images/tutorials/transformer/CNN.png">
</td>
</tr>
</tbody></table>
<p>RNNs and CNNs have their limitations.</p>
<p>Las RNNs y las CNNs tienen sus limitaciones:</p>
<ul>
<li>La RNN permite que la información fluya a lo largo de toda la secuencia, pero atraviesa muchos pasos de procesamiento para llegar allí (limitando el flujo del gradiente). Estos pasos de la RNN deben ejecutarse secuencialmente, por lo que la RNN es menos capaz de aprovechar los dispositivos paralelos modernos.</li>
<li>En la CNN, cada ubicación puede procesarse en paralelo, pero solo proporciona un campo receptivo limitado. El campo receptivo solo crece linealmente con el número de capas CNN. Se necesita apilar varias capas convolucionales para transmitir información a través de la secuencia.</li>
</ul>
<p>Por otro lado, la capa de auto-atención global permite que cada elemento de la secuencia acceda directamente a todos los demás elementos de la secuencia, con solo unas pocas operaciones, y todas las salidas se pueden calcular en paralelo.</p>
<p>Para implementar esta capa, solo necesitas pasar la secuencia objetivo, <code>x</code>, como los argumentos <code>query</code> (consulta) y <code>value</code> (valor) a la capa <code>mha</code>:</p>
<div id="cell-85" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GlobalSelfAttention(BaseAttention):</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> call(<span class="va">self</span>, x):</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>    attn_output <span class="op">=</span> <span class="va">self</span>.mha(</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>        query<span class="op">=</span>x,</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>        value<span class="op">=</span>x,</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>        key<span class="op">=</span>x)</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.add([x, attn_output])</span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.layernorm(x)</span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-86" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="2ce9565e-ab51-4667-a5ed-7ac88d6c1355" data-execution_count="33">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>sample_gsa <span class="op">=</span> GlobalSelfAttention(num_heads<span class="op">=</span><span class="dv">2</span>, key_dim<span class="op">=</span><span class="dv">512</span>)</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pt_emb.shape)</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sample_gsa(pt_emb).shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(64, 64, 512)
(64, 64, 512)</code></pre>
</div>
</div>
<p>Siguiendo con el mismo estilo de antes, podriamos dibujarlo así:</p>
<table>
<tbody><tr>
<th colspan="1">
La capa global de auto-atención
</th>
</tr><tr>
</tr><tr>
<td>
<img width="330" src="https://www.tensorflow.org/images/tutorials/transformer/SelfAttention-new-full.png">
</td>
</tr>
</tbody></table>
<p>De nuevo, las conexiones residuales se omiten para mayor claridad.</p>
<p>Es más compacto e igual de preciso dibujarlo así:</p>
<table>
<tbody><tr>
<th colspan="1">
La capa de auto-atención global
</th>
</tr><tr>
</tr><tr>
<td>
<img width="500" src="https://www.tensorflow.org/images/tutorials/transformer/SelfAttention-new.png">
</td>
</tr>
</tbody></table>
</section>
<section id="capa-de-auto-atención-causal" class="level5">
<h5 class="anchored" data-anchor-id="capa-de-auto-atención-causal"><strong>Capa de auto-atención causal</strong></h5>
<p>Esta capa realiza un trabajo similar al de la capa de autoatención global, para la secuencia de salida:</p>
<table>
<tbody><tr>
<th colspan="1">
Capa causal de auto-atención
</th>
</tr><tr>
</tr><tr>
<td>
<img src="https://www.tensorflow.org/images/tutorials/transformer/CausalSelfAttention.png">
</td>
</tr>
</tbody></table>
<p>This needs to be handled differently from the encoder’s global self-attention layer.</p>
<p>Like the <a href="https://www.tensorflow.org/text/tutorials/text_generation">text generation tutorial</a>, and the <a href="https://www.tensorflow.org/text/tutorials/nmt_with_attention">NMT with attention</a> tutorial, Transformers are an “autoregressive” model: They generate the text one token at a time and feed that output back to the input. To make this <em>efficient</em>, these models ensure that the output for each sequence element only depends on the previous sequence elements; the models are “causal”.</p>
<p>Una RNN unidireccional es causal por definición. Para hacer una convolución causal, solo necesitas rellenar (<em>pad</em>) la entrada y desplazar la salida para que se alinee correctamente (puedes usar <code>layers.Conv1D(padding='causal')</code>).</p>
<table>
<tbody><tr>
<th colspan="1">
Causalidad en RNNs y CNNs
</th>
</tr>
<tr>
<td>
<img width="500" src="https://www.tensorflow.org/images/tutorials/transformer/RNN.png">
</td>
</tr>
<tr>
<td>
<img width="500" src="https://www.tensorflow.org/images/tutorials/transformer/CNN-causal.png">
</td>
</tr>
</tbody></table>
<p>Un modelo causal es eficiente de dos maneras:</p>
<ol type="1">
<li><strong>En el entrenamiento</strong>, te permite calcular la pérdida para cada posición en la secuencia de salida ejecutando el modelo una sola vez.</li>
<li><strong>Durante la inferencia</strong>, para cada nuevo token generado, solo necesitas calcular sus salidas; las salidas de los elementos de la secuencia anterior se pueden reutilizar.
<ul>
<li>Para una RNN, solo necesitas el estado de la RNN para tener en cuenta los cálculos previos (pasa <code>return_state=True</code> al constructor de la capa RNN).</li>
<li>Para una CNN, necesitarías seguir el enfoque de <a href="https://arxiv.org/abs/1611.09482">Fast Wavenet</a>.</li>
</ul></li>
</ol>
<p>Para construir una capa de auto-atención causal, necesitas usar una máscara apropiada al calcular las puntuaciones de atención y sumar los <code>value</code>s de atención.</p>
<p>Esto se gestiona automáticamente si pasas <code>use_causal_mask = True</code> a la capa <code>MultiHeadAttention</code> cuando la llamas:</p>
<div id="cell-99" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CausalSelfAttention(BaseAttention):</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> call(<span class="va">self</span>, x):</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>    attn_output <span class="op">=</span> <span class="va">self</span>.mha(</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>        query<span class="op">=</span>x,</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>        value<span class="op">=</span>x,</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>        key<span class="op">=</span>x,</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>        use_causal_mask <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.add([x, attn_output])</span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.layernorm(x)</span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>La máscara causal garantiza que cada lugar sólo tenga acceso a los lugares que le preceden:</p>
<table>
<tbody><tr>
<th colspan="1">
Capa de auto-atención causal.
</th>
</tr><tr>
</tr><tr>
<td>
<img width="330" src="https://www.tensorflow.org/images/tutorials/transformer/CausalSelfAttention-new-full.png">
</td>
</tr>
</tbody></table>
<p>De nuevo, las conexiones residuales se omiten por simplicidad.</p>
<p>La representación más compacta de esta capa sería:</p>
<table>

<tbody><tr><th colspan="1">
Capa de auto-atención causal.
</th>
</tr><tr>
</tr><tr>
<td>
<img width="430" src="https://www.tensorflow.org/images/tutorials/transformer/CausalSelfAttention-new.png">
</td>
</tr>
</tbody></table>
<p>Ejemplo:</p>
<div id="cell-105" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="0c89c6fd-d248-4c33-fd2f-dba9901da126" data-execution_count="35">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>sample_csa <span class="op">=</span> CausalSelfAttention(num_heads<span class="op">=</span><span class="dv">2</span>, key_dim<span class="op">=</span><span class="dv">512</span>)</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(en_emb.shape)</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sample_csa(en_emb).shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(64, 64, 512)
(64, 64, 512)</code></pre>
</div>
</div>
<p>La salida de los primeros elementos de la secuencia no depende de los elementos posteriores, por lo que no debería importar si recorta los elementos antes o después de aplicar la capa:</p>
<div id="cell-107" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="9cdcfa7c-3fc5-472d-b36a-1c6faa882ea0" data-execution_count="36">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ejemplo donde la sálida debe ser cercana a cero</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="co"># ya que no deberia influir información posterior</span></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>out1 <span class="op">=</span> sample_csa(embed_en(en[:, :<span class="dv">3</span>]))</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>out2 <span class="op">=</span> sample_csa(embed_en(en))[:, :<span class="dv">3</span>]</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>tf.reduce_max(<span class="bu">abs</span>(out1 <span class="op">-</span> out2)).numpy()</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a><span class="co"># nota omitir el warning por ahora</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>np.float32(7.1525574e-07)</code></pre>
</div>
</div>
<p>Nota: Cuando se utilizan máscaras Keras, los valores de salida en lugares no válidos no están bien definidos. Por lo tanto, lo anterior puede no ser válido para las regiones enmascaradas.</p>
</section>
</section>
<section id="la-red-feed-forward-mlp" class="level4">
<h4 class="anchored" data-anchor-id="la-red-feed-forward-mlp">La red feed forward (MLP)</h4>
<p>El Transformer también incluye esta red neuronal feed-forward <em>point-wise</em> tanto en el codificador como en el decodificador:</p>
<table>
<tbody><tr>
<th colspan="1">
La red feed forward (MLP)
</th>
</tr><tr>
</tr><tr>
<td>
<img src="https://www.tensorflow.org/images/tutorials/transformer/FeedForward.png">
</td>
</tr>
</tbody></table>
<p>La red consiste en dos capas lineales (<code>tf.keras.layers.Dense</code>) con una función de activación ReLU entre ellas, y una capa de dropout. Al igual que con las capas de atención, el código aquí también incluye la conexión residual y la normalización:</p>
<div id="cell-113" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FeedForward(tf.keras.layers.Layer):</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, dff, dropout_rate<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.seq <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>      tf.keras.layers.Dense(dff, activation<span class="op">=</span><span class="st">'relu'</span>),</span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>      tf.keras.layers.Dense(d_model),</span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>      tf.keras.layers.Dropout(dropout_rate)</span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>    ])</span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.add <span class="op">=</span> tf.keras.layers.Add()</span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.layer_norm <span class="op">=</span> tf.keras.layers.LayerNormalization()</span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> call(<span class="va">self</span>, x):</span>
<span id="cb62-13"><a href="#cb62-13" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.add([x, <span class="va">self</span>.seq(x)])</span>
<span id="cb62-14"><a href="#cb62-14" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.layer_norm(x)</span>
<span id="cb62-15"><a href="#cb62-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Prueba:</p>
<div id="cell-115" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="f55e79ff-ff9a-4080-ee10-723c7b648947" data-execution_count="38">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>sample_ffn <span class="op">=</span> FeedForward(<span class="dv">512</span>, <span class="dv">2048</span>)</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(en_emb.shape)</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sample_ffn(en_emb).shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(64, 64, 512)
(64, 64, 512)</code></pre>
</div>
</div>
</section>
<section id="la-capa-encoder" class="level4">
<h4 class="anchored" data-anchor-id="la-capa-encoder">La capa encoder</h4>
<p>El codificador contiene una pila de <code>N</code> capas de codificador. Donde cada <code>EncoderLayer</code> contiene una capa <code>GlobalSelfAttention</code> y una capa <code>FeedForward</code>:</p>
<table>
<tbody><tr>
<th colspan="1">
La capa encoder
</th>
</tr><tr>
</tr><tr>
<td>
<img src="https://www.tensorflow.org/images/tutorials/transformer/EncoderLayer.png">
</td>
</tr>
</tbody></table>
<p>Aquí la estructura de la capa <code>EncoderLayer</code>:</p>
<div id="cell-120" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EncoderLayer(tf.keras.layers.Layer):</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,<span class="op">*</span>, d_model, num_heads, dff, dropout_rate<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.self_attention <span class="op">=</span> GlobalSelfAttention(</span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>        num_heads<span class="op">=</span>num_heads,</span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>        key_dim<span class="op">=</span>d_model,</span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a>        dropout<span class="op">=</span>dropout_rate)</span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.ffn <span class="op">=</span> FeedForward(d_model, dff)</span>
<span id="cb65-11"><a href="#cb65-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-12"><a href="#cb65-12" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> call(<span class="va">self</span>, x):</span>
<span id="cb65-13"><a href="#cb65-13" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.self_attention(x)</span>
<span id="cb65-14"><a href="#cb65-14" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.ffn(x)</span>
<span id="cb65-15"><a href="#cb65-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Prueba:</p>
<div id="cell-122" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="63875d50-ad27-4ece-c02b-8d16d1715934" data-execution_count="40">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>sample_encoder_layer <span class="op">=</span> EncoderLayer(d_model<span class="op">=</span><span class="dv">512</span>, num_heads<span class="op">=</span><span class="dv">8</span>, dff<span class="op">=</span><span class="dv">2048</span>)</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pt_emb.shape)</span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sample_encoder_layer(pt_emb).shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(64, 64, 512)
(64, 64, 512)</code></pre>
</div>
</div>
</section>
<section id="el-módulo-encoder" class="level4">
<h4 class="anchored" data-anchor-id="el-módulo-encoder">El módulo Encoder</h4>
<p>Construyamos el encoder, agregandole la parte de embedings y la codificación posicional.</p>
<table>
<tbody><tr>
<th colspan="1">
El encoder
</th>
</tr><tr>
</tr><tr>
<td>
<img src="https://www.tensorflow.org/images/tutorials/transformer/Encoder.png">
</td>
</tr>
</tbody></table>
<p>El codificador consiste en:</p>
<ul>
<li>Una capa <code>PositionalEmbedding</code> en la entrada.</li>
<li>Una pila de capas <code>EncoderLayer</code>.</li>
</ul>
<div id="cell-127" class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Encoder(tf.keras.layers.Layer):</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, <span class="op">*</span>, num_layers, d_model, num_heads,</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>               dff, vocab_size, dropout_rate<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.d_model <span class="op">=</span> d_model</span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.num_layers <span class="op">=</span> num_layers</span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.pos_embedding <span class="op">=</span> PositionalEmbedding(</span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a>        vocab_size<span class="op">=</span>vocab_size, d_model<span class="op">=</span>d_model)</span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-12"><a href="#cb68-12" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.enc_layers <span class="op">=</span> [</span>
<span id="cb68-13"><a href="#cb68-13" aria-hidden="true" tabindex="-1"></a>        EncoderLayer(d_model<span class="op">=</span>d_model,</span>
<span id="cb68-14"><a href="#cb68-14" aria-hidden="true" tabindex="-1"></a>                     num_heads<span class="op">=</span>num_heads,</span>
<span id="cb68-15"><a href="#cb68-15" aria-hidden="true" tabindex="-1"></a>                     dff<span class="op">=</span>dff,</span>
<span id="cb68-16"><a href="#cb68-16" aria-hidden="true" tabindex="-1"></a>                     dropout_rate<span class="op">=</span>dropout_rate)</span>
<span id="cb68-17"><a href="#cb68-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)]</span>
<span id="cb68-18"><a href="#cb68-18" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.dropout <span class="op">=</span> tf.keras.layers.Dropout(dropout_rate)</span>
<span id="cb68-19"><a href="#cb68-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-20"><a href="#cb68-20" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> call(<span class="va">self</span>, x):</span>
<span id="cb68-21"><a href="#cb68-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># `x` es token-IDs shape: (batch, seq_len)</span></span>
<span id="cb68-22"><a href="#cb68-22" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.pos_embedding(x)  <span class="co"># Shape `(batch_size, seq_len, d_model)`.</span></span>
<span id="cb68-23"><a href="#cb68-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-24"><a href="#cb68-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># añadir dropout.</span></span>
<span id="cb68-25"><a href="#cb68-25" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb68-26"><a href="#cb68-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-27"><a href="#cb68-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.num_layers):</span>
<span id="cb68-28"><a href="#cb68-28" aria-hidden="true" tabindex="-1"></a>      x <span class="op">=</span> <span class="va">self</span>.enc_layers[i](x)</span>
<span id="cb68-29"><a href="#cb68-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-30"><a href="#cb68-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x  <span class="co"># Shape `(batch_size, seq_len, d_model)`.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Probar:</p>
<div id="cell-129" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="ce94a408-5480-41d9-83de-9a4f621549c7" data-execution_count="42">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Instanciar el Encoder.</span></span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>sample_encoder <span class="op">=</span> Encoder(num_layers<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>                         d_model<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>                         num_heads<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>                         dff<span class="op">=</span><span class="dv">2048</span>,</span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>                         vocab_size<span class="op">=</span><span class="dv">8500</span>)</span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Fijar training en false</span></span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a>sample_encoder_output <span class="op">=</span> sample_encoder(pt, training<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-10"><a href="#cb69-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Dimensiones</span></span>
<span id="cb69-11"><a href="#cb69-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pt.shape)</span>
<span id="cb69-12"><a href="#cb69-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sample_encoder_output.shape)  <span class="co"># Shape `(batch_size, input_seq_len, d_model)`.</span></span>
<span id="cb69-13"><a href="#cb69-13" aria-hidden="true" tabindex="-1"></a><span class="co"># ignorar los warnings</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(64, 64)
(64, 64, 512)</code></pre>
</div>
</div>
</section>
<section id="la-capa-decoder" class="level4">
<h4 class="anchored" data-anchor-id="la-capa-decoder">La capa decoder</h4>
<p>El decodificador es ligeramente más compleja, con cada <code>DecoderLayer</code> conteniendo una capa <code>CausalSelfAttention</code>, una capa <code>CrossAttention</code> y una capa <code>FeedForward</code>:</p>
<table>
<tbody><tr>
<th colspan="1">
La capa decoder
</th>
</tr><tr>
</tr><tr>
<td>
<img src="https://www.tensorflow.org/images/tutorials/transformer/DecoderLayer.png">
</td>
</tr>
</tbody></table>
<div id="cell-133" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DecoderLayer(tf.keras.layers.Layer):</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>               <span class="op">*</span>,</span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>               d_model,</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>               num_heads,</span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a>               dff,</span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a>               dropout_rate<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">super</span>(DecoderLayer, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.causal_self_attention <span class="op">=</span> CausalSelfAttention(</span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a>        num_heads<span class="op">=</span>num_heads,</span>
<span id="cb71-12"><a href="#cb71-12" aria-hidden="true" tabindex="-1"></a>        key_dim<span class="op">=</span>d_model,</span>
<span id="cb71-13"><a href="#cb71-13" aria-hidden="true" tabindex="-1"></a>        dropout<span class="op">=</span>dropout_rate)</span>
<span id="cb71-14"><a href="#cb71-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-15"><a href="#cb71-15" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.cross_attention <span class="op">=</span> CrossAttention(</span>
<span id="cb71-16"><a href="#cb71-16" aria-hidden="true" tabindex="-1"></a>        num_heads<span class="op">=</span>num_heads,</span>
<span id="cb71-17"><a href="#cb71-17" aria-hidden="true" tabindex="-1"></a>        key_dim<span class="op">=</span>d_model,</span>
<span id="cb71-18"><a href="#cb71-18" aria-hidden="true" tabindex="-1"></a>        dropout<span class="op">=</span>dropout_rate)</span>
<span id="cb71-19"><a href="#cb71-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-20"><a href="#cb71-20" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.ffn <span class="op">=</span> FeedForward(d_model, dff)</span>
<span id="cb71-21"><a href="#cb71-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-22"><a href="#cb71-22" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> call(<span class="va">self</span>, x, context):</span>
<span id="cb71-23"><a href="#cb71-23" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.causal_self_attention(x<span class="op">=</span>x)</span>
<span id="cb71-24"><a href="#cb71-24" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.cross_attention(x<span class="op">=</span>x, context<span class="op">=</span>context)</span>
<span id="cb71-25"><a href="#cb71-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-26"><a href="#cb71-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Solo para efectos de visualización posterior</span></span>
<span id="cb71-27"><a href="#cb71-27" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.last_attn_scores <span class="op">=</span> <span class="va">self</span>.cross_attention.last_attn_scores</span>
<span id="cb71-28"><a href="#cb71-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-29"><a href="#cb71-29" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.ffn(x)  <span class="co"># Shape `(batch_size, seq_len, d_model)`.</span></span>
<span id="cb71-30"><a href="#cb71-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Probar la capa del decoder:</p>
<div id="cell-135" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="4e920438-e86e-4907-b88a-647fa49cc35c" data-execution_count="44">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>sample_decoder_layer <span class="op">=</span> DecoderLayer(d_model<span class="op">=</span><span class="dv">512</span>, num_heads<span class="op">=</span><span class="dv">8</span>, dff<span class="op">=</span><span class="dv">2048</span>)</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>sample_decoder_layer_output <span class="op">=</span> sample_decoder_layer(</span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>en_emb, context<span class="op">=</span>pt_emb)</span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(en_emb.shape)</span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pt_emb.shape)</span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sample_decoder_layer_output.shape)  <span class="co"># `(batch_size, seq_len, d_model)`</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(64, 64, 512)
(64, 64, 512)
(64, 64, 512)</code></pre>
</div>
</div>
</section>
<section id="el-módulo-decoder" class="level4">
<h4 class="anchored" data-anchor-id="el-módulo-decoder">El módulo decoder</h4>
<p>De forma similar al Codificador, el Decodificador consiste en un <code>PositionalEmbedding</code> y una pila de <code>DecoderLayer</code>’s:</p>
<table>
<tbody><tr>
<th colspan="1">
Capa Decoder + Embedding + PE
</th>
</tr><tr>
</tr><tr>
<td>
<img src="https://www.tensorflow.org/images/tutorials/transformer/Decoder.png">
</td>
</tr>
</tbody></table>
<p>Definimos el Decoder extendiendo <code>tf.keras.layers.Layer</code>:</p>
<div id="cell-140" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Decoder(tf.keras.layers.Layer):</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, <span class="op">*</span>, num_layers, d_model, num_heads, dff, vocab_size,</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>               dropout_rate<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">super</span>(Decoder, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.d_model <span class="op">=</span> d_model</span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.num_layers <span class="op">=</span> num_layers</span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.pos_embedding <span class="op">=</span> PositionalEmbedding(vocab_size<span class="op">=</span>vocab_size,</span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a>                                             d_model<span class="op">=</span>d_model)</span>
<span id="cb74-11"><a href="#cb74-11" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.dropout <span class="op">=</span> tf.keras.layers.Dropout(dropout_rate)</span>
<span id="cb74-12"><a href="#cb74-12" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.dec_layers <span class="op">=</span> [</span>
<span id="cb74-13"><a href="#cb74-13" aria-hidden="true" tabindex="-1"></a>        DecoderLayer(d_model<span class="op">=</span>d_model, num_heads<span class="op">=</span>num_heads,</span>
<span id="cb74-14"><a href="#cb74-14" aria-hidden="true" tabindex="-1"></a>                     dff<span class="op">=</span>dff, dropout_rate<span class="op">=</span>dropout_rate)</span>
<span id="cb74-15"><a href="#cb74-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)]</span>
<span id="cb74-16"><a href="#cb74-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-17"><a href="#cb74-17" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.last_attn_scores <span class="op">=</span> <span class="va">None</span></span>
<span id="cb74-18"><a href="#cb74-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-19"><a href="#cb74-19" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> call(<span class="va">self</span>, x, context):</span>
<span id="cb74-20"><a href="#cb74-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># `x` es token-IDs shape (batch, target_seq_len)</span></span>
<span id="cb74-21"><a href="#cb74-21" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.pos_embedding(x)  <span class="co"># (batch_size, target_seq_len, d_model)</span></span>
<span id="cb74-22"><a href="#cb74-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-23"><a href="#cb74-23" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb74-24"><a href="#cb74-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-25"><a href="#cb74-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.num_layers):</span>
<span id="cb74-26"><a href="#cb74-26" aria-hidden="true" tabindex="-1"></a>      x  <span class="op">=</span> <span class="va">self</span>.dec_layers[i](x, context)</span>
<span id="cb74-27"><a href="#cb74-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-28"><a href="#cb74-28" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.last_attn_scores <span class="op">=</span> <span class="va">self</span>.dec_layers[<span class="op">-</span><span class="dv">1</span>].last_attn_scores</span>
<span id="cb74-29"><a href="#cb74-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-30"><a href="#cb74-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># EL shape de x es (batch_size, target_seq_len, d_model).</span></span>
<span id="cb74-31"><a href="#cb74-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Probar:</p>
<div id="cell-142" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="9b4c661c-4deb-4fd3-df00-3bdb4c3b7b09" data-execution_count="46">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Instanciar el decoder</span></span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>sample_decoder <span class="op">=</span> Decoder(num_layers<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>                         d_model<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>                         num_heads<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a>                         dff<span class="op">=</span><span class="dv">2048</span>,</span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a>                         vocab_size<span class="op">=</span><span class="dv">8000</span>)</span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-8"><a href="#cb75-8" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> sample_decoder(</span>
<span id="cb75-9"><a href="#cb75-9" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>en,</span>
<span id="cb75-10"><a href="#cb75-10" aria-hidden="true" tabindex="-1"></a>    context<span class="op">=</span>pt_emb)</span>
<span id="cb75-11"><a href="#cb75-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-12"><a href="#cb75-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Shapes.</span></span>
<span id="cb75-13"><a href="#cb75-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(en.shape)</span>
<span id="cb75-14"><a href="#cb75-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pt_emb.shape)</span>
<span id="cb75-15"><a href="#cb75-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(output.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(64, 64)
(64, 64, 512)
(64, 64, 512)</code></pre>
</div>
</div>
<div id="cell-143" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="c689c537-f5e3-4791-e9fd-55bd9d1b2cbb" data-execution_count="47">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>sample_decoder.last_attn_scores.shape  <span class="co"># (batch, heads, target_seq, input_seq)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="47">
<pre><code>TensorShape([64, 8, 64, 64])</code></pre>
</div>
</div>
<p>Una vez creados el codificador y el decodificador Transformer, es hora de construir el modelo Transformer y entrenarlo.</p>
</section>
</section>
</section>
<section id="el-transformer" class="level2">
<h2 class="anchored" data-anchor-id="el-transformer">El Transformer</h2>
<p>You now have <code>Encoder</code> and <code>Decoder</code>. To complete the <code>Transformer</code> model, you need to put them together and add a final linear (<code>Dense</code>) layer which converts the resulting vector at each location into output token probabilities.</p>
<p>The output of the decoder is the input to this final linear layer.</p>
<table>
<tbody><tr>
<th colspan="1">
El transformer
</th>
</tr><tr>
</tr><tr>
<td>
<img src="https://www.tensorflow.org/images/tutorials/transformer/transformer.png">
</td>
</tr>
</tbody></table>
<p>Un <code>Transformer</code> con una capa tanto en el <code>Codificador</code> como en el <code>Decodificador</code> se parece casi exactamente al modelo del <a href="https://www.tensorflow.org/text/tutorials/nmt_with_attention">tutorial de RNN+atención</a>. Un Transformer multi-capa tiene más capas, pero fundamentalmente está haciendo lo mismo.</p>
<table>
<tbody><tr>
<th colspan="1">
Transformer de una capa
</th>
<th colspan="1">
Transformer de 4 capas
</th>
</tr>
<tr>
<td>
<img width="400" src="https://www.tensorflow.org/images/tutorials/transformer/Transformer-1layer-compact.png">
</td>
<td rowspan="3">
<img width="330" src="https://www.tensorflow.org/images/tutorials/transformer/Transformer-4layer-compact.png">
</td>
</tr>
<tr>
<th colspan="1">
RNN + Modelo de atención
</th>
</tr>
<tr>
<td>
<img width="400" src="https://www.tensorflow.org/images/tutorials/transformer/RNN+attention-compact.png">
</td>
</tr>
</tbody></table>
<p>Crea el <code>Transformer</code> extendiendo <code>tf.keras.Model</code>:</p>
<blockquote class="blockquote">
<p>Nota: El <a href="https://arxiv.org/pdf/1706.03762.pdf">artículo original</a>, sección 3.4, comparte la matriz de pesos entre la capa de embedding y la capa lineal final. Para mantener las cosas simples, este tutorial utiliza dos matrices de pesos separadas.</p>
</blockquote>
<div id="cell-151" class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Transformer(tf.keras.Model):</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, <span class="op">*</span>, num_layers, d_model, num_heads, dff,</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>               input_vocab_size, target_vocab_size, dropout_rate<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.encoder <span class="op">=</span> Encoder(num_layers<span class="op">=</span>num_layers, d_model<span class="op">=</span>d_model,</span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a>                           num_heads<span class="op">=</span>num_heads, dff<span class="op">=</span>dff,</span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a>                           vocab_size<span class="op">=</span>input_vocab_size,</span>
<span id="cb79-8"><a href="#cb79-8" aria-hidden="true" tabindex="-1"></a>                           dropout_rate<span class="op">=</span>dropout_rate)</span>
<span id="cb79-9"><a href="#cb79-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-10"><a href="#cb79-10" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.decoder <span class="op">=</span> Decoder(num_layers<span class="op">=</span>num_layers, d_model<span class="op">=</span>d_model,</span>
<span id="cb79-11"><a href="#cb79-11" aria-hidden="true" tabindex="-1"></a>                           num_heads<span class="op">=</span>num_heads, dff<span class="op">=</span>dff,</span>
<span id="cb79-12"><a href="#cb79-12" aria-hidden="true" tabindex="-1"></a>                           vocab_size<span class="op">=</span>target_vocab_size,</span>
<span id="cb79-13"><a href="#cb79-13" aria-hidden="true" tabindex="-1"></a>                           dropout_rate<span class="op">=</span>dropout_rate)</span>
<span id="cb79-14"><a href="#cb79-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-15"><a href="#cb79-15" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.final_layer <span class="op">=</span> tf.keras.layers.Dense(target_vocab_size)</span>
<span id="cb79-16"><a href="#cb79-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-17"><a href="#cb79-17" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb79-18"><a href="#cb79-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Para usar el `.fit` del modelo keras usted debe pasar</span></span>
<span id="cb79-19"><a href="#cb79-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># todas las inputs como el primer argumento</span></span>
<span id="cb79-20"><a href="#cb79-20" aria-hidden="true" tabindex="-1"></a>    context, x  <span class="op">=</span> inputs</span>
<span id="cb79-21"><a href="#cb79-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-22"><a href="#cb79-22" aria-hidden="true" tabindex="-1"></a>    context <span class="op">=</span> <span class="va">self</span>.encoder(context)  <span class="co"># (batch_size, context_len, d_model)</span></span>
<span id="cb79-23"><a href="#cb79-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-24"><a href="#cb79-24" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.decoder(x, context)  <span class="co"># (batch_size, target_len, d_model)</span></span>
<span id="cb79-25"><a href="#cb79-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-26"><a href="#cb79-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Capa final densa lineal</span></span>
<span id="cb79-27"><a href="#cb79-27" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> <span class="va">self</span>.final_layer(x)  <span class="co"># (batch_size, target_len, target_vocab_size)</span></span>
<span id="cb79-28"><a href="#cb79-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-29"><a href="#cb79-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb79-30"><a href="#cb79-30" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Eliminar las máscaras para que no afecten el cálculo del loss y métricas</span></span>
<span id="cb79-31"><a href="#cb79-31" aria-hidden="true" tabindex="-1"></a>      <span class="co"># b/250038731 --&gt; relacionado con este bug que fue reportado</span></span>
<span id="cb79-32"><a href="#cb79-32" aria-hidden="true" tabindex="-1"></a>      <span class="kw">del</span> logits._keras_mask</span>
<span id="cb79-33"><a href="#cb79-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">AttributeError</span>:</span>
<span id="cb79-34"><a href="#cb79-34" aria-hidden="true" tabindex="-1"></a>      <span class="cf">pass</span></span>
<span id="cb79-35"><a href="#cb79-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-36"><a href="#cb79-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># retornar la salida final</span></span>
<span id="cb79-37"><a href="#cb79-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> logits</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="hiperparámetros" class="level3">
<h3 class="anchored" data-anchor-id="hiperparámetros">Hiperparámetros</h3>
<p>Para mantener este ejemplo pequeño y relativamente rápido, el número de capas (<code>num_layers</code>), la dimensionalidad de los embeddings (<code>d_model</code>) y la dimensionalidad interna de la capa <code>FeedForward</code> (<code>dff</code>) se han reducido.</p>
<p>El modelo base descrito en el artículo original del Transformer utilizaba <code>num_layers=6</code>, <code>d_model=512</code> y <code>dff=2048</code>.</p>
<p>El número de cabezas de auto-atención será (<code>num_heads=4</code>).</p>
<div id="cell-154" class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>num_layers <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>d_model <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a>dff <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>num_heads <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>dropout_rate <span class="op">=</span> <span class="fl">0.1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="probemos-el-transformer" class="level3">
<h3 class="anchored" data-anchor-id="probemos-el-transformer">Probemos el transformer</h3>
<p>Instanciar el modelo <code>Transformer</code>:</p>
<div id="cell-157" class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>transformer <span class="op">=</span> Transformer(</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>    num_layers<span class="op">=</span>num_layers,</span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>    d_model<span class="op">=</span>d_model,</span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a>    num_heads<span class="op">=</span>num_heads,</span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a>    dff<span class="op">=</span>dff,</span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a>    input_vocab_size<span class="op">=</span>tokenizers.pt.get_vocab_size().numpy(),</span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a>    target_vocab_size<span class="op">=</span>tokenizers.en.get_vocab_size().numpy(),</span>
<span id="cb81-8"><a href="#cb81-8" aria-hidden="true" tabindex="-1"></a>    dropout_rate<span class="op">=</span>dropout_rate)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Probar</p>
<div id="cell-159" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="02c682e4-ca37-4de3-9f08-4d265b8c87a6" data-execution_count="51">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> transformer((pt, en))</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(en.shape)</span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pt.shape)</span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(output.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(64, 64)
(64, 64)
(64, 64, 7010)</code></pre>
</div>
</div>
<div id="cell-160" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="886a54f2-7c21-4b88-a1ca-828df2816f21" data-execution_count="52">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="co"># acceder a los scores de atención</span></span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>attn_scores <span class="op">=</span> transformer.decoder.dec_layers[<span class="op">-</span><span class="dv">1</span>].last_attn_scores</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attn_scores.shape)  <span class="co"># (batch, heads, target_seq, input_seq)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(64, 4, 64, 64)</code></pre>
</div>
</div>
</section>
</section>
<section id="training" class="level2">
<h2 class="anchored" data-anchor-id="training">Training</h2>
<p>Tiempo de entrenar!!</p>
<section id="configurar-el-optimizador" class="level3">
<h3 class="anchored" data-anchor-id="configurar-el-optimizador">Configurar el optimizador</h3>
<p>Utilizar el optimizador Adam con un planificador personalizado para la tasa de aprendizaje según la fórmula del Transformer original. <a href="https://arxiv.org/abs/1706.03762">paper</a>.</p>
<p><span class="math display">\[\Large{lrate = d_{model}^{-0.5} * \min(step{\_}num^{-0.5}, step{\_}num \cdot warmup{\_}steps^{-1.5})}\]</span></p>
</section>
<section id="explicación-esquema-de-learning-rate-del-transformer" class="level3">
<h3 class="anchored" data-anchor-id="explicación-esquema-de-learning-rate-del-transformer">Explicación esquema de Learning Rate del Transformer</h3>
<p><strong>Parámetros:</strong></p>
<ol type="1">
<li><strong><code>d_model</code> (Dimensionalidad del Modelo):</strong>
<ul>
<li><strong>Intuición:</strong> “Ancho” del modelo. Mayor <code>d_model</code> = representaciones más ricas, más capacidad.</li>
<li><strong>Efecto:</strong> Actúa como un factor de escala base para el learning rate.
<ul>
<li>Si d_model es grande, este factor será pequeño, lo que tiende a reducir el learning rate general. La intuición es que modelos más grandes pueden necesitar pasos de aprendizaje más pequeños para evitar inestabilidad debido a su mayor número de parámetros.</li>
<li>Si d_model es pequeño, este factor será más grande, lo que tiende a aumentar el learning rate general.</li>
</ul></li>
</ul></li>
<li><strong><code>step_num</code> (Número de Paso de Entrenamiento):</strong>
<ul>
<li><strong>Intuición:</strong> Progreso del entrenamiento (iteración actual).</li>
<li><strong>Efecto:</strong> Determina la fase del learning rate:
<ul>
<li><strong>Fase de Decaimiento (<span class="math inline">\({step\\_num}^{-0.5}\)</span>):</strong> Este término hace que el learning rate disminuya a medida que avanza el entrenamiento. La intuición es que al principio queremos dar pasos de aprendizaje más grandes para explorar el espacio de parámetros rápidamente. A medida que nos acercamos a un buen conjunto de pesos, queremos dar pasos más pequeños para “afinar” los valores y evitar oscilar alrededor del mínimo óptimo. La disminución es más pronunciada al principio y se ralentiza con el tiempo.</li>
<li><strong>Fase de Calentamiento (<span class="math inline">\({step\\_num * warmup\\_steps}^{-1.5}\)</span>):</strong> Este término está activo principalmente durante la fase de “calentamiento”. Hace que el learning rate aumente linealmente con el número de paso hasta que step_num se acerca a warmup_steps.</li>
</ul></li>
</ul></li>
<li><strong><code>warmup_steps</code> (Pasos de Calentamiento):</strong>
<ul>
<li><strong>Intuición:</strong> Duración de la fase inicial de aumento gradual del learning rate.</li>
<li><strong>Efecto:</strong> Controla cuántos pasos se tarda en alcanzar el learning rate “base”. Ayuda a evitar inestabilidad al inicio.</li>
</ul></li>
</ol>
<p><strong>En resumen:</strong></p>
<p>El learning rate:</p>
<ul>
<li><strong>Aumenta gradualmente</strong> durante los primeros <code>warmup_steps</code> para estabilizar el entrenamiento inicial.</li>
<li><strong>Disminuye gradualmente</strong> después de <code>warmup_steps</code> para permitir una convergencia fina.</li>
<li>Su magnitud general está influenciada por la dimensionalidad del modelo (<code>d_model</code>).</li>
</ul>
<div id="cell-165" class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):</span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, warmup_steps<span class="op">=</span><span class="dv">4000</span>):</span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.d_model <span class="op">=</span> d_model</span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.d_model <span class="op">=</span> tf.cast(<span class="va">self</span>.d_model, tf.float32)</span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-8"><a href="#cb86-8" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.warmup_steps <span class="op">=</span> warmup_steps</span>
<span id="cb86-9"><a href="#cb86-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-10"><a href="#cb86-10" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, step):</span>
<span id="cb86-11"><a href="#cb86-11" aria-hidden="true" tabindex="-1"></a>    step <span class="op">=</span> tf.cast(step, dtype<span class="op">=</span>tf.float32)</span>
<span id="cb86-12"><a href="#cb86-12" aria-hidden="true" tabindex="-1"></a>    arg1 <span class="op">=</span> tf.math.rsqrt(step)</span>
<span id="cb86-13"><a href="#cb86-13" aria-hidden="true" tabindex="-1"></a>    arg2 <span class="op">=</span> step <span class="op">*</span> (<span class="va">self</span>.warmup_steps <span class="op">**</span> <span class="op">-</span><span class="fl">1.5</span>)</span>
<span id="cb86-14"><a href="#cb86-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-15"><a href="#cb86-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tf.math.rsqrt(<span class="va">self</span>.d_model) <span class="op">*</span> tf.math.minimum(arg1, arg2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Instanciar el optimizador (<code>tf.keras.optimizers.Adam</code>):</p>
<div id="cell-167" class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> CustomSchedule(d_model)</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> tf.keras.optimizers.Adam(learning_rate, beta_1<span class="op">=</span><span class="fl">0.9</span>, beta_2<span class="op">=</span><span class="fl">0.98</span>,</span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a>                                     epsilon<span class="op">=</span><span class="fl">1e-9</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Probar el scheduler para el learning rate:</p>
<div id="cell-169" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:449}}" data-outputid="6384536f-7283-48d5-dbae-a711e5f0f221" data-execution_count="55">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>plot_lr_planificador(learning_rate)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Nb_2c_Traduccion_usando_transformers_keras_files/figure-html/cell-56-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="ajustar-función-de-pérdida-y-métricas" class="level3">
<h3 class="anchored" data-anchor-id="ajustar-función-de-pérdida-y-métricas">Ajustar función de pérdida y métricas</h3>
<p>Dado que las secuencias objetivo están rellenadas (padded), es importante aplicar una máscara de padding al calcular la pérdida. Utiliza la función de pérdida de entropía cruzada (<code>tf.keras.losses.SparseCategoricalCrossentropy</code>):</p>
<div id="cell-172" class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> masked_loss(label, pred):</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>  mask <span class="op">=</span> label <span class="op">!=</span> <span class="dv">0</span> <span class="co"># Indica padding</span></span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># from_logits=True indica que las predicciones no han pasado por softmax.</span></span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># reduction='none' hace que se devuelva la pérdida por cada ejemplo.</span></span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a>  loss_object <span class="op">=</span> tf.keras.losses.SparseCategoricalCrossentropy(</span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a>      from_logits<span class="op">=</span><span class="va">True</span>, reduction<span class="op">=</span><span class="st">'none'</span>)</span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-8"><a href="#cb89-8" aria-hidden="true" tabindex="-1"></a>  loss <span class="op">=</span> loss_object(label, pred)</span>
<span id="cb89-9"><a href="#cb89-9" aria-hidden="true" tabindex="-1"></a>  mask <span class="op">=</span> tf.cast(mask, dtype<span class="op">=</span>loss.dtype)</span>
<span id="cb89-10"><a href="#cb89-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Aplica la máscara a la pérdida, multiplicando las pérdidas de las posiciones</span></span>
<span id="cb89-11"><a href="#cb89-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># de padding por cero, lo que las elimina del cálculo total.</span></span>
<span id="cb89-12"><a href="#cb89-12" aria-hidden="true" tabindex="-1"></a>  loss <span class="op">*=</span> mask</span>
<span id="cb89-13"><a href="#cb89-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-14"><a href="#cb89-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Calcula la pérdida promedio solo sobre las posiciones no enmascaradas.</span></span>
<span id="cb89-15"><a href="#cb89-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Suma todas las pérdidas y divide por el número de posiciones no enmascaradas.</span></span>
<span id="cb89-16"><a href="#cb89-16" aria-hidden="true" tabindex="-1"></a>  loss <span class="op">=</span> tf.reduce_sum(loss)<span class="op">/</span>tf.reduce_sum(mask)</span>
<span id="cb89-17"><a href="#cb89-17" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> loss</span>
<span id="cb89-18"><a href="#cb89-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-19"><a href="#cb89-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-20"><a href="#cb89-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> masked_accuracy(label, pred):</span>
<span id="cb89-21"><a href="#cb89-21" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Obtiene la clase predicha con la probabilidad más alta (el índice máximo)</span></span>
<span id="cb89-22"><a href="#cb89-22" aria-hidden="true" tabindex="-1"></a>  <span class="co"># a lo largo del eje de vocabulario (axis=2).</span></span>
<span id="cb89-23"><a href="#cb89-23" aria-hidden="true" tabindex="-1"></a>  pred <span class="op">=</span> tf.argmax(pred, axis<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb89-24"><a href="#cb89-24" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Convierte las etiquetas al mismo tipo de dato que las predicciones para comparar.</span></span>
<span id="cb89-25"><a href="#cb89-25" aria-hidden="true" tabindex="-1"></a>  label <span class="op">=</span> tf.cast(label, pred.dtype)</span>
<span id="cb89-26"><a href="#cb89-26" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Crea un tensor booleano donde True indica que la predicción coincide con la etiqueta.</span></span>
<span id="cb89-27"><a href="#cb89-27" aria-hidden="true" tabindex="-1"></a>  match <span class="op">=</span> label <span class="op">==</span> pred</span>
<span id="cb89-28"><a href="#cb89-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-29"><a href="#cb89-29" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Crea una máscara donde True indica que la etiqueta no es padding (no es 0).</span></span>
<span id="cb89-30"><a href="#cb89-30" aria-hidden="true" tabindex="-1"></a>  mask <span class="op">=</span> label <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb89-31"><a href="#cb89-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-32"><a href="#cb89-32" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Combina la máscara con las coincidencias. Solo consideramos como "match"</span></span>
<span id="cb89-33"><a href="#cb89-33" aria-hidden="true" tabindex="-1"></a>  <span class="co"># las predicciones correctas en las posiciones que no son padding.</span></span>
<span id="cb89-34"><a href="#cb89-34" aria-hidden="true" tabindex="-1"></a>  match <span class="op">=</span> match <span class="op">&amp;</span> mask</span>
<span id="cb89-35"><a href="#cb89-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-36"><a href="#cb89-36" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Convierte los booleanos de 'match' y 'mask' a float para poder calcular la media.</span></span>
<span id="cb89-37"><a href="#cb89-37" aria-hidden="true" tabindex="-1"></a>  match <span class="op">=</span> tf.cast(match, dtype<span class="op">=</span>tf.float32)</span>
<span id="cb89-38"><a href="#cb89-38" aria-hidden="true" tabindex="-1"></a>  mask <span class="op">=</span> tf.cast(mask, dtype<span class="op">=</span>tf.float32)</span>
<span id="cb89-39"><a href="#cb89-39" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Calcula la precisión promedio solo sobre las posiciones no enmascaradas.</span></span>
<span id="cb89-40"><a href="#cb89-40" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Suma las coincidencias (1 para cada predicción correcta no enmascarada)</span></span>
<span id="cb89-41"><a href="#cb89-41" aria-hidden="true" tabindex="-1"></a>  <span class="co"># y divide por el número total de posiciones no enmascaradas.</span></span>
<span id="cb89-42"><a href="#cb89-42" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> tf.reduce_sum(match)<span class="op">/</span>tf.reduce_sum(mask)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="entrenar-el-modelo" class="level3">
<h3 class="anchored" data-anchor-id="entrenar-el-modelo">Entrenar el modelo</h3>
<p>Con todo listo, vamos a compilar usando <code>model.compile</code>, y luego entrenar con <code>model.fit</code>:</p>
<div id="cell-175" class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>transformer.<span class="bu">compile</span>(</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>    loss<span class="op">=</span>masked_loss,</span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a>    optimizer<span class="op">=</span>optimizer,</span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a>    metrics<span class="op">=</span>[masked_accuracy])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-176" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="44e94123-1951-4247-eb7e-56ae8645f25e" data-execution_count="58">
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a>transformer.fit(train_batches,</span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a>                epochs<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a>                validation_data<span class="op">=</span>val_batches)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<div class="ansi-escaped-output">
<pre>Epoch 1/3

<span class="ansi-bold">810/810</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">1037s</span> 1s/step - loss: 7.7200 - masked_accuracy: 0.0833 - val_loss: 4.9830 - val_masked_accuracy: 0.2540

Epoch 2/3

<span class="ansi-bold">810/810</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">531s</span> 633ms/step - loss: 4.6781 - masked_accuracy: 0.2912 - val_loss: 3.9608 - val_masked_accuracy: 0.3668

Epoch 3/3

<span class="ansi-bold">810/810</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">424s</span> 463ms/step - loss: 3.6996 - masked_accuracy: 0.4002 - val_loss: 3.4028 - val_masked_accuracy: 0.4305
</pre>
</div>
</div>
<div class="cell-output cell-output-display" data-execution_count="58">
<pre><code>&lt;keras.src.callbacks.history.History at 0x7bffd0607690&gt;</code></pre>
</div>
</div>
</section>
</section>
<section id="ejecutar-inferencia" class="level2">
<h2 class="anchored" data-anchor-id="ejecutar-inferencia">Ejecutar inferencia</h2>
<p>Ahora puedes probar el modelo realizando una traducción. Los siguientes pasos se utilizan para la inferencia:</p>
<ul>
<li>Codifica la frase de entrada utilizando el tokenizador portugués (<code>tokenizers.pt</code>). Esta es la entrada del codificador.</li>
<li>La entrada del decodificador se inicializa con el token <code>[START]</code>.</li>
<li>Calcula las máscaras de padding y las máscaras causales (para la auto-atención causal).</li>
<li>El <code>decodificador</code> luego genera las predicciones observando la <code>salida del codificador</code> y su propia salida (auto-atención).</li>
<li>Concatena el token predicho a la entrada del decodificador y lo pasa de nuevo al decodificador.</li>
<li>En este enfoque, el decodificador predice el siguiente token basándose en los tokens que predijo previamente.</li>
</ul>
<p>Nota: El modelo está optimizado para un <em>entrenamiento eficiente</em> y realiza una predicción del siguiente token para cada token en la salida simultáneamente. Esto es redundante durante la inferencia, y solo se utiliza la última predicción. Este modelo puede hacerse más eficiente para la inferencia si solo se calcula la última predicción cuando se ejecuta en modo de inferencia (<code>training=False</code>).</p>
<p>Define la clase <code>Translator</code> extendiendo <code>tf.Module</code>:</p>
<div id="cell-179" class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Translator(tf.Module):</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, tokenizers, transformer):</span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.tokenizers <span class="op">=</span> tokenizers</span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.transformer <span class="op">=</span> transformer</span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-6"><a href="#cb93-6" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, sentence, max_length<span class="op">=</span>MAX_TOKENS):</span>
<span id="cb93-7"><a href="#cb93-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># La frase de entrada es portugués, por lo que se añaden los tokens `[START]` y `[</span><span class="re">END</span><span class="co">]`.</span></span>
<span id="cb93-8"><a href="#cb93-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="bu">isinstance</span>(sentence, tf.Tensor)</span>
<span id="cb93-9"><a href="#cb93-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(sentence.shape) <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb93-10"><a href="#cb93-10" aria-hidden="true" tabindex="-1"></a>      sentence <span class="op">=</span> sentence[tf.newaxis]</span>
<span id="cb93-11"><a href="#cb93-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-12"><a href="#cb93-12" aria-hidden="true" tabindex="-1"></a>    sentence <span class="op">=</span> <span class="va">self</span>.tokenizers.pt.tokenize(sentence).to_tensor()</span>
<span id="cb93-13"><a href="#cb93-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-14"><a href="#cb93-14" aria-hidden="true" tabindex="-1"></a>    encoder_input <span class="op">=</span> sentence</span>
<span id="cb93-15"><a href="#cb93-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-16"><a href="#cb93-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Como el lenguaje de sálida es inglés</span></span>
<span id="cb93-17"><a href="#cb93-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Inicializar con el token `[START]`</span></span>
<span id="cb93-18"><a href="#cb93-18" aria-hidden="true" tabindex="-1"></a>    start_end <span class="op">=</span> <span class="va">self</span>.tokenizers.en.tokenize([<span class="st">''</span>])[<span class="dv">0</span>]</span>
<span id="cb93-19"><a href="#cb93-19" aria-hidden="true" tabindex="-1"></a>    start <span class="op">=</span> start_end[<span class="dv">0</span>][tf.newaxis]</span>
<span id="cb93-20"><a href="#cb93-20" aria-hidden="true" tabindex="-1"></a>    end <span class="op">=</span> start_end[<span class="dv">1</span>][tf.newaxis]</span>
<span id="cb93-21"><a href="#cb93-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-22"><a href="#cb93-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># con el TensorArray es posible hacer seguimiento al blucle dinámico</span></span>
<span id="cb93-23"><a href="#cb93-23" aria-hidden="true" tabindex="-1"></a>    output_array <span class="op">=</span> tf.TensorArray(dtype<span class="op">=</span>tf.int64, size<span class="op">=</span><span class="dv">0</span>, dynamic_size<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb93-24"><a href="#cb93-24" aria-hidden="true" tabindex="-1"></a>    output_array <span class="op">=</span> output_array.write(<span class="dv">0</span>, start)</span>
<span id="cb93-25"><a href="#cb93-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-26"><a href="#cb93-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> tf.<span class="bu">range</span>(max_length):</span>
<span id="cb93-27"><a href="#cb93-27" aria-hidden="true" tabindex="-1"></a>      <span class="co"># convierte la lista de tokens generados secuencialmente</span></span>
<span id="cb93-28"><a href="#cb93-28" aria-hidden="true" tabindex="-1"></a>      <span class="co"># (almacenada en el TensorArray) en un tensor con la forma (1, secuencia_de_tokens),</span></span>
<span id="cb93-29"><a href="#cb93-29" aria-hidden="true" tabindex="-1"></a>      <span class="co"># donde la secuencia de tokens representa la traducción generada hasta ese punto.</span></span>
<span id="cb93-30"><a href="#cb93-30" aria-hidden="true" tabindex="-1"></a>      output <span class="op">=</span> tf.transpose(output_array.stack())</span>
<span id="cb93-31"><a href="#cb93-31" aria-hidden="true" tabindex="-1"></a>      predictions <span class="op">=</span> <span class="va">self</span>.transformer([encoder_input, output], training<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb93-32"><a href="#cb93-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-33"><a href="#cb93-33" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Seleccionar las predicciones para el último token de `seq_len`.</span></span>
<span id="cb93-34"><a href="#cb93-34" aria-hidden="true" tabindex="-1"></a>      predictions <span class="op">=</span> predictions[:, <span class="op">-</span><span class="dv">1</span>:, :]  <span class="co"># Shape `(batch_size, 1, vocab_size)`.</span></span>
<span id="cb93-35"><a href="#cb93-35" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Encontrar la pos del token id con la mayor probabilidad</span></span>
<span id="cb93-36"><a href="#cb93-36" aria-hidden="true" tabindex="-1"></a>      predicted_id <span class="op">=</span> tf.argmax(predictions, axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb93-37"><a href="#cb93-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-38"><a href="#cb93-38" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Concatenar la predicción con los anteriores tokens del decoder.</span></span>
<span id="cb93-39"><a href="#cb93-39" aria-hidden="true" tabindex="-1"></a>      output_array <span class="op">=</span> output_array.write(i<span class="op">+</span><span class="dv">1</span>, predicted_id[<span class="dv">0</span>])</span>
<span id="cb93-40"><a href="#cb93-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-41"><a href="#cb93-41" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> predicted_id <span class="op">==</span> end:</span>
<span id="cb93-42"><a href="#cb93-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb93-43"><a href="#cb93-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-44"><a href="#cb93-44" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> tf.transpose(output_array.stack())</span>
<span id="cb93-45"><a href="#cb93-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># La dimensión de sálida es `(1, tokens)`.</span></span>
<span id="cb93-46"><a href="#cb93-46" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> tokenizers.en.detokenize(output)[<span class="dv">0</span>]  <span class="co"># Shape: `()`.</span></span>
<span id="cb93-47"><a href="#cb93-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-48"><a href="#cb93-48" aria-hidden="true" tabindex="-1"></a>    tokens <span class="op">=</span> tokenizers.en.lookup(output)[<span class="dv">0</span>]</span>
<span id="cb93-49"><a href="#cb93-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-50"><a href="#cb93-50" aria-hidden="true" tabindex="-1"></a>    <span class="co"># `@tf.function` optimiza la función, dificultando el acceso</span></span>
<span id="cb93-51"><a href="#cb93-51" aria-hidden="true" tabindex="-1"></a>    <span class="co"># a valores dinámicos en bucles. Recalculamos la atención</span></span>
<span id="cb93-52"><a href="#cb93-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># final fuera del bucle para obtener `attention_weights`.</span></span>
<span id="cb93-53"><a href="#cb93-53" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.transformer([encoder_input, output[:,:<span class="op">-</span><span class="dv">1</span>]], training<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb93-54"><a href="#cb93-54" aria-hidden="true" tabindex="-1"></a>    attention_weights <span class="op">=</span> <span class="va">self</span>.transformer.decoder.last_attn_scores</span>
<span id="cb93-55"><a href="#cb93-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-56"><a href="#cb93-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> text, tokens, attention_weights</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Nota: Esta función utiliza un bucle unrolled, no un bucle dinámico. Genera <code>MAX_TOKENS</code> en cada llamada. Consulta el tutorial de <a href="https://www.tensorflow.org/text/tutorials/nmt_with_attention">NMT con atención</a> para ver un ejemplo de implementación con un bucle dinámico, que puede ser mucho más eficiente.</p>
<p>Probemos la traducción:</p>
<div id="cell-182" class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>translator <span class="op">=</span> Translator(tokenizers, transformer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Ejemplo 1:</p>
<div id="cell-184" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="f7461c50-e550-42ff-cf52-6d22000f6861" data-execution_count="61">
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>sentence <span class="op">=</span> <span class="st">'este é um problema que temos que resolver.'</span></span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>ground_truth <span class="op">=</span> <span class="st">'this is a problem we have to solve .'</span></span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a>translated_text, translated_tokens, attention_weights <span class="op">=</span> translator(</span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a>    tf.constant(sentence))</span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a>plot_traduccion(sentence, translated_text, ground_truth)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Input:         : este é um problema que temos que resolver.
Predicción     : this is a problem that we have to solve .
Ground truth   : this is a problem we have to solve .</code></pre>
</div>
</div>
<p>Ejemplo 2:</p>
<div id="cell-186" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="c9745211-ca89-435f-80ba-f302473539fc" data-execution_count="62">
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a>sentence <span class="op">=</span> <span class="st">'os meus vizinhos ouviram sobre esta ideia.'</span></span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a>ground_truth <span class="op">=</span> <span class="st">'and my neighboring homes heard about this idea .'</span></span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a>translated_text, translated_tokens, attention_weights <span class="op">=</span> translator(</span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a>    tf.constant(sentence))</span>
<span id="cb97-6"><a href="#cb97-6" aria-hidden="true" tabindex="-1"></a>plot_traduccion(sentence, translated_text, ground_truth)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Input:         : os meus vizinhos ouviram sobre esta ideia.
Predicción     : my friends heard about this idea .
Ground truth   : and my neighboring homes heard about this idea .</code></pre>
</div>
</div>
<p>Ejemplo 3:</p>
<div id="cell-188" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="bc4d00a5-8924-4b5f-f6cf-cde1fa92ecd9" data-execution_count="63">
<div class="sourceCode cell-code" id="cb99"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a>sentence <span class="op">=</span> <span class="st">'vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.'</span></span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a>ground_truth <span class="op">=</span> <span class="st">"so i'll just share with you some stories very quickly of some magical things that have happened."</span></span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-4"><a href="#cb99-4" aria-hidden="true" tabindex="-1"></a>translated_text, translated_tokens, attention_weights <span class="op">=</span> translator(</span>
<span id="cb99-5"><a href="#cb99-5" aria-hidden="true" tabindex="-1"></a>    tf.constant(sentence))</span>
<span id="cb99-6"><a href="#cb99-6" aria-hidden="true" tabindex="-1"></a>plot_traduccion(sentence, translated_text, ground_truth)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Input:         : vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.
Predicción     : i ' m going very quickly quickly quickly to share with some stories of things , things that will be going to be going to be going to be going to be going to be going to be able to be able to be able to be very good .
Ground truth   : so i'll just share with you some stories very quickly of some magical things that have happened.</code></pre>
</div>
</div>
</section>
<section id="crear-los-plots-de-atención" class="level2">
<h2 class="anchored" data-anchor-id="crear-los-plots-de-atención">Crear los plots de atención</h2>
<p>Usando la clase traductor <code>Translator</code> que almacena los scores de atención, podemos usarlos para ver su relevancia:</p>
<p>Por ejemplo:</p>
<div id="cell-191" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="f06ebf36-7546-49e7-ed6a-985ce946bcf4" data-execution_count="64">
<div class="sourceCode cell-code" id="cb101"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a>sentence <span class="op">=</span> <span class="st">'este é o primeiro livro que eu fiz.'</span></span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a>ground_truth <span class="op">=</span> <span class="st">"this is the first book i've ever done."</span></span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a>translated_text, translated_tokens, attention_weights <span class="op">=</span> translator(</span>
<span id="cb101-5"><a href="#cb101-5" aria-hidden="true" tabindex="-1"></a>    tf.constant(sentence))</span>
<span id="cb101-6"><a href="#cb101-6" aria-hidden="true" tabindex="-1"></a>plot_traduccion(sentence, translated_text, ground_truth)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Input:         : este é o primeiro livro que eu fiz.
Predicción     : this is the first book that i did i did .
Ground truth   : this is the first book i've ever done.</code></pre>
</div>
</div>
<p>Crear una función que grafique la atención cuando se genera un token:</p>
<div id="cell-193" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="d49ed566-cec1-4fcc-d5b4-57ee29ab6319" data-execution_count="65">
<div class="sourceCode cell-code" id="cb103"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a>head <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Shape: `(batch=1, num_heads, seq_len_q, seq_len_k)`.</span></span>
<span id="cb103-3"><a href="#cb103-3" aria-hidden="true" tabindex="-1"></a>attention_heads <span class="op">=</span> tf.squeeze(attention_weights, <span class="dv">0</span>)</span>
<span id="cb103-4"><a href="#cb103-4" aria-hidden="true" tabindex="-1"></a>attention <span class="op">=</span> attention_heads[head]</span>
<span id="cb103-5"><a href="#cb103-5" aria-hidden="true" tabindex="-1"></a>attention.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="65">
<pre><code>TensorShape([12, 11])</code></pre>
</div>
</div>
<p>Son las inputs tokens en portugués:</p>
<div id="cell-195" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="da56e799-d926-4749-a04f-592cad4a3661" data-execution_count="66">
<div class="sourceCode cell-code" id="cb105"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a>in_tokens <span class="op">=</span> tf.convert_to_tensor([sentence])</span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a>in_tokens <span class="op">=</span> tokenizers.pt.tokenize(in_tokens).to_tensor()</span>
<span id="cb105-3"><a href="#cb105-3" aria-hidden="true" tabindex="-1"></a>in_tokens <span class="op">=</span> tokenizers.pt.lookup(in_tokens)[<span class="dv">0</span>]</span>
<span id="cb105-4"><a href="#cb105-4" aria-hidden="true" tabindex="-1"></a>in_tokens</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="66">
<pre><code>&lt;tf.Tensor: shape=(11,), dtype=string, numpy=
array([b'[START]', b'este', b'e', b'o', b'primeiro', b'livro', b'que',
       b'eu', b'fiz', b'.', b'[END]'], dtype=object)&gt;</code></pre>
</div>
</div>
<p>Estas son las sálidas (tokens en inglés, traducción)</p>
<div id="cell-197" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="aaaef1e6-7d46-4301-9c94-490692ae5336" data-execution_count="67">
<div class="sourceCode cell-code" id="cb107"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a>translated_tokens</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="67">
<pre><code>&lt;tf.Tensor: shape=(13,), dtype=string, numpy=
array([b'[START]', b'this', b'is', b'the', b'first', b'book', b'that',
       b'i', b'did', b'i', b'did', b'.', b'[END]'], dtype=object)&gt;</code></pre>
</div>
</div>
<div id="cell-198" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:381}}" data-outputid="e66df5bd-88e0-49d5-b3a1-e289255e7b6f" data-execution_count="68">
<div class="sourceCode cell-code" id="cb109"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a>plot_attention_weights(sentence,</span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a>                       translated_tokens,</span>
<span id="cb109-3"><a href="#cb109-3" aria-hidden="true" tabindex="-1"></a>                       attention_weights[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Nb_2c_Traduccion_usando_transformers_keras_files/figure-html/cell-69-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="exportar-el-modelo" class="level2">
<h2 class="anchored" data-anchor-id="exportar-el-modelo">Exportar el modelo</h2>
<p>Hemos probado el modelo y la inferencia funciona. A continuación, puedes exportarlo como un <code>tf.saved_model</code>. Para aprender cómo guardar y cargar un modelo en formato SavedModel, consulta <a href="https://www.tensorflow.org/guide/saved_model">esta guía</a>.</p>
<p>Crea una clase llamada <code>ExportTranslator</code> extendiendo la subclase <code>tf.Module</code> con un <code>@tf.function</code> en el método <code>__call__</code>:</p>
<div id="cell-201" class="cell" data-execution_count="69">
<div class="sourceCode cell-code" id="cb110"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ExportTranslator(tf.Module):</span>
<span id="cb110-2"><a href="#cb110-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, translator):</span>
<span id="cb110-3"><a href="#cb110-3" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.translator <span class="op">=</span> translator</span>
<span id="cb110-4"><a href="#cb110-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-5"><a href="#cb110-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">@tf.function</span>(input_signature<span class="op">=</span>[tf.TensorSpec(shape<span class="op">=</span>[], dtype<span class="op">=</span>tf.string)])</span>
<span id="cb110-6"><a href="#cb110-6" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, sentence):</span>
<span id="cb110-7"><a href="#cb110-7" aria-hidden="true" tabindex="-1"></a>    (result,</span>
<span id="cb110-8"><a href="#cb110-8" aria-hidden="true" tabindex="-1"></a>     tokens,</span>
<span id="cb110-9"><a href="#cb110-9" aria-hidden="true" tabindex="-1"></a>     attention_weights) <span class="op">=</span> <span class="va">self</span>.translator(sentence, max_length<span class="op">=</span>MAX_TOKENS)</span>
<span id="cb110-10"><a href="#cb110-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-11"><a href="#cb110-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> result</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-202" class="cell" data-execution_count="70">
<div class="sourceCode cell-code" id="cb111"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Empaqueta el objeto `translator` en la nueva clase `ExportTranslator` creada:</span></span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a>translator <span class="op">=</span> ExportTranslator(translator)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Dado que el modelo está decodificando las predicciones utilizando <code>tf.argmax</code>, las predicciones son deterministas. El modelo original y uno recargado desde su <code>SavedModel</code> deberían dar predicciones idénticas:</p>
<div id="cell-204" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="32588aad-fd46-4ea7-9939-53f49ca4515d" data-execution_count="71">
<div class="sourceCode cell-code" id="cb112"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a>translator(<span class="st">'este é o primeiro livro que eu fiz.'</span>).numpy()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="71">
<pre><code>b'this is the first book that i did i did .'</code></pre>
</div>
</div>
<div id="cell-205" class="cell" data-execution_count="72">
<div class="sourceCode cell-code" id="cb114"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a>tf.saved_model.save(translator, export_dir<span class="op">=</span><span class="st">'translator'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-206" class="cell" data-execution_count="73">
<div class="sourceCode cell-code" id="cb115"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a>reloaded <span class="op">=</span> tf.saved_model.load(<span class="st">'translator'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-207" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="3e6dbbd3-014b-489d-c341-485bbdff84d8" data-execution_count="74">
<div class="sourceCode cell-code" id="cb116"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a>reloaded(<span class="st">'este é o primeiro livro que eu fiz.'</span>).numpy()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="74">
<pre><code>b'this is the first book that i did i did .'</code></pre>
</div>
</div>
</section>
<section id="conclusión" class="level2">
<h2 class="anchored" data-anchor-id="conclusión">Conclusión</h2>
<p>En este tutorial aprendiste sobre:</p>
<ul>
<li>Los Transformers y su importancia en el aprendizaje automático</li>
<li>Atención, auto-atención y atención multi-cabeza</li>
<li>Codificación posicional con embeddings</li>
<li>La arquitectura codificador-decodificador del Transformer original</li>
<li>Enmascaramiento en la auto-atención</li>
<li>Cómo juntar todo para traducir texto</li>
</ul>
<p>Las desventajas de esta arquitectura son:</p>
<ul>
<li>Para una serie temporal, la salida para un paso de tiempo se calcula a partir de la <em>historia completa</em> en lugar de solo las entradas y el estado oculto actual. Esto <em>podría</em> ser menos eficiente.</li>
<li>Si la entrada tiene una relación temporal/espacial, como texto o imágenes, debe añadirse alguna codificación posicional o el modelo efectivamente verá una bolsa de palabras.</li>
</ul>
<p><em>Este notebook se basó en el notebook de <a href="https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb">Neural Machine Translation with a Transformer and Keras</a> para el curso de Deep Learning práctico en 3 semanas.</em></p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/jeffersonrodriguezc/deep-learning-en-3-semanas/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>