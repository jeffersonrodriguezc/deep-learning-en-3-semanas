[
  {
    "objectID": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html",
    "href": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html",
    "title": "IntroducciÃ³n a la explicabilidad e interpretabilidad en modelos",
    "section": "",
    "text": "Open In Colab\nEn este notebook encontrarÃ¡s material introductorio para entender los conceptos de expicabilidad e interpretabilidad en modelos de inteligencia artificial.\nAbordaremos el siguiente paso a paso:\n#@title Importar librerÃ­as\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Input\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets\nfrom tensorflow.keras.preprocessing import image\n#@title Importar functions\ndef show_images(image_titles, images, cmap='viridis'):\n    f, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n    for i, title in enumerate(image_titles):\n        ax[i].set_title(title, fontsize=16)\n        ax[i].imshow(images[i], cmap=cmap)\n        ax[i].axis('off')\n    plt.tight_layout()\n    plt.show()\n\ndef show_images_with_heatmap(image_titles, images, cam, cmap='viridis'):\n    f, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n    for i, title in enumerate(image_titles):\n        heatmap = np.uint8(cm.jet(cam[i])[..., :3] * 255)\n        ax[i].set_title(title, fontsize=16)\n        ax[i].imshow(images[i], cmap=cmap)\n        ax[i].imshow(heatmap, cmap='jet', alpha=0.5)  # overlay\n        ax[i].axis('off')\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html#explicabilidad-en-imÃ¡genes",
    "href": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html#explicabilidad-en-imÃ¡genes",
    "title": "IntroducciÃ³n a la explicabilidad e interpretabilidad en modelos",
    "section": "Explicabilidad en ImÃ¡genes",
    "text": "Explicabilidad en ImÃ¡genes\n\n!pip install tf-keras-vis tensorflow\n\n\nCollecting tf-keras-vis\n\n  Downloading tf_keras_vis-0.8.7-py3-none-any.whl.metadata (10 kB)\n\nRequirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from tf-keras-vis) (1.16.0)\n\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from tf-keras-vis) (11.3.0)\n\nCollecting deprecated (from tf-keras-vis)\n\n  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n\nRequirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (from tf-keras-vis) (2.37.0)\n\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tf-keras-vis) (25.0)\n\nRequirement already satisfied: absl-py&gt;=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n\nRequirement already satisfied: astunparse&gt;=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n\nRequirement already satisfied: flatbuffers&gt;=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,&gt;=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n\nRequirement already satisfied: google-pasta&gt;=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n\nRequirement already satisfied: libclang&gt;=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n\nRequirement already satisfied: opt-einsum&gt;=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,&lt;6.0.0dev,&gt;=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n\nRequirement already satisfied: requests&lt;3,&gt;=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n\nRequirement already satisfied: six&gt;=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n\nRequirement already satisfied: termcolor&gt;=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n\nRequirement already satisfied: typing-extensions&gt;=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n\nRequirement already satisfied: wrapt&gt;=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n\nRequirement already satisfied: grpcio&lt;2.0,&gt;=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.74.0)\n\nRequirement already satisfied: tensorboard&lt;2.19,&gt;=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n\nRequirement already satisfied: keras&gt;=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n\nRequirement already satisfied: numpy&lt;2.1.0,&gt;=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n\nRequirement already satisfied: h5py&gt;=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n\nRequirement already satisfied: ml-dtypes&lt;0.5.0,&gt;=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n\nRequirement already satisfied: tensorflow-io-gcs-filesystem&gt;=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n\nRequirement already satisfied: wheel&lt;1.0,&gt;=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse&gt;=1.6.0-&gt;tensorflow) (0.45.1)\n\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras&gt;=3.5.0-&gt;tensorflow) (13.9.4)\n\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras&gt;=3.5.0-&gt;tensorflow) (0.1.0)\n\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras&gt;=3.5.0-&gt;tensorflow) (0.17.0)\n\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.11/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow) (3.4.2)\n\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.11/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow) (3.10)\n\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow) (2.5.0)\n\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow) (2025.7.14)\n\nRequirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard&lt;2.19,&gt;=2.18-&gt;tensorflow) (3.8.2)\n\nRequirement already satisfied: tensorboard-data-server&lt;0.8.0,&gt;=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard&lt;2.19,&gt;=2.18-&gt;tensorflow) (0.7.2)\n\nRequirement already satisfied: werkzeug&gt;=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard&lt;2.19,&gt;=2.18-&gt;tensorflow) (3.1.3)\n\nRequirement already satisfied: MarkupSafe&gt;=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug&gt;=1.0.1-&gt;tensorboard&lt;2.19,&gt;=2.18-&gt;tensorflow) (3.0.2)\n\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich-&gt;keras&gt;=3.5.0-&gt;tensorflow) (3.0.0)\n\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich-&gt;keras&gt;=3.5.0-&gt;tensorflow) (2.19.2)\n\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras&gt;=3.5.0-&gt;tensorflow) (0.1.2)\n\nDownloading tf_keras_vis-0.8.7-py3-none-any.whl (52 kB)\n\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 52.5/52.5 kB 3.0 MB/s eta 0:00:00\n\nDownloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n\nInstalling collected packages: deprecated, tf-keras-vis\n\nSuccessfully installed deprecated-1.2.18 tf-keras-vis-0.8.7\n\n\n\n\nEl primer paso es tener listo nuestra red convolucional, pre-cargada o entrenada desde cero.\n\n# Primero cargamos el modelo\nmodel =tf.keras.applications.vgg16.VGG16(weights='imagenet', include_top=True)\nmodel.summary()\n\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n\n553467096/553467096 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 0us/step\n\n\n\n\nModel: \"vgg16\"\n\n\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ Layer (type)                    â”ƒ Output Shape           â”ƒ       Param # â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ input_layer (InputLayer)        â”‚ (None, 224, 224, 3)    â”‚             0 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block1_conv1 (Conv2D)           â”‚ (None, 224, 224, 64)   â”‚         1,792 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block1_conv2 (Conv2D)           â”‚ (None, 224, 224, 64)   â”‚        36,928 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block1_pool (MaxPooling2D)      â”‚ (None, 112, 112, 64)   â”‚             0 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block2_conv1 (Conv2D)           â”‚ (None, 112, 112, 128)  â”‚        73,856 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block2_conv2 (Conv2D)           â”‚ (None, 112, 112, 128)  â”‚       147,584 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block2_pool (MaxPooling2D)      â”‚ (None, 56, 56, 128)    â”‚             0 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block3_conv1 (Conv2D)           â”‚ (None, 56, 56, 256)    â”‚       295,168 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block3_conv2 (Conv2D)           â”‚ (None, 56, 56, 256)    â”‚       590,080 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block3_conv3 (Conv2D)           â”‚ (None, 56, 56, 256)    â”‚       590,080 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block3_pool (MaxPooling2D)      â”‚ (None, 28, 28, 256)    â”‚             0 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block4_conv1 (Conv2D)           â”‚ (None, 28, 28, 512)    â”‚     1,180,160 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block4_conv2 (Conv2D)           â”‚ (None, 28, 28, 512)    â”‚     2,359,808 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block4_conv3 (Conv2D)           â”‚ (None, 28, 28, 512)    â”‚     2,359,808 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block4_pool (MaxPooling2D)      â”‚ (None, 14, 14, 512)    â”‚             0 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block5_conv1 (Conv2D)           â”‚ (None, 14, 14, 512)    â”‚     2,359,808 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block5_conv2 (Conv2D)           â”‚ (None, 14, 14, 512)    â”‚     2,359,808 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block5_conv3 (Conv2D)           â”‚ (None, 14, 14, 512)    â”‚     2,359,808 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block5_pool (MaxPooling2D)      â”‚ (None, 7, 7, 512)      â”‚             0 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ flatten (Flatten)               â”‚ (None, 25088)          â”‚             0 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ fc1 (Dense)                     â”‚ (None, 4096)           â”‚   102,764,544 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ fc2 (Dense)                     â”‚ (None, 4096)           â”‚    16,781,312 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ predictions (Dense)             â”‚ (None, 1000)           â”‚     4,097,000 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n Total params: 138,357,544 (527.79 MB)\n\n\n\n Trainable params: 138,357,544 (527.79 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# Cargar imÃ¡genes\n# Diccionario de nombres y URLs pÃºblicas\nimage_info = {\n    'goldfish.jpg': 'https://images.unsplash.com/photo-1668862347626-70a980820f06?q=80&w=1170&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D',\n    'bear.jpg': 'https://plus.unsplash.com/premium_photo-1664298010187-091137b9e296?q=80&w=687&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D',\n    'soldier.jpg': 'https://plus.unsplash.com/premium_photo-1683133493443-0eee93bfe24f?q=80&w=1170&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D'\n}\n\nimage_titles = ['goldfish', 'bear', 'Assault rifle']\nloaded_images = []\n\nfor name, url in image_info.items():\n    path = tf.keras.utils.get_file(name, origin=url)\n    img = image.load_img(path, target_size=(224, 224))\n    loaded_images.append(np.array(img))\n\n# convertir a numpy y organizarlas en un batch\nimages = np.asarray(loaded_images)\n\n# Preparar la imÃ¡genes como entrada para VGG16\nX = tf.keras.applications.vgg16.preprocess_input(images)\n\n# Mostrar imÃ¡genes\nshow_images(image_titles, images, cmap='viridis')\n\n\n\n\n\n\n\n\nğŸ“Œ Nota sobre la funciÃ³n de activaciÃ³n en la Ãºltima capa.\nCuando se aplica la funciÃ³n de activaciÃ³n softmax en la Ãºltima capa del modelo, esto puede interferir con la generaciÃ³n de mapas de atenciÃ³n (attention maps). Por esta razÃ³n, es recomendable reemplazarla por una funciÃ³n de activaciÃ³n lineal (es decir, sin activaciÃ³n).\nEn este ejemplo usamos una clase llamada ReplaceToLinear para hacer esa sustituciÃ³n automÃ¡ticamente. Sin embargo, tambiÃ©n es posible definir nuestra propia funciÃ³n modificadora del modelo si se desea mÃ¡s control.\n\nfrom tf_keras_vis.utils.model_modifiers import ReplaceToLinear\n\nreplace2linear = ReplaceToLinear()\n\n# TambiÃ©n es posible definir una funciÃ³n totalmente desde cero\ndef model_modifier_function(cloned_model):\n    cloned_model.layers[-1].activation = tf.keras.activations.linear\n\nğŸ¯ Definir la funciÃ³n de puntuaciÃ³n (score)\nLuego, es necesario crear una instancia de Score o definir una funciÃ³n personalizada de puntuaciÃ³n que devuelva los valores objetivo (scores) del modelo.\nEn este caso, la funciÃ³n devuelve las puntuaciones correspondientes a las clases de interÃ©s: Goldfish, Bear y Assault Rifle.\nEsto es fundamental para indicar al mÃ©todo de visualizaciÃ³n (como Grad-CAM) quÃ© clase especÃ­fica queremos analizar en la imagen.\n\nfrom tf_keras_vis.utils.scores import CategoricalScore\n\n# clase 1 es el index de imagenet correspondiente a Goldfish\n# Clase 294 a Bear\n# Clase 413 a Assault Rifle\n\nscore = CategoricalScore([1, 294, 413])\n\n# Para definir un score tambiÃ©n es posible hacerlo desde cero\ndef score_function(output):\n    # La `output` refiere a la salida del modelo,\n    # En este caso, `output` es `(3, 1000)`\n    return (output[0][1], output[1][294], output[2][413])\n\nğŸ§  Â¿QuÃ© es un Saliency Map?\nSaliency genera un saliency map (mapa de saliencia) que resalta las regiones de la imagen de entrada que mÃ¡s contribuyen al valor de salida del modelo.\nEn otras palabras, muestra quÃ© partes de la imagen fueron mÃ¡s importantes para que el modelo tomara su decisiÃ³n (por ejemplo, clasificar como â€œosoâ€ o â€œpez doradoâ€).\nEste tipo de visualizaciÃ³n es Ãºtil para interpretar y explicar el comportamiento de redes neuronales profundas.\n\nfrom tf_keras_vis.saliency import Saliency\n\n# Crear un objeto de saliencia.\nsaliency = Saliency(model, model_modifier=replace2linear, clone=True)\n\n# Generar el mapa de saliencia\nsaliency_map = saliency(score, X)\n\n# Mostrar imÃ¡genes\nshow_images(image_titles, saliency_map, cmap='jet')\n\n/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\nExpected: keras_tensor\nReceived: inputs=['Tensor(shape=(3, 224, 224, 3))']\n  warnings.warn(msg)\n\n\n\n\n\n\n\n\n\nğŸ§¹ Mejora del Saliency Map con SmoothGrad\nComo puedes ver en el ejemplo anterior, el mapa de saliencia (Vanilla Saliency) suele ser muy ruidoso. Para mejorar su claridad, usaremos SmoothGrad, una tÃ©cnica que reduce el ruido agregando pequeÃ±as perturbaciones (ruido) a la imagen de entrada y promediando los mapas resultantes.\nEsto permite obtener un mapa de saliencia mÃ¡s suave y mÃ¡s interpretable.\nğŸ“Œ Nota: SmoothGrad necesita calcular el gradiente muchas veces, por lo que puede tardar 1 a 3 minutos si se usa CPU. Se recomienda ejecutar en GPU si es posible.\n\n# Generate saliency map with smoothing that reduce noise by adding noise\nsaliency_map = saliency(\n    score,\n    X,\n    smooth_samples=20,  # El nÃºmero de iteraciones para calcular los gradientes.\n    smooth_noise=0.20)  # nivel de ruido aplicado.\n\n# Mostrar imÃ¡genes\nshow_images(image_titles, saliency_map, cmap='jet')\n\n\n\n\n\n\n\n\nğŸ‘€ VisualizaciÃ³n de la atenciÃ³n: Grad-CAM\nSaliency es una tÃ©cnica Ãºtil para visualizar la atenciÃ³n del modelo, ya que muestra quÃ© regiones de la imagen de entrada contribuyen mÃ¡s al valor de salida (por ejemplo, a la clasificaciÃ³n final).\nOtra forma popular de visualizar la atenciÃ³n es Grad-CAM (Gradient-weighted Class Activation Mapping).\nA diferencia de Saliency, que utiliza directamente los gradientes del valor de salida, Grad-CAM se basa en la salida de la capa convolucional previa a las capas densas (tambiÃ©n conocida como penultimate layer). Esto permite generar mapas de atenciÃ³n mÃ¡s localizados y visualmente interpretables, sobre todo en modelos CNN.\n\nfrom matplotlib import cm\nfrom tf_keras_vis.gradcam import Gradcam\n\n# Crear el objeto GRAD-CAM\ngradcam = Gradcam(model, model_modifier=replace2linear, clone=True)\n\n# Generar un mapa de calor con GradCAM\ncam = gradcam(score, X, penultimate_layer=-1)\n\n# Mostrar imÃ¡genes\nshow_images_with_heatmap(image_titles, images, cam, cmap='jet')\n\n\n\n\n\n\n\n\nğŸ” Grad-CAM++: AtenciÃ³n mejorada\nGrad-CAM++ es una extensiÃ³n de Grad-CAM que puede proporcionar explicaciones visuales mÃ¡s precisas y detalladas sobre las predicciones de un modelo CNN.\nEsta tÃ©cnica mejora la localizaciÃ³n de las regiones importantes, especialmente en situaciones donde hay mÃºltiples objetos o detalles finos en la imagen. Es Ãºtil cuando se desea una interpretaciÃ³n mÃ¡s fina que la que ofrece Grad-CAM estÃ¡ndar.\n\nfrom tf_keras_vis.gradcam_plus_plus import GradcamPlusPlus\n\n# Crear el objeto GradCAM++\ngradcam = GradcamPlusPlus(model, model_modifier=replace2linear, clone=True)\n\n# Generar el mapa con GradCAM++\ncam = gradcam(score, X, penultimate_layer=-1)\n\n# Mostrar imÃ¡genes\nshow_images_with_heatmap(image_titles, images, cam, cmap='jet')\n\n\n\n\n\n\n\n\nâš¡ï¸ Faster-ScoreCAM\nScoreCAM es un mÃ©todo muy eficaz para visualizar la atenciÃ³n de los modelos, pero suele ser mÃ¡s lento que otras variantes de CAM (como Grad-CAM o Grad-CAM++).\nLa buena noticia es que existe una versiÃ³n optimizada llamada Faster-ScoreCAM, que mejora el rendimiento de ScoreCAM sin perder precisiÃ³n.\nğŸ§  La idea detrÃ¡s de Faster-ScoreCAM es que solo algunos canales del mapa de activaciones son realmente relevantes para generar el mapa final. Por eso, Faster-ScoreCAM usa solo los canales con mayor varianza como mÃ¡scaras, lo que reduce significativamente el tiempo de procesamiento.\nğŸ”§ Nota tÃ©cnica: Establecer max_N = -1 utiliza el comportamiento original de Score-CAM (es decir, sin filtrado por varianza).\n\nfrom tf_keras_vis.scorecam import Scorecam\n\n# Create ScoreCAM object\nscorecam = Scorecam(model, model_modifier=replace2linear)\n\n# Crear el mapa de calor con Faster-ScoreCAM\ncam = scorecam(score, X, penultimate_layer=-1, max_N=10)\n\n# Mostrar imÃ¡genes\nshow_images_with_heatmap(image_titles, images, cam, cmap='jet')\n\n/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\nExpected: keras_tensor\nReceived: inputs=('Tensor(shape=(32, 224, 224, 3))',)\n  warnings.warn(msg)\n\n\n\n2/3 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 159ms/step\n\n\n\n/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\nExpected: keras_tensor\nReceived: inputs=('Tensor(shape=(None, 224, 224, 3))',)\n  warnings.warn(msg)\nWARNING:tensorflow:5 out of the last 54 calls to &lt;function TensorFlowTrainer.make_predict_function.&lt;locals&gt;.one_step_on_data_distributed at 0x795ca4526e80&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n\n\n\n3/3 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 373ms/step"
  },
  {
    "objectID": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html#explicabilidad-en-texto-y-datos-tabulares",
    "href": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html#explicabilidad-en-texto-y-datos-tabulares",
    "title": "IntroducciÃ³n a la explicabilidad e interpretabilidad en modelos",
    "section": "Explicabilidad en texto y datos tabulares",
    "text": "Explicabilidad en texto y datos tabulares\n\n!pip install -q lime\n\n\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0.0/275.7 kB ? eta -:--:--\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸â” 266.2/275.7 kB 11.0 MB/s eta 0:00:01\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 275.7/275.7 kB 5.6 MB/s eta 0:00:00\n\n  Preparing metadata (setup.py) ... done\n\n  Building wheel for lime (setup.py) ... done\n\n\n\n\n\n#importamos las librerias necesarias a utilizar para explicabilidad\nimport shap\nfrom lime.lime_tabular import LimeTabularExplainer\n\n%matplotlib inline"
  },
  {
    "objectID": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html#dataset",
    "href": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html#dataset",
    "title": "IntroducciÃ³n a la explicabilidad e interpretabilidad en modelos",
    "section": "Dataset",
    "text": "Dataset\nEl dataset cuenta con 178 registros, cada uno con 13 caracteristicas:\n\nAlcohol\nMalic Acid\nAsh\nAlcalinity of Ash\nMagnesium\nTotal Phenols\nFlavanoids\nNonflavanoid Phenols\nProanthocyanins\nColour Intensity\nHue\nOD280/OD315 of diluted wines\nProline\n\nEl dataset contiene 3 clases diferentes: Class_1, Class_2,Class_3\n\n#Cargamos el conjunto de datos y procesamos\nwine = load_wine()\nX, y = wine.data, wine.target\n\n# dividimos los datos\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size=0.3,\n                                                    random_state=42,\n                                                    stratify=y)\n\n# convertimos y a one hot encoded vector\none_hot_encoder = OneHotEncoder(sparse_output=False)\ny_train = one_hot_encoder.fit_transform(y_train.reshape(-1, 1))\ny_test = one_hot_encoder.transform(y_test.reshape(-1, 1))\n\n# escalamos los datos\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nprint(\"Dimension datos de entrenamiento: \", X_train.shape)\nprint(\"Dimension datos de prueba: \", X_test.shape)\n\nDimension datos de entrenamiento:  (124, 13)\nDimension datos de prueba:  (54, 13)"
  },
  {
    "objectID": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html#modelo",
    "href": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html#modelo",
    "title": "IntroducciÃ³n a la explicabilidad e interpretabilidad en modelos",
    "section": "Modelo",
    "text": "Modelo\n\n#Definimos la red neuronal a entrenar y compilamos el modelo\nmodel = Sequential()\n\nmodel.add(Input(shape=(X_train.shape[1],)))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(y_train.shape[1], activation='softmax'))\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n\n#Entrenamos la red\nhistory = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=1)\n\n\nEpoch 1/20\n\n4/4 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 363ms/step - accuracy: 0.4157 - loss: 1.0442 - val_accuracy: 0.7200 - val_loss: 0.8392\n\nEpoch 2/20\n\n4/4 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 20ms/step - accuracy: 0.5802 - loss: 0.9172 - val_accuracy: 0.8800 - val_loss: 0.7460\n\nEpoch 3/20\n\n4/4 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 26ms/step - accuracy: 0.5664 - loss: 0.9099 - val_accuracy: 0.9600 - val_loss: 0.6597\n\nEpoch 4/20\n\n4/4 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 21ms/step - accuracy: 0.7142 - loss: 0.7492 - val_accuracy: 0.9600 - val_loss: 0.5856\n\nEpoch 5/20\n\n4/4 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 18ms/step - accuracy: 0.8364 - loss: 0.6740 - val_accuracy: 0.9600 - val_loss: 0.5215\n\nEpoch 6/20\n\n4/4 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 22ms/step - accuracy: 0.7955 - loss: 0.6507 - val_accuracy: 0.9600 - val_loss: 0.4634\n\nEpoch 7/20\n\n4/4 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 41ms/step - accuracy: 0.8352 - loss: 0.6070 - val_accuracy: 0.9600 - val_loss: 0.4121\n\nEpoch 8/20\n\n4/4 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 39ms/step - accuracy: 0.9290 - loss: 0.5245 - val_accuracy: 0.9600 - val_loss: 0.3665\n\nEpoch 9/20\n\n4/4 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 28ms/step - accuracy: 0.8888 - loss: 0.5035 - val_accuracy: 0.9600 - val_loss: 0.3268\n\nEpoch 10/20\n\n4/4 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 39ms/step - accuracy: 0.8951 - loss: 0.4608 - val_accuracy: 0.9600 - val_loss: 0.2910\n\nEpoch 11/20\n\n4/4 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 38ms/step - accuracy: 0.9351 - loss: 0.4075 - val_accuracy: 0.9600 - val_loss: 0.2605\n\nEpoch 12/20\n\n4/4 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 42ms/step - accuracy: 0.9570 - loss: 0.3141 - val_accuracy: 0.9600 - val_loss: 0.2323\n\nEpoch 13/20\n\n4/4 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 41ms/step - accuracy: 0.9350 - loss: 0.3289 - val_accuracy: 0.9600 - val_loss: 0.2072\n\nEpoch 14/20\n\n4/4 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 42ms/step - accuracy: 0.9445 - loss: 0.2903 - val_accuracy: 0.9600 - val_loss: 0.1856\n\nEpoch 15/20\n\n4/4 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 21ms/step - accuracy: 0.9784 - loss: 0.2655 - val_accuracy: 0.9600 - val_loss: 0.1673\n\nEpoch 16/20\n\n4/4 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 20ms/step - accuracy: 0.9753 - loss: 0.2490 - val_accuracy: 0.9600 - val_loss: 0.1537\n\nEpoch 17/20\n\n4/4 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 19ms/step - accuracy: 0.9691 - loss: 0.2239 - val_accuracy: 0.9600 - val_loss: 0.1403\n\nEpoch 18/20\n\n4/4 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 20ms/step - accuracy: 0.9784 - loss: 0.1995 - val_accuracy: 0.9600 - val_loss: 0.1281\n\nEpoch 19/20\n\n4/4 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 20ms/step - accuracy: 0.9878 - loss: 0.1343 - val_accuracy: 0.9600 - val_loss: 0.1183\n\nEpoch 20/20\n\n4/4 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 19ms/step - accuracy: 0.9753 - loss: 0.1496 - val_accuracy: 0.9600 - val_loss: 0.1111\n\n\n\n\n\ny_pred = model.predict(X_test)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_test_classes = np.argmax(y_test, axis=1)\n\naccuracy = accuracy_score(y_test_classes, y_pred_classes)\nprint(f\"Accuracy en el conjunto de prueba: {accuracy:.4f}\")\n\n\n2/2 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 123ms/step\n\nAccuracy en el conjunto de prueba: 1.0000\n\n\n\n\n\nplt.figure(figsize=(6, 3))\nplt.plot(history.history['accuracy'], label='accuray en entrenamiento')\nplt.plot(history.history['val_accuracy'], label='accuracy de validaciÃ³n')\nplt.xlabel('Ã©pocas')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('Rendimiento del modelo durante el entrenamiento')\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nYa tenemos un MLP o red densa que nos predice la clase del vino bastante bien. Sin embargo, lo que no estÃ¡ muy claro es cÃ³mo cada una de estas caracterÃ­sticas contribuye a la probabilidad de clase de vino predicha. Podemos pensar en estas explicaciones en tÃ©rminos globales (es decir, Â¿cÃ³mo impacta cada caracterÃ­stica en los resultados en promedio para todo los datos?) o en tÃ©rminos locales (es decir, Â¿cÃ³mo impacta cada caracterÃ­stica en las predicciones para una muestra en especÃ­fico?).\nAlgunos modelos tienen propiedades incorporadas que proporcionan este tipo de explicaciones. Estos se conocen tÃ­picamente como modelos de caja blanca (white-box) y los ejemplos incluyen la regresiÃ³n lineal (coeficientes del modelo), la regresiÃ³n logÃ­stica (coeficientes del modelo) y los Ã¡rboles de decisiÃ³n (importancia de las caracterÃ­sticas). Debido a su complejidad, otros modelos, como las MÃ¡quinas de Vectores de Soporte (SVM) y las Redes Neuronales (incluyendo nuestro PerceptrÃ³n Multicapa) etc., no tienen mÃ©todos directos para explicar sus predicciones. Para estos modelos (tambiÃ©n conocidos como modelos de caja negra (black-box)), se pueden aplicar enfoques como LIME y SHAP."
  },
  {
    "objectID": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html#lime-para-datos-tabulares",
    "href": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html#lime-para-datos-tabulares",
    "title": "IntroducciÃ³n a la explicabilidad e interpretabilidad en modelos",
    "section": "LIME para datos tabulares",
    "text": "LIME para datos tabulares\nLocal Interpretable Model-agnostic Explanation (LIME) proporciona un mÃ©todo rÃ¡pido y relativamente simple para explicar localmente modelos de caja negra. El algoritmo LIME se puede simplificar en unos pocos pasos:\n\nPara un punto de datos dado, perturba aleatoriamente sus caracterÃ­sticas repetidamente. Para datos tabulares, esto implica agregar una pequeÃ±a cantidad de ruido a cada caracterÃ­stica.\nObtÃ©n predicciones para cada instancia de datos perturbada. Esto nos ayuda a construir una imagen local de la superficie de decisiÃ³n en ese punto.\nUsa las predicciones para calcular un â€œmodelo de explicaciÃ³nâ€ lineal aproximado utilizando las predicciones. Los coeficientes del modelo lineal se utilizan como explicaciones.\n\nLa librerÃ­a de Python LIME proporciona interfaces para explicar modelos construidos sobre datos tabulares (TabularExplainer), imÃ¡genes (LimeImageExplainer) y texto (LimeTextExplainer).\nEn la siguiente secciÃ³n, intentaremos explicar las predicciones de una Ãºnica instancia de datos de prueba utilizando LimeTabularExplainer\n\ndata_df = pd.DataFrame(X_train,\n                      columns=wine.feature_names)\n\nlime_explainer = LimeTabularExplainer(training_data=data_df.values,\n                                      feature_names=list(data_df.columns),\n                                      class_names = ['Class_1', 'Class_2', 'Class_3'],\n                                      # bÃ¡sicamente no tenemos datos\n                                      # categÃ³ricos\n                                      categorical_features=[],\n                                      mode=\"classification\")\n\n\nindex = 9\nexplanation = lime_explainer.explain_instance(X_test[index],\n                                              model.predict,\n                                              num_features=len(wine.feature_names),\n                                              top_labels=2)\n\nprint(f\"ExplicaciÃ³n para la muestra con Ã­ndice: {index} con clase verdadera: class_{np.argmax(y_test[index])+1}\")\nexplanation.show_in_notebook(show_table=True, show_all=True,)\n\n\n157/157 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 2ms/step\n\nExplicaciÃ³n para la muestra con Ã­ndice: 9 con clase verdadera: class_3\n\n\n\n\n\n        \n        \n        \n        \n        \n        \n\n\nNOTA: Esta nota es para recordar que el rendimiento de este algoritmo no es plenamente confiable ya que se basa en que tanto el modelo lineal que LIME crea pueda explicar el modelo NO LINEAL original. AsÃ­ que, las capacidades de explicabilidad y confianza son limitadas. En la vida real, habrÃ¡ que explorar crear mejores aproximaciones. Sin embargo seguirÃ¡ siendo una limitante grande.\nPara tratar con este problema, es posible hacer uso de algunos atributos de LIME:\nexplanation.intercept\nexplanation.local_pred\nexplanation.local_exp\nexplanation.score\nexplanation.top_labels\n\nhelp(LimeTabularExplainer)\n\nHelp on class LimeTabularExplainer in module lime.lime_tabular:\n\nclass LimeTabularExplainer(builtins.object)\n |  LimeTabularExplainer(training_data, mode='classification', training_labels=None, feature_names=None, categorical_features=None, categorical_names=None, kernel_width=None, kernel=None, verbose=False, class_names=None, feature_selection='auto', discretize_continuous=True, discretizer='quartile', sample_around_instance=False, random_state=None, training_data_stats=None)\n |  \n |  Explains predictions on tabular (i.e. matrix) data.\n |  For numerical features, perturb them by sampling from a Normal(0,1) and\n |  doing the inverse operation of mean-centering and scaling, according to the\n |  means and stds in the training data. For categorical features, perturb by\n |  sampling according to the training distribution, and making a binary\n |  feature that is 1 when the value is the same as the instance being\n |  explained.\n |  \n |  Methods defined here:\n |  \n |  __init__(self, training_data, mode='classification', training_labels=None, feature_names=None, categorical_features=None, categorical_names=None, kernel_width=None, kernel=None, verbose=False, class_names=None, feature_selection='auto', discretize_continuous=True, discretizer='quartile', sample_around_instance=False, random_state=None, training_data_stats=None)\n |      Init function.\n |      \n |      Args:\n |          training_data: numpy 2d array\n |          mode: \"classification\" or \"regression\"\n |          training_labels: labels for training data. Not required, but may be\n |              used by discretizer.\n |          feature_names: list of names (strings) corresponding to the columns\n |              in the training data.\n |          categorical_features: list of indices (ints) corresponding to the\n |              categorical columns. Everything else will be considered\n |              continuous. Values in these columns MUST be integers.\n |          categorical_names: map from int to list of names, where\n |              categorical_names[x][y] represents the name of the yth value of\n |              column x.\n |          kernel_width: kernel width for the exponential kernel.\n |              If None, defaults to sqrt (number of columns) * 0.75\n |          kernel: similarity kernel that takes euclidean distances and kernel\n |              width as input and outputs weights in (0,1). If None, defaults to\n |              an exponential kernel.\n |          verbose: if true, print local prediction values from linear model\n |          class_names: list of class names, ordered according to whatever the\n |              classifier is using. If not present, class names will be '0',\n |              '1', ...\n |          feature_selection: feature selection method. can be\n |              'forward_selection', 'lasso_path', 'none' or 'auto'.\n |              See function 'explain_instance_with_data' in lime_base.py for\n |              details on what each of the options does.\n |          discretize_continuous: if True, all non-categorical features will\n |              be discretized into quartiles.\n |          discretizer: only matters if discretize_continuous is True\n |              and data is not sparse. Options are 'quartile', 'decile',\n |              'entropy' or a BaseDiscretizer instance.\n |          sample_around_instance: if True, will sample continuous features\n |              in perturbed samples from a normal centered at the instance\n |              being explained. Otherwise, the normal is centered on the mean\n |              of the feature data.\n |          random_state: an integer or numpy.RandomState that will be used to\n |              generate random numbers. If None, the random state will be\n |              initialized using the internal numpy seed.\n |          training_data_stats: a dict object having the details of training data\n |              statistics. If None, training data information will be used, only matters\n |              if discretize_continuous is True. Must have the following keys:\n |              means\", \"mins\", \"maxs\", \"stds\", \"feature_values\",\n |              \"feature_frequencies\"\n |  \n |  explain_instance(self, data_row, predict_fn, labels=(1,), top_labels=None, num_features=10, num_samples=5000, distance_metric='euclidean', model_regressor=None)\n |      Generates explanations for a prediction.\n |      \n |      First, we generate neighborhood data by randomly perturbing features\n |      from the instance (see __data_inverse). We then learn locally weighted\n |      linear models on this neighborhood data to explain each of the classes\n |      in an interpretable way (see lime_base.py).\n |      \n |      Args:\n |          data_row: 1d numpy array or scipy.sparse matrix, corresponding to a row\n |          predict_fn: prediction function. For classifiers, this should be a\n |              function that takes a numpy array and outputs prediction\n |              probabilities. For regressors, this takes a numpy array and\n |              returns the predictions. For ScikitClassifiers, this is\n |              `classifier.predict_proba()`. For ScikitRegressors, this\n |              is `regressor.predict()`. The prediction function needs to work\n |              on multiple feature vectors (the vectors randomly perturbed\n |              from the data_row).\n |          labels: iterable with labels to be explained.\n |          top_labels: if not None, ignore labels and produce explanations for\n |              the K labels with highest prediction probabilities, where K is\n |              this parameter.\n |          num_features: maximum number of features present in explanation\n |          num_samples: size of the neighborhood to learn the linear model\n |          distance_metric: the distance metric to use for weights.\n |          model_regressor: sklearn regressor to use in explanation. Defaults\n |              to Ridge regression in LimeBase. Must have model_regressor.coef_\n |              and 'sample_weight' as a parameter to model_regressor.fit()\n |      \n |      Returns:\n |          An Explanation object (see explanation.py) with the corresponding\n |          explanations.\n |  \n |  ----------------------------------------------------------------------\n |  Static methods defined here:\n |  \n |  convert_and_round(values)\n |  \n |  validate_training_data_stats(training_data_stats)\n |      Method to validate the structure of training data stats\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables\n |  \n |  __weakref__\n |      list of weak references to the object\n\n\n\nEsta idea de aplicar perturbaciones y generar explicaciones locales, ha sido extendida a texto e imÃ¡genes:\nLIME para texto: LIME para ImÃ¡genes"
  },
  {
    "objectID": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html#shap-para-datos-tabulares",
    "href": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html#shap-para-datos-tabulares",
    "title": "IntroducciÃ³n a la explicabilidad e interpretabilidad en modelos",
    "section": "SHAP para datos tabulares",
    "text": "SHAP para datos tabulares\nSitio oficial\nImagina esta situaciÃ³n: Tenemos a tres brillantes cientÃ­ficos de datos (llamÃ©moslos Ana, Luis y SofÃ­a) trabajando juntos en un proyecto para la empresa. Al final del aÃ±o, su increÃ­ble modelo predictivo ha generado un aumento de ganancias de 5 millones de euros. Ahora, la pregunta es: Â¿cÃ³mo repartimos estos 5 millones de manera justa entre Ana, Luis y SofÃ­a, de acuerdo con la contribuciÃ³n real de cada uno al Ã©xito del modelo?\nSHAP (SHapley Additive exPlanations) nos da una manera de hacer precisamente esto, pero en lugar de cientÃ­ficos de datos y ganancias, piensa en las caracterÃ­sticas de tus datos y la predicciÃ³n de tu modelo.\nLa Idea Central: El Valor de Shapley\nLa base de SHAP son los llamados â€œvalores de Shapleyâ€. Estos valores vienen de un Ã¡rea de las matemÃ¡ticas que estudia cÃ³mo repartir justamente las ganancias en juegos de colaboraciÃ³n entre varios jugadores. SHAP toma esta idea y la aplica a las caracterÃ­sticas de tu modelo.\nÂ¿CÃ³mo se calcula este â€œvalor justoâ€ para cada caracterÃ­stica?\nImagina que vamos aÃ±adiendo las caracterÃ­sticas a nuestro modelo una por una, en todos los Ã³rdenes posibles, y observamos cuÃ¡nto cambia la predicciÃ³n en cada paso.\nPor ejemplo, imÃ¡gina en un servicio de subscripciÃ³n por cable y pensemos en predecir si un cliente va a abandonar el servicio de cable, basÃ¡ndonos en tres caracterÃ­sticas: su antigÃ¼edad, sus cargos mensuales y si tiene fibra Ã³ptica.\nPrimero, podrÃ­amos usar solo la antigÃ¼edad para hacer una predicciÃ³n (aunque probablemente no serÃ­a muy buena). Anotamos esta predicciÃ³n. Luego, aÃ±adimos los cargos mensuales a la antigÃ¼edad y vemos cÃ³mo cambia la predicciÃ³n. La diferencia entre la nueva predicciÃ³n y la anterior serÃ­a la â€œcontribuciÃ³n marginalâ€ de los cargos mensuales en este orden especÃ­fico. Finalmente, aÃ±adimos si tiene fibra Ã³ptica a las dos anteriores y vemos el cambio en la predicciÃ³n. Esa serÃ­a la contribuciÃ³n marginal de la fibra Ã³ptica en este orden. Pero, Â¡el orden importa! PodrÃ­amos haber empezado aÃ±adiendo la fibra Ã³ptica primero, luego la antigÃ¼edad y despuÃ©s los cargos mensuales, y la â€œcontribuciÃ³nâ€ de cada caracterÃ­stica en cada paso podrÃ­a ser diferente.\nEl Truco de SHAP: Promediar Todas las Posibilidades\nPara obtener el valor de Shapley de una caracterÃ­stica, SHAP hace precisamente esto: calcula la contribuciÃ³n marginal promedio de esa caracterÃ­stica en Â¡todos los posibles Ã³rdenes en los que podrÃ­amos haber aÃ±adido las caracterÃ­sticas al modelo!\nAplicando esto al Aprendizaje AutomÃ¡tico:\nEn el contexto de tu modelo de Deep Learning (como tu PerceptrÃ³n Multicapa), SHAP trata cada caracterÃ­stica de tus datos como si fuera uno de nuestros cientÃ­ficos de datos, y la predicciÃ³n del modelo como si fueran las ganancias.\nPara entender cÃ³mo contribuye cada caracterÃ­stica a una predicciÃ³n especÃ­fica, SHAP calcula el valor de Shapley para cada caracterÃ­stica de esa instancia. Un valor de Shapley positivo para una caracterÃ­stica significa que ese valor de la caracterÃ­stica empujÃ³ la predicciÃ³n del modelo hacia un resultado particular (por ejemplo, una mayor probabilidad de abandono). Un valor de Shapley negativo significa que empujÃ³ la predicciÃ³n en la direcciÃ³n opuesta. Algo parecido a LIME.\nÂ¿Por quÃ© SHAP es especial? Dos GarantÃ­as Importantes:\nSHAP tiene dos propiedades muy importantes que lo hacen destacar frente a otras formas de explicar modelos (como LIME o la importancia de las caracterÃ­sticas basada en la permutaciÃ³n):\nPrecisiÃ³n Local: La suma de los valores de Shapley de todas las caracterÃ­sticas para una predicciÃ³n especÃ­fica, mÃ¡s un valor base (la predicciÃ³n promedio del modelo), debe ser igual a la predicciÃ³n real del modelo para esa instancia. Esto significa que la explicaciÃ³n local es consistente con la salida del modelo original.\nConsistencia: Si cambias tu modelo de tal manera que una caracterÃ­stica tenga un mayor impacto en la predicciÃ³n en todos los posibles Ã³rdenes en los que se podrÃ­a aÃ±adir, entonces su valor de Shapley (su â€œcrÃ©ditoâ€) nunca deberÃ­a disminuir. Esto asegura que las explicaciones sean intuitivas y coherentes con los cambios en el modelo.\nEn la PrÃ¡ctica: Simulando la EliminaciÃ³n de CaracterÃ­sticas\nUna dificultad prÃ¡ctica es cÃ³mo simular la â€œeliminaciÃ³nâ€ de una caracterÃ­stica al calcular su contribuciÃ³n marginal en un modelo ya entrenado. No podemos simplemente quitar una columna de datos y esperar que el modelo siga funcionando.\nLa librerÃ­a SHAP utiliza una tÃ©cnica inteligente para abordar esto. Simula la ausencia de una caracterÃ­stica reemplazÃ¡ndola con los valores que esa caracterÃ­stica toma en un â€œconjunto de datos de fondoâ€ (background dataset). Este conjunto de datos de fondo representa la distribuciÃ³n â€œtÃ­picaâ€ de las caracterÃ­sticas.\nEn resumen, SHAP te da una manera justa y consistente de entender la contribuciÃ³n de cada caracterÃ­stica a la predicciÃ³n de tu modelo para una instancia especÃ­fica, basÃ¡ndose en la idea de cÃ³mo se repartirÃ­an las ganancias en un juego colaborativo. Te dice cuÃ¡nto â€œresponsableâ€ es cada caracterÃ­stica del resultado final de la predicciÃ³n.\n\n#Computemos los valores SHAP de nuestro modelo\nexplainer = shap.Explainer(model, X_train,feature_names=wine.feature_names)\nshap_values = explainer(X_test)\n\n\nPor ejemplo. Para nuestro modelo, la caracteristica que mas aporta para que se prediga de la clase 1, es proline, y la que menos aporta es malic_acid.\n\nPero atenciÃ³n! Esto ya es a nivel de Clase no de explicaciÃ³n de una muestra.\n\nshap.plots.bar(shap_values[:,:,0], max_display=X_test.shape[1])\n\n\n\n\n\n\n\n\n\nPara la clase 2, la caracteristica que mas aporta es alcohol/proline y la que menos aporta a dicha predicciÃ³n es proanthocyanins.\n\n\nshap.plots.bar(shap_values[:,:,1], max_display=X_test.shape[1])\n\n\n\n\n\n\n\n\n\nPara la tercera clase, la caracteristica mas importante es diluted wines y la menos importante es magnesium\n\n\nshap.plots.bar(shap_values[:,:,2], max_display=X_test.shape[1])\n\n\n\n\n\n\n\n\nPara ver estas contribuciones podemos verlo en un grÃ¡fico conjunto para un subconjunto de datos.\n\nshap.initjs()\nshap.summary_plot(shap_values, X_test, feature_names=wine.feature_names,\n                  max_display=10);\n\n\n\n\nFutureWarning: The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.\n  shap.summary_plot(shap_values, X_test, feature_names=wine.feature_names,\n\n\n\n\n\n\n\n\n\n\nTambien podriamos ver para cada clase como aporta cada caracteristica a la predicciÃ³n del modelo.\n\n\nprint(\"clase 1\")\nshap.plots.beeswarm(shap_values[:,:,0], max_display=X_test.shape[1])\n\nclase 1\n\n\n\n\n\n\n\n\n\n\nprint(\"clase 2\")\nshap.plots.beeswarm(shap_values[:,:,1], max_display=X_test.shape[1])\n\nclase 2\n\n\n\n\n\n\n\n\n\n\nprint(\"clase 3\")\nshap.plots.beeswarm(shap_values[:,:,2], max_display=X_test.shape[1])\n\nclase 3\n\n\n\n\n\n\n\n\n\nAhora si explicaciones mÃ¡s locales por muestra:\n\nAhora tomemos un dato de test y veamos como las caracteristicas influyeron para que la red se inclinara por la categorÃ­a cierta.\n\n\nclase = y_test[9]\nprint(\"clase: \", clase)\nshap.plots.bar(shap_values[9,:,np.argmax(clase)], max_display=X_test.shape[1])\n\nclase:  [0. 0. 1.]\n\n\n\n\n\n\n\n\n\nSHAP ha sido adaptado para otros dominios y problemas:\nImÃ¡genes\nTexto"
  },
  {
    "objectID": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html#conclusiÃ³n-lime-vs.-shap-cuÃ¡ndo-usar-cuÃ¡l",
    "href": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html#conclusiÃ³n-lime-vs.-shap-cuÃ¡ndo-usar-cuÃ¡l",
    "title": "IntroducciÃ³n a la explicabilidad e interpretabilidad en modelos",
    "section": "ConclusiÃ³n: LIME vs.Â SHAP: Â¿CuÃ¡ndo usar cuÃ¡l?",
    "text": "ConclusiÃ³n: LIME vs.Â SHAP: Â¿CuÃ¡ndo usar cuÃ¡l?\nTanto LIME como SHAP son buenos mÃ©todos para explicar los modelos de aprendizaje automÃ¡tico.\nEn teorÃ­a, SHAP es el mejor enfoque porque ofrece garantÃ­as matemÃ¡ticas sobre la precisiÃ³n y consistencia de sus explicaciones. Esto significa que podemos confiar mÃ¡s en que las explicaciones de SHAP reflejan fielmente cÃ³mo funciona el modelo.\nA continuaciÃ³n, mencionamos algunas limitaciones adicionales de ambos mÃ©todos:\nLimitaciones de LIME:\n\nNo estÃ¡ diseÃ±ado para datos con â€œone-hot encodingâ€: El â€œone-hot encodingâ€ es cuando transformamos variables categÃ³ricas (como â€œcolorâ€ con valores â€œrojoâ€, â€œazulâ€, â€œverdeâ€) en varias columnas binarias (una para â€œcolor_rojoâ€, otra para â€œcolor_azulâ€, etc.). LIME funciona creando pequeÃ±as variaciones (perturbaciones) de tus datos para ver cÃ³mo cambia la predicciÃ³n. Si perturbas una variable â€œone-hot encodedâ€, podrÃ­as terminar con combinaciones sin sentido (por ejemplo, que una observaciÃ³n sea â€œrojoâ€ y â€œazulâ€ al mismo tiempo, o que no sea ningÃºn color), lo que llevarÃ­a a explicaciones poco fiables. (Puedes ver una discusiÃ³n sobre esto aquÃ­).\nDepende de cÃ³mo â€œperturbasâ€ los datos: LIME necesita alterar las muestras de datos de forma que tenga sentido para tu caso especÃ­fico. Para datos tabulares, esto suele implicar aÃ±adir un poco de â€œruidoâ€ aleatorio a cada caracterÃ­stica. Para imÃ¡genes, podrÃ­a significar reemplazar pequeÃ±as regiones de la imagen (superpÃ­xeles) con un color promedio o con ceros. Para texto, podrÃ­a ser quitar palabras del texto. Es importante pensar si estas formas de perturbar tus datos podrÃ­an tener efectos secundarios no deseados que afecten la confianza en las explicaciones.\nEl modelo local de LIME podrÃ­a no ser un buen reflejo del modelo original: LIME crea un modelo local mÃ¡s simple para explicar una predicciÃ³n especÃ­fica. A veces, este modelo local puede no capturar bien el comportamiento del modelo original complejo. Es una buena prÃ¡ctica verificar si hay inconsistencias antes de confiar plenamente en las explicaciones de LIME.\nFunciona mejor con modelos que dan probabilidades: LIME estÃ¡ pensado para modelos de clasificaciÃ³n que predicen la probabilidad de cada clase (por ejemplo, â€œ70% de probabilidad de ser clase Aâ€). Algunos modelos, como las MÃ¡quinas de Vectores de Soporte (SVMs), no estÃ¡n diseÃ±ados naturalmente para dar probabilidades (aunque se les puede forzar, a veces con problemas). Usar LIME con las â€œpseudo-probabilidadesâ€ de estos modelos podrÃ­a introducir algÃºn sesgo en las explicaciones.\n\nLimitaciones de SHAP:\n\nDepende de un â€œconjunto de datos de fondoâ€ (background dataset): SHAP necesita un conjunto de datos de referencia para calcular un valor base o esperado de la predicciÃ³n. Si tu conjunto de datos es muy grande, usarlo todo para este cÃ¡lculo puede ser muy costoso computacionalmente (llevarÃ­a mucho tiempo). Por eso, a menudo se usan aproximaciones, como tomar una muestra mÃ¡s pequeÃ±a del conjunto de datos. Esto podrÃ­a afectar un poco la precisiÃ³n de la explicaciÃ³n.\nExplica la desviaciÃ³n respecto a un valor base estimado del entrenamiento: SHAP te dice cÃ³mo la predicciÃ³n de una instancia se desvÃ­a del valor promedio que el modelo aprendiÃ³ con todo el conjunto de datos de entrenamiento. Sin embargo, dependiendo de tu objetivo, podrÃ­a ser mÃ¡s Ãºtil comparar la predicciÃ³n con un grupo mÃ¡s especÃ­fico. Ejemplo: Si estÃ¡s prediciendo la â€œfuga de clientesâ€ (churn), quizÃ¡s te interese mÃ¡s explicar por quÃ© un cliente se va a ir en comparaciÃ³n con los clientes que no se fueron, en lugar de compararlo con el promedio de todos los clientes (incluyendo los que se fueron y los que no). En este caso, querrÃ­as usar el conjunto de datos de los clientes que no se fueron como tu â€œconjunto de datos de fondoâ€."
  },
  {
    "objectID": "semana_3/index.html",
    "href": "semana_3/index.html",
    "title": "Semana 3: TÃ©cnicas Avanzadas y Robustez",
    "section": "",
    "text": "La Ãºltima semana se centra en refinar y aplicar tÃ©cnicas avanzadas para construir modelos robustos e interpretables.\nTemas Clave:\n\nPrincipios de GeneralizaciÃ³n.\nTransferencia de Aprendizaje y Fine-tuning.\nExplicabilidad e Interpretabilidad de modelos.\nOptimizaciÃ³n de hiperparÃ¡metros.\nEntrenamiento Adversarial y Robustez.\n\nEnfoque PrÃ¡ctico: DesarrollarÃ¡s habilidades para mejorar la eficacia de tus modelos en entornos reales. ImplementarÃ¡s tÃ©cnicas de transferencia de aprendizaje, fine-tuning, generalizaciÃ³n y usarÃ¡s herramientas de explicabilidad para comprender y comunicar los resultados de modelos complejos.\nMateriales de la Semana:\n\nSlides de la Semana 3 (PrÃ³ximamente)\nNotebook: IntroducciÃ³n a Transfer Learning y Finetuning\nNotebook: IntroducciÃ³n a Explicabilidad e Interpretabilidad"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2b_Resolviendo_desafios_con_RNNs_usando_keras.html",
    "href": "semana_2/notebooks/Nb_2b_Resolviendo_desafios_con_RNNs_usando_keras.html",
    "title": "Resolviendo diversidad de tareas con RNNs",
    "section": "",
    "text": "Ãšltima actualizaciÃ³n: 09/05/2025\n#@title Importar librerÃ­as\n#importar librerÃ­as necesarias\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\n#@title Funciones complementarias\n\ndef plot_graphs(history, metric):\n  plt.plot(history.history[metric])\n  plt.plot(history.history['val_'+metric], '')\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(metric)\n  plt.legend([metric, 'val_'+metric])\n\ndef plot_variables(df, date_time):\n    plot_cols = ['T (degC)', 'p (mbar)', 'rho (g/m**3)']\n    plot_features = df[plot_cols]\n    plot_features.index = date_time\n    _ = plot_features.plot(subplots=True)\n\n    plot_features = df[plot_cols][:480]\n    plot_features.index = date_time[:480]\n    _ = plot_features.plot(subplots=True)\n\ndef read_dataset_clima():\n\n    zip_path = tf.keras.utils.get_file(\n        origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n        fname='jena_climate_2009_2016.csv.zip',\n        extract=True)\n    csv_path, _ = os.path.splitext(zip_path)\n\n    df = pd.read_csv(csv_path+'/'+'jena_climate_2009_2016.csv')\n    # Slice [start:stop:step], starting from index 5 take every 6th record.\n    df = df[5::6]\n\n    date_time = pd.to_datetime(df.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')\n\n    return df, date_time\n\ndef split_data(df):\n    column_indices = {name: i for i, name in enumerate(df.columns)}\n\n    n = len(df)\n    train_df = df[0:int(n*0.7)]\n    val_df = df[int(n*0.7):int(n*0.9)]\n    test_df = df[int(n*0.9):]\n\n    num_features = df.shape[1]\n\n    return train_df, val_df, test_df, num_features, column_indices\n\ndef normalizacion_datos(train_df, val_df, test_df):\n    train_mean = train_df.mean()\n    train_std = train_df.std()\n\n    train_df = (train_df - train_mean) / train_std\n    val_df = (val_df - train_mean) / train_std\n    test_df = (test_df - train_mean) / train_std\n\n    return train_df, val_df, test_df\n\n\nclass WindowGenerator():\n  def __init__(self, input_width, label_width, shift,\n               train_df, val_df, test_df,\n               label_columns=None):\n    # Store the raw data.\n    self.train_df = train_df\n    self.val_df = val_df\n    self.test_df = test_df\n\n    # Work out the label column indices.\n    self.label_columns = label_columns\n    if label_columns is not None:\n      self.label_columns_indices = {name: i for i, name in\n                                    enumerate(label_columns)}\n    self.column_indices = {name: i for i, name in\n                           enumerate(train_df.columns)}\n\n    # Work out the window parameters.\n    self.input_width = input_width\n    self.label_width = label_width\n    self.shift = shift\n\n    self.total_window_size = input_width + shift\n\n    self.input_slice = slice(0, input_width)\n    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n\n    self.label_start = self.total_window_size - self.label_width\n    self.labels_slice = slice(self.label_start, None)\n    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n\n  def __repr__(self):\n    return '\\n'.join([\n        f'Total window size: {self.total_window_size}',\n        f'Input indices: {self.input_indices}',\n        f'Label indices: {self.label_indices}',\n        f'Label column name(s): {self.label_columns}'])\n\n\ndef split_window(self, features):\n  inputs = features[:, self.input_slice, :]\n  labels = features[:, self.labels_slice, :]\n  if self.label_columns is not None:\n    labels = tf.stack(\n        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n        axis=-1)\n\n  # Slicing doesn't preserve static shape information, so set the shapes\n  # manually. This way the `tf.data.Datasets` are easier to inspect.\n  inputs.set_shape([None, self.input_width, None])\n  labels.set_shape([None, self.label_width, None])\n\n  return inputs, labels\n\ndef plot(self, model=None, plot_col='T (degC)', max_subplots=3):\n  inputs, labels = self.example\n  plt.figure(figsize=(12, 8))\n  plot_col_index = self.column_indices[plot_col]\n  max_n = min(max_subplots, len(inputs))\n  for n in range(max_n):\n    plt.subplot(max_n, 1, n+1)\n    plt.ylabel(f'{plot_col} [normed]')\n    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n             label='Inputs', marker='.', zorder=-10)\n\n    if self.label_columns:\n      label_col_index = self.label_columns_indices.get(plot_col, None)\n    else:\n      label_col_index = plot_col_index\n\n    if label_col_index is None:\n      continue\n\n    plt.scatter(self.label_indices, labels[n, :, label_col_index],\n                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n    if model is not None:\n      predictions = model(inputs)\n      plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n                  marker='X', edgecolors='k', label='Predictions',\n                  c='#ff7f0e', s=64)\n\n    if n == 0:\n      plt.legend()\n\n  plt.xlabel('Time [h]')\n\ndef make_dataset(self, data):\n  data = np.array(data, dtype=np.float32)\n  ds = tf.keras.utils.timeseries_dataset_from_array(\n      data=data,\n      targets=None,\n      sequence_length=self.total_window_size,\n      sequence_stride=1,\n      shuffle=True,\n      batch_size=32,)\n\n  ds = ds.map(self.split_window)\n\n  return ds\n\n\n@property\ndef train(self):\n  return self.make_dataset(self.train_df)\n\n@property\ndef val(self):\n  return self.make_dataset(self.val_df)\n\n@property\ndef test(self):\n  return self.make_dataset(self.test_df)\n\n@property\ndef example(self):\n  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n  result = getattr(self, '_example', None)\n  if result is None:\n    # No example batch was found, so get one from the `.train` dataset\n    result = next(iter(self.train))\n    # And cache it for next time\n    self._example = result\n  return result\n\n\n\nWindowGenerator.plot = plot\nWindowGenerator.split_window = split_window\nWindowGenerator.make_dataset = make_dataset\nWindowGenerator.train = train\nWindowGenerator.val = val\nWindowGenerator.test = test\nWindowGenerator.example = example"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2b_Resolviendo_desafios_con_RNNs_usando_keras.html#clasificaciÃ³n-de-texto-usando-rnns-relaciÃ³n-muchas-entradas-a-una-salida",
    "href": "semana_2/notebooks/Nb_2b_Resolviendo_desafios_con_RNNs_usando_keras.html#clasificaciÃ³n-de-texto-usando-rnns-relaciÃ³n-muchas-entradas-a-una-salida",
    "title": "Resolviendo diversidad de tareas con RNNs",
    "section": "ClasificaciÃ³n de texto usando RNNs (RelaciÃ³n muchas entradas a una salida)",
    "text": "ClasificaciÃ³n de texto usando RNNs (RelaciÃ³n muchas entradas a una salida)\nEn este escenario se reciben multiples entradas pero solo se genera una salida. AquÃ­ podemos abordar el ejemplo mÃ¡s comÃºn que es la clasificaciÃ³n de texto.\n\nDescargar el dataset de reviews de peliculas usando Tensorflow.\nEl gran conjunto de datos de crÃ­ticas de pelÃ­culas de IMDB es un conjunto de datos de clasificaciÃ³n binaria: todas las crÃ­ticas tienen un sentimiento positivo o negativo.\n\ndataset, info = tfds.load('imdb_reviews', with_info=True,\n                          as_supervised=True)\ntrain_dataset, test_dataset = dataset['train'], dataset['test']\n\nWARNING:absl:Variant folder /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0 has no dataset_info.json\n\n\nDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\n\n\n\n# contar cuantas muestras tiene train_dataset y test_dataset\nprint(info.splits)\n\n{Split('train'): &lt;SplitInfo num_examples=25000, num_shards=1&gt;, Split('test'): &lt;SplitInfo num_examples=25000, num_shards=1&gt;, Split('unsupervised'): &lt;SplitInfo num_examples=50000, num_shards=1&gt;}\n\n\n\n# extraer una muestra del conjunto de entrenamiento\ntrain_example, train_label = next(iter(train_dataset.batch(1)))\n\nprint('Para la etiqueta: {} se tiene el texto: {}'.format(train_label, train_example.numpy()))\n\nPara la etiqueta: [0] se tiene el texto: [b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"]\n\n\nLas siguientes lineas son para optimizar los conjuntos y que el entrenamiento sea mÃ¡s acelerado.\n\nBUFFER_SIZE = 10000\nBATCH_SIZE = 32\n\n# optimizaciÃ³n para train\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\n# optimizaciÃ³n para test\ntest_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\n\n# muestras para el conjunto de test\nfor ejemplo, label in test_dataset.take(1):\n  print('textos: ', ejemplo.numpy()[:3])\n  print()\n  print('labels: ', label.numpy()[:3])\n\ntextos:  [b\"There are films that make careers. For George Romero, it was NIGHT OF THE LIVING DEAD; for Kevin Smith, CLERKS; for Robert Rodriguez, EL MARIACHI. Add to that list Onur Tukel's absolutely amazing DING-A-LING-LESS. Flawless film-making, and as assured and as professional as any of the aforementioned movies. I haven't laughed this hard since I saw THE FULL MONTY. (And, even then, I don't think I laughed quite this hard... So to speak.) Tukel's talent is considerable: DING-A-LING-LESS is so chock full of double entendres that one would have to sit down with a copy of this script and do a line-by-line examination of it to fully appreciate the, uh, breadth and width of it. Every shot is beautifully composed (a clear sign of a sure-handed director), and the performances all around are solid (there's none of the over-the-top scenery chewing one might've expected from a film like this). DING-A-LING-LESS is a film whose time has come.\"\n b\"A blackly comic tale of a down-trodden priest, Nazarin showcases the economy that Luis Bunuel was able to achieve in being able to tell a deeply humanist fable with a minimum of fuss. As an output from his Mexican era of film making, it was an invaluable talent to possess, with little money and extremely tight schedules. Nazarin, however, surpasses many of Bunuel's previous Mexican films in terms of the acting (Francisco Rabal is excellent), narrative and theme.&lt;br /&gt;&lt;br /&gt;The theme, interestingly, is something that was explored again in Viridiana, made three years later in Spain. It concerns the individual's struggle for humanity and altruism amongst a society that rejects any notion of virtue. Father Nazarin, however, is portrayed more sympathetically than Sister Viridiana. Whereas the latter seems to choose charity because she wishes to atone for her (perceived) sins, Nazarin's whole existence and reason for being seems to be to help others, whether they (or we) like it or not. The film's last scenes, in which he casts doubt on his behaviour and, in a split second, has to choose between the life he has been leading or the conventional life that is expected of a priest, are so emotional because they concern his moral integrity and we are never quite sure whether it remains intact or not.&lt;br /&gt;&lt;br /&gt;This is a remarkable film and I would urge anyone interested in classic cinema to seek it out. It is one of Bunuel's most moving films, and encapsulates many of his obsessions: frustrated desire, mad love, religious hypocrisy etc. In my view 'Nazarin' is second only to 'The Exterminating Angel', in terms of his Mexican movies, and is certainly near the top of the list of Bunuel's total filmic output.\"\n b'Scary Movie 1-4, Epic Movie, Date Movie, Meet the Spartans, Not another Teen Movie and Another Gay Movie. Making \"Superhero Movie\" the eleventh in a series that single handily ruined the parody genre. Now I\\'ll admit it I have a soft spot for classics such as Airplane and The Naked Gun but you know you\\'ve milked a franchise so bad when you can see the gags a mile off. In fact the only thing that might really temp you into going to see this disaster is the incredibly funny but massive sell-out Leslie Neilson.&lt;br /&gt;&lt;br /&gt;You can tell he needs the money, wither that or he intends to go down with the ship like a good Capitan would. In no way is he bringing down this genre but hell he\\'s not helping it. But if I feel sorry for anybody in this film its decent actor Drake Bell who is put through an immense amount of embarrassment. The people who are put through the largest amount of torture by far however is the audience forced to sit through 90 minutes of laughless bile no funnier than herpes.&lt;br /&gt;&lt;br /&gt;After spoofing disaster films in Airplane!, police shows in The Naked Gun, and Hollywood horrors in Scary Movie 3 and 4, producer David Zucker sets his satirical sights on the superhero genre with this anarchic comedy lampooning everything from Spider-Man to X-Men and Superman Returns.&lt;br /&gt;&lt;br /&gt;Shortly after being bitten by a genetically altered dragonfly, high-school outcast Rick Riker (Drake Bell) begins to experience a startling transformation. Now Rick\\'s skin is as strong as steel, and he possesses the strength of ten men. Determined to use his newfound powers to fight crime, Rick creates a special costume and assumes the identity of The Dragonfly -- a fearless crime fighter dedicated to keeping the streets safe for law-abiding citizens.&lt;br /&gt;&lt;br /&gt;But every superhero needs a nemesis, and after Lou Landers (Christopher McDonald) is caught in the middle of an experiment gone horribly awry, he develops the power to leech the life force out of anyone he meets and becomes the villainous Hourglass. Intent on achieving immortality, the Hourglass attempts to gather as much life force as possible as the noble Dragonfly sets out to take down his archenemy and realize his destiny as a true hero. Craig Mazin writes and directs this low-flying spoof.&lt;br /&gt;&lt;br /&gt;featuring Tracy Morgan, Pamela Anderson, Leslie Nielsen, Marion Ross, Jeffrey Tambor, and Regina Hall.&lt;br /&gt;&lt;br /&gt;Hell Superhero Movie may earn some merit in the fact that it\\'s a hell of a lot better than Meet the Spartans and Epic Movie. But with great responsibility comes one of the worst outings of 2008 to date. Laughless but a little less irritating than Meet the Spartans. And in the same sense much more forgettable than meet the Spartans. But maybe that\\'s a good reason. There are still some of us trying to scrape away the stain that was Meet the Spartans from our memory.&lt;br /&gt;&lt;br /&gt;My final verdict? Avoid, unless you\\'re one of thoses people who enjoy such car crash cinema. As bad as Date Movie and Scary Movie 2 but not quite as bad as Meet the Spartans or Epic Movie. Super Villain.']\n\nlabels:  [1 1 0]\n\n\n\n\nPre-procesamiento y modelado.\n\n\n\nbidirectional.png\n\n\nExplicaciÃ³n del modelo:\nEste modelo se puede construir como un tf.keras.Sequential.\n\nLa primera capa (amarilla) es el codificador, que convierte el texto en una secuencia de Ã­ndices de tokens.\nDespuÃ©s del codificador hay una capa de embedding (verde). Una capa de embedding almacena un vector por palabra. Cuando se llama, convierte las secuencias de Ã­ndices de palabras en secuencias de vectores. Estos vectores son entrenables. DespuÃ©s del entrenamiento (con suficientes datos), las palabras con significados similares a menudo tienen vectores similares.\nUna red neuronal recurrente (RNN) procesa la entrada secuencial iterando a travÃ©s de los elementos. Las RNN pasan las salidas de un instante de tiempo a su entrada en el siguiente instante de tiempo.\nLa capa tf.keras.layers.Bidirectional tambiÃ©n se puede usar con una capa RNN. Esto propaga la entrada hacia adelante y hacia atrÃ¡s a travÃ©s de la capa RNN y luego concatena la salida final.\n\nLa principal ventaja de una RNN bidireccional es que la seÃ±al desde el comienzo de la entrada no necesita ser procesada a lo largo de cada instante de tiempo para afectar la salida.\nLa principal desventaja de una RNN bidireccional es que no se pueden transmitir predicciones de manera eficiente a medida que se agregan palabras al final.\n\nDespuÃ©s de que la RNN ha convertido la secuencia en un solo vector, las dos capas Dense realizan un procesamiento final y convierten esta representaciÃ³n vectorial en un solo valor como salida de clasificaciÃ³n.\n\n\nCapa Codificador (Text Vectorization)\nEl texto sin procesar cargado por tfds necesita ser procesado antes de que pueda ser utilizado en un modelo. La forma mÃ¡s sencilla de procesar texto para el entrenamiento es utilizando la capa TextVectorization.\nExplicaciÃ³n:\nLa capa TextVectorization es una herramienta en TensorFlow que transforma texto sin procesar en una representaciÃ³n numÃ©rica que los modelos de aprendizaje automÃ¡tico pueden entender. Convierte las palabras en vectores, lo que facilita el entrenamiento del modelo. Esto incluye tareas como:\n\nTokenizaciÃ³n: Dividir el texto en palabras o subpalabras.\nNormalizaciÃ³n: Convertir todo el texto a minÃºsculas, eliminar caracteres especiales, etc.\nVectorizaciÃ³n: Asignar un Ã­ndice o valor numÃ©rico a cada palabra o token, lo que permite representarlo como una matriz numÃ©rica.\n\n\nVOCAB_SIZE = 1000\nencoder = tf.keras.layers.TextVectorization(\n    max_tokens=VOCAB_SIZE)\n\n# Crea la capa y pasa el texto del conjunto de datos al mÃ©todo .adapt de la capa\nencoder.adapt(train_dataset.map(lambda text, label: text))\n\nEl mÃ©todo .adapt se utiliza para â€œentrenarâ€ la capa en el conjunto de datos de texto. Esto significa que la capa analizarÃ¡ el texto y aprenderÃ¡ el vocabulario, asÃ­ como otras caracterÃ­sticas (como la frecuencia de las palabras) que serÃ¡n Ãºtiles para la vectorizaciÃ³n\n\n# revisar los primeros 20 elementos del vocabulario creado\nvocab = np.array(encoder.get_vocabulary())\nvocab[:20]\n\narray(['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i',\n       'this', 'that', 'br', 'was', 'as', 'for', 'with', 'movie', 'but'],\n      dtype='&lt;U14')\n\n\nUna vez que el vocabulario estÃ¡ establecido, la capa puede codificar el texto en Ã­ndices. Los tensores de Ã­ndices se rellenan con ceros hasta la secuencia mÃ¡s larga en el batch (a menos que establezcas una longitud de salida fija).\n\nencoded_ejemplo = encoder(ejemplo)[:3].numpy()\nencoded_ejemplo\n\narray([[ 48,  24,  95, ...,   0,   0,   0],\n       [  4,   1, 723, ...,   0,   0,   0],\n       [633,  18,   1, ...,  18,   1,   1]])\n\n\n\n# Visualizamos algunas oraciones\nfor n in range(3):\n  print(\"Original: \", ejemplo[n].numpy())\n  # intentar decodificar solo usando el vocab\n  print(\"Round-trip: \", \" \".join(vocab[encoded_ejemplo[n]]))\n  print()\n\nOriginal:  b\"There are films that make careers. For George Romero, it was NIGHT OF THE LIVING DEAD; for Kevin Smith, CLERKS; for Robert Rodriguez, EL MARIACHI. Add to that list Onur Tukel's absolutely amazing DING-A-LING-LESS. Flawless film-making, and as assured and as professional as any of the aforementioned movies. I haven't laughed this hard since I saw THE FULL MONTY. (And, even then, I don't think I laughed quite this hard... So to speak.) Tukel's talent is considerable: DING-A-LING-LESS is so chock full of double entendres that one would have to sit down with a copy of this script and do a line-by-line examination of it to fully appreciate the, uh, breadth and width of it. Every shot is beautifully composed (a clear sign of a sure-handed director), and the performances all around are solid (there's none of the over-the-top scenery chewing one might've expected from a film like this). DING-A-LING-LESS is a film whose time has come.\"\nRound-trip:  there are films that make [UNK] for george [UNK] it was night of the living dead for [UNK] [UNK] [UNK] for robert [UNK] [UNK] [UNK] add to that [UNK] [UNK] [UNK] absolutely amazing [UNK] [UNK] [UNK] and as [UNK] and as [UNK] as any of the [UNK] movies i havent [UNK] this hard since i saw the full [UNK] and even then i dont think i [UNK] quite this hard so to [UNK] [UNK] talent is [UNK] [UNK] is so [UNK] full of [UNK] [UNK] that one would have to sit down with a [UNK] of this script and do a [UNK] [UNK] of it to [UNK] [UNK] the [UNK] [UNK] and [UNK] of it every shot is [UNK] [UNK] a clear [UNK] of a [UNK] director and the performances all around are [UNK] theres none of the [UNK] [UNK] [UNK] one [UNK] expected from a film like this [UNK] is a film whose time has come                                                                                                                                                                                                                                                                                                                                                                                        \n\nOriginal:  b\"A blackly comic tale of a down-trodden priest, Nazarin showcases the economy that Luis Bunuel was able to achieve in being able to tell a deeply humanist fable with a minimum of fuss. As an output from his Mexican era of film making, it was an invaluable talent to possess, with little money and extremely tight schedules. Nazarin, however, surpasses many of Bunuel's previous Mexican films in terms of the acting (Francisco Rabal is excellent), narrative and theme.&lt;br /&gt;&lt;br /&gt;The theme, interestingly, is something that was explored again in Viridiana, made three years later in Spain. It concerns the individual's struggle for humanity and altruism amongst a society that rejects any notion of virtue. Father Nazarin, however, is portrayed more sympathetically than Sister Viridiana. Whereas the latter seems to choose charity because she wishes to atone for her (perceived) sins, Nazarin's whole existence and reason for being seems to be to help others, whether they (or we) like it or not. The film's last scenes, in which he casts doubt on his behaviour and, in a split second, has to choose between the life he has been leading or the conventional life that is expected of a priest, are so emotional because they concern his moral integrity and we are never quite sure whether it remains intact or not.&lt;br /&gt;&lt;br /&gt;This is a remarkable film and I would urge anyone interested in classic cinema to seek it out. It is one of Bunuel's most moving films, and encapsulates many of his obsessions: frustrated desire, mad love, religious hypocrisy etc. In my view 'Nazarin' is second only to 'The Exterminating Angel', in terms of his Mexican movies, and is certainly near the top of the list of Bunuel's total filmic output.\"\nRound-trip:  a [UNK] comic tale of a [UNK] [UNK] [UNK] [UNK] the [UNK] that [UNK] [UNK] was able to [UNK] in being able to tell a [UNK] [UNK] [UNK] with a [UNK] of [UNK] as an [UNK] from his [UNK] [UNK] of film making it was an [UNK] talent to [UNK] with little money and extremely [UNK] [UNK] [UNK] however [UNK] many of [UNK] previous [UNK] films in [UNK] of the acting [UNK] [UNK] is excellent [UNK] and [UNK] br the theme [UNK] is something that was [UNK] again in [UNK] made three years later in [UNK] it [UNK] the [UNK] [UNK] for [UNK] and [UNK] [UNK] a society that [UNK] any [UNK] of [UNK] father [UNK] however is portrayed more [UNK] than sister [UNK] [UNK] the [UNK] seems to [UNK] [UNK] because she [UNK] to [UNK] for her [UNK] [UNK] [UNK] whole [UNK] and reason for being seems to be to help others whether they or we like it or not the films last scenes in which he [UNK] doubt on his [UNK] and in a [UNK] second has to [UNK] between the life he has been leading or the [UNK] life that is expected of a [UNK] are so emotional because they [UNK] his [UNK] [UNK] and we are never quite sure whether it [UNK] [UNK] or [UNK] br this is a [UNK] film and i would [UNK] anyone interested in classic cinema to [UNK] it out it is one of [UNK] most moving films and [UNK] many of his [UNK] [UNK] [UNK] [UNK] love [UNK] [UNK] etc in my view [UNK] is second only to the [UNK] [UNK] in [UNK] of his [UNK] movies and is certainly near the top of the [UNK] of [UNK] total [UNK] [UNK]                                                                                                                                                                                                                                                   \n\nOriginal:  b'Scary Movie 1-4, Epic Movie, Date Movie, Meet the Spartans, Not another Teen Movie and Another Gay Movie. Making \"Superhero Movie\" the eleventh in a series that single handily ruined the parody genre. Now I\\'ll admit it I have a soft spot for classics such as Airplane and The Naked Gun but you know you\\'ve milked a franchise so bad when you can see the gags a mile off. In fact the only thing that might really temp you into going to see this disaster is the incredibly funny but massive sell-out Leslie Neilson.&lt;br /&gt;&lt;br /&gt;You can tell he needs the money, wither that or he intends to go down with the ship like a good Capitan would. In no way is he bringing down this genre but hell he\\'s not helping it. But if I feel sorry for anybody in this film its decent actor Drake Bell who is put through an immense amount of embarrassment. The people who are put through the largest amount of torture by far however is the audience forced to sit through 90 minutes of laughless bile no funnier than herpes.&lt;br /&gt;&lt;br /&gt;After spoofing disaster films in Airplane!, police shows in The Naked Gun, and Hollywood horrors in Scary Movie 3 and 4, producer David Zucker sets his satirical sights on the superhero genre with this anarchic comedy lampooning everything from Spider-Man to X-Men and Superman Returns.&lt;br /&gt;&lt;br /&gt;Shortly after being bitten by a genetically altered dragonfly, high-school outcast Rick Riker (Drake Bell) begins to experience a startling transformation. Now Rick\\'s skin is as strong as steel, and he possesses the strength of ten men. Determined to use his newfound powers to fight crime, Rick creates a special costume and assumes the identity of The Dragonfly -- a fearless crime fighter dedicated to keeping the streets safe for law-abiding citizens.&lt;br /&gt;&lt;br /&gt;But every superhero needs a nemesis, and after Lou Landers (Christopher McDonald) is caught in the middle of an experiment gone horribly awry, he develops the power to leech the life force out of anyone he meets and becomes the villainous Hourglass. Intent on achieving immortality, the Hourglass attempts to gather as much life force as possible as the noble Dragonfly sets out to take down his archenemy and realize his destiny as a true hero. Craig Mazin writes and directs this low-flying spoof.&lt;br /&gt;&lt;br /&gt;featuring Tracy Morgan, Pamela Anderson, Leslie Nielsen, Marion Ross, Jeffrey Tambor, and Regina Hall.&lt;br /&gt;&lt;br /&gt;Hell Superhero Movie may earn some merit in the fact that it\\'s a hell of a lot better than Meet the Spartans and Epic Movie. But with great responsibility comes one of the worst outings of 2008 to date. Laughless but a little less irritating than Meet the Spartans. And in the same sense much more forgettable than meet the Spartans. But maybe that\\'s a good reason. There are still some of us trying to scrape away the stain that was Meet the Spartans from our memory.&lt;br /&gt;&lt;br /&gt;My final verdict? Avoid, unless you\\'re one of thoses people who enjoy such car crash cinema. As bad as Date Movie and Scary Movie 2 but not quite as bad as Meet the Spartans or Epic Movie. Super Villain.'\nRound-trip:  scary movie [UNK] [UNK] movie [UNK] movie meet the [UNK] not another [UNK] movie and another [UNK] movie making [UNK] movie the [UNK] in a series that single [UNK] [UNK] the [UNK] genre now ill admit it i have a [UNK] [UNK] for [UNK] such as [UNK] and the [UNK] [UNK] but you know youve [UNK] a [UNK] so bad when you can see the [UNK] a [UNK] off in fact the only thing that might really [UNK] you into going to see this [UNK] is the incredibly funny but [UNK] [UNK] [UNK] [UNK] br you can tell he needs the money [UNK] that or he [UNK] to go down with the [UNK] like a good [UNK] would in no way is he [UNK] down this genre but hell hes not [UNK] it but if i feel sorry for [UNK] in this film its decent actor [UNK] [UNK] who is put through an [UNK] [UNK] of [UNK] the people who are put through the [UNK] [UNK] of [UNK] by far however is the audience forced to sit through [UNK] minutes of [UNK] [UNK] no [UNK] than [UNK] br after [UNK] [UNK] films in [UNK] police shows in the [UNK] [UNK] and hollywood [UNK] in scary movie 3 and 4 [UNK] david [UNK] sets his [UNK] [UNK] on the [UNK] genre with this [UNK] comedy [UNK] everything from [UNK] to [UNK] and [UNK] [UNK] br [UNK] after being [UNK] by a [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] begins to experience a [UNK] [UNK] now [UNK] [UNK] is as strong as [UNK] and he [UNK] the [UNK] of ten men [UNK] to use his [UNK] [UNK] to fight crime [UNK] [UNK] a special [UNK] and [UNK] the [UNK] of the [UNK] a [UNK] crime [UNK] [UNK] to [UNK] the [UNK] [UNK] for [UNK] [UNK] br but every [UNK] needs a [UNK] and after [UNK] [UNK] [UNK] [UNK] is [UNK] in the middle of an [UNK] gone [UNK] [UNK] he [UNK] the power to [UNK] the life [UNK] out of anyone he meets and becomes the [UNK] [UNK] [UNK] on [UNK] [UNK] the [UNK] attempts to [UNK] as much life [UNK] as possible as the [UNK] [UNK] sets out to take down his [UNK] and realize his [UNK] as a true hero [UNK] [UNK] [UNK] and [UNK] this [UNK] [UNK] br [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] and [UNK] [UNK] br hell [UNK] movie may [UNK] some [UNK] in the fact that its a hell of a lot better than meet the [UNK] and [UNK] movie but with great [UNK] comes one of the worst [UNK] of [UNK] to [UNK] [UNK] but a little less [UNK] than meet the [UNK] and in the same sense much more [UNK] than meet the [UNK] but maybe thats a good reason there are still some of us trying to [UNK] away the [UNK] that was meet the [UNK] from our [UNK] br my final [UNK] avoid unless youre one of [UNK] people who enjoy such car [UNK] cinema as bad as [UNK] movie and scary movie 2 but not quite as bad as meet the [UNK] or [UNK] movie [UNK] [UNK]\n\n\n\n\n\nCreaciÃ³n del modelo usando keras\n\n'''model = tf.keras.Sequential([\n    encoder,\n    tf.keras.layers.Embedding(\n        input_dim=len(encoder.get_vocabulary()),\n        output_dim=64,\n        # Se ignora los valores en 0\n        mask_zero=True),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64), merge_mode='concat'),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1)\n])'''\n\ndef rnn():\n    # Ahora utilizamos la API funcional de Keras\n    inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)  # El input serÃ¡ una cadena de texto\n    x = encoder(inputs)  # Aplicamos el encoder\n\n    x = tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary()), output_dim=64, mask_zero=True)(x)  # Capa de Embedding\n\n    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False), merge_mode='concat')(x)  # Capa LSTM Bidireccional\n\n    x = tf.keras.layers.Dense(64, activation='relu')(x)  # Capa densa\n    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)  # Capa de salida\n\n    # Definimos el modelo\n    model = tf.keras.Model(inputs, outputs)\n\n    return model\n\n# crear la red RNN\nmodel = rnn()\n\n# Compilamos el modelo\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n              optimizer=tf.keras.optimizers.Adam(1e-4),\n              metrics=['accuracy'])\n\n# Verificamos la estructura del modelo\nmodel.summary()\n\nModel: \"functional_1\"\n\n\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ Layer (type)        â”ƒ Output Shape      â”ƒ    Param # â”ƒ Connected to      â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ input_layer_1       â”‚ (None, 1)         â”‚          0 â”‚ -                 â”‚\nâ”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ text_vectorization  â”‚ (None, None)      â”‚          0 â”‚ input_layer_1[0]â€¦ â”‚\nâ”‚ (TextVectorization) â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ embedding_1         â”‚ (None, None, 64)  â”‚     64,000 â”‚ text_vectorizatiâ€¦ â”‚\nâ”‚ (Embedding)         â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ not_equal_1         â”‚ (None, None)      â”‚          0 â”‚ text_vectorizatiâ€¦ â”‚\nâ”‚ (NotEqual)          â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ bidirectional_1     â”‚ (None, 128)       â”‚     66,048 â”‚ embedding_1[0][0â€¦ â”‚\nâ”‚ (Bidirectional)     â”‚                   â”‚            â”‚ not_equal_1[0][0] â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_2 (Dense)     â”‚ (None, 64)        â”‚      8,256 â”‚ bidirectional_1[â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_3 (Dense)     â”‚ (None, 1)         â”‚         65 â”‚ dense_2[0][0]     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n Total params: 138,369 (540.50 KB)\n\n\n\n Trainable params: 138,369 (540.50 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\ntf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)\n\n\n\n\n\n\n\n\n\n# El texto crudo que quieres predecir\nsample_text = ('The movie was cool. The animation and the graphics were out of this world.')\n\n# No es necesario hacer la vectorizaciÃ³n manual aquÃ­, simplemente pasa el texto crudo al modelo\npredictions = model.predict(tf.constant([sample_text]))\n\n# Imprime la predicciÃ³n .. que debe ser casi 0.5 (neutralidad)\nprint(predictions[0])\n\n\n1/1 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 1s/step\n\n[0.4999323]\n\n\n\n\n\n\nEntrenamiento y evaluaciÃ³n del modelo\n\nhistory = model.fit(train_dataset, epochs=10,\n                    validation_data=test_dataset,\n                    validation_steps=30)\n\n\nEpoch 1/10\n\n782/782 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 39s 44ms/step - accuracy: 0.5798 - loss: 0.6706 - val_accuracy: 0.7823 - val_loss: 0.5019\n\nEpoch 2/10\n\n782/782 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 40s 45ms/step - accuracy: 0.7957 - loss: 0.4493 - val_accuracy: 0.8271 - val_loss: 0.4129\n\nEpoch 3/10\n\n782/782 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 41s 52ms/step - accuracy: 0.8513 - loss: 0.3502 - val_accuracy: 0.8427 - val_loss: 0.3751\n\nEpoch 4/10\n\n782/782 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 82s 52ms/step - accuracy: 0.8605 - loss: 0.3311 - val_accuracy: 0.8521 - val_loss: 0.3591\n\nEpoch 5/10\n\n782/782 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 74s 43ms/step - accuracy: 0.8665 - loss: 0.3233 - val_accuracy: 0.8698 - val_loss: 0.3445\n\nEpoch 6/10\n\n782/782 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 36s 46ms/step - accuracy: 0.8688 - loss: 0.3157 - val_accuracy: 0.8687 - val_loss: 0.3407\n\nEpoch 7/10\n\n782/782 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 82s 99ms/step - accuracy: 0.8722 - loss: 0.3120 - val_accuracy: 0.8531 - val_loss: 0.3814\n\nEpoch 8/10\n\n782/782 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 41s 46ms/step - accuracy: 0.8702 - loss: 0.3176 - val_accuracy: 0.8490 - val_loss: 0.3559\n\nEpoch 9/10\n\n782/782 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 41s 52ms/step - accuracy: 0.8701 - loss: 0.3108 - val_accuracy: 0.8646 - val_loss: 0.3362\n\nEpoch 10/10\n\n782/782 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 75s 44ms/step - accuracy: 0.8703 - loss: 0.3075 - val_accuracy: 0.8302 - val_loss: 0.3808\n\n\n\n\n\ntest_loss, test_acc = model.evaluate(test_dataset)\n\nprint('Test Loss:', test_loss)\nprint('Test Accuracy:', test_acc)\n\n\n782/782 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 17s 22ms/step - accuracy: 0.8340 - loss: 0.3708\n\nTest Loss: 0.37266772985458374\n\nTest Accuracy: 0.8334000110626221\n\n\n\n\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplot_graphs(history, 'accuracy')\nplt.ylim(None, 1)\nplt.subplot(1, 2, 2)\nplot_graphs(history, 'loss')\nplt.ylim(0, None)\n\n\n\n\n\n\n\n\n\n# realizar una predicciÃ³n des pues de entrenar\nsample_text = ('The movie was cool. The animation and the graphics were out of this world.')\npredictions = model.predict(tf.constant([sample_text]))\nprint(predictions)\n\n\n1/1 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 30ms/step\n\n[[0.6709632]]\n\n\n\n\n\n# realizar una predicciÃ³n des pues de entrenar\nsample_text = ('The movie was very cool. The animation and the graphics were out of this world.')\npredictions = model.predict(tf.constant([sample_text]))\nprint(predictions)\n\n\n1/1 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 34ms/step\n\n[[0.71158457]]\n\n\n\n\nCÃ³mo podrÃ­amos agregar mÃ¡s capas LSTM para aumentar la complejidad del modelo?\n\n\n\nlayered_bidirectional.png\n\n\nLas capas recurrentes de Keras tienen dos modos disponibles que son controlados por el argumento del constructor return_sequences:\n\nSi es False devuelve sÃ³lo la Ãºltima salida para cada secuencia de entrada (un tensor 2D de forma (batch_size, output_features)). Este es el valor por defecto, utilizado en el modelo anterior.\nSi es True se devuelven las secuencias completas de salidas sucesivas para cada paso de tiempo (un tensor 3D de forma (batch_size, timesteps, output_features)).\n\n# model = tf.keras.Sequential([\n    encoder,\n    tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1)\n])"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2b_Resolviendo_desafios_con_RNNs_usando_keras.html#leer-datos",
    "href": "semana_2/notebooks/Nb_2b_Resolviendo_desafios_con_RNNs_usando_keras.html#leer-datos",
    "title": "Resolviendo diversidad de tareas con RNNs",
    "section": "Leer datos",
    "text": "Leer datos\n\ndf, date_time = read_dataset_clima()\n\n\ndf\n\n\n    \n\n\n\n\n\n\np (mbar)\nT (degC)\nTpot (K)\nTdew (degC)\nrh (%)\nVPmax (mbar)\nVPact (mbar)\nVPdef (mbar)\nsh (g/kg)\nH2OC (mmol/mol)\nrho (g/m**3)\nwv (m/s)\nmax. wv (m/s)\nwd (deg)\n\n\n\n\n5\n996.50\n-8.05\n265.38\n-8.78\n94.40\n3.33\n3.14\n0.19\n1.96\n3.15\n1307.86\n0.21\n0.63\n192.7\n\n\n11\n996.62\n-8.88\n264.54\n-9.77\n93.20\n3.12\n2.90\n0.21\n1.81\n2.91\n1312.25\n0.25\n0.63\n190.3\n\n\n17\n996.84\n-8.81\n264.59\n-9.66\n93.50\n3.13\n2.93\n0.20\n1.83\n2.94\n1312.18\n0.18\n0.63\n167.2\n\n\n23\n996.99\n-9.05\n264.34\n-10.02\n92.60\n3.07\n2.85\n0.23\n1.78\n2.85\n1313.61\n0.10\n0.38\n240.0\n\n\n29\n997.46\n-9.63\n263.72\n-10.65\n92.20\n2.94\n2.71\n0.23\n1.69\n2.71\n1317.19\n0.40\n0.88\n157.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n420521\n1002.18\n-0.98\n272.01\n-5.36\n72.00\n5.69\n4.09\n1.59\n2.54\n4.08\n1280.70\n0.87\n1.36\n190.6\n\n\n420527\n1001.40\n-1.40\n271.66\n-6.84\n66.29\n5.51\n3.65\n1.86\n2.27\n3.65\n1281.87\n1.02\n1.92\n225.4\n\n\n420533\n1001.19\n-2.75\n270.32\n-6.90\n72.90\n4.99\n3.64\n1.35\n2.26\n3.63\n1288.02\n0.71\n1.56\n158.7\n\n\n420539\n1000.65\n-2.89\n270.22\n-7.15\n72.30\n4.93\n3.57\n1.37\n2.22\n3.57\n1288.03\n0.35\n0.68\n216.7\n\n\n420545\n1000.11\n-3.93\n269.23\n-8.09\n72.60\n4.56\n3.31\n1.25\n2.06\n3.31\n1292.41\n0.56\n1.00\n202.6\n\n\n\n\n70091 rows Ã— 14 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\ndate_time\n\n\n\n\n\n\n\n\nDate Time\n\n\n\n\n5\n2009-01-01 01:00:00\n\n\n11\n2009-01-01 02:00:00\n\n\n17\n2009-01-01 03:00:00\n\n\n23\n2009-01-01 04:00:00\n\n\n29\n2009-01-01 05:00:00\n\n\n...\n...\n\n\n420521\n2016-12-31 19:10:00\n\n\n420527\n2016-12-31 20:10:00\n\n\n420533\n2016-12-31 21:10:00\n\n\n420539\n2016-12-31 22:10:00\n\n\n420545\n2016-12-31 23:10:00\n\n\n\n\n70091 rows Ã— 1 columns\ndtype: datetime64[ns]\n\n\n\nplot_variables(df, date_time)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_df, val_df, test_df, num_features, column_indices = split_data(df)\ntrain_df.shape, val_df.shape, test_df.shape\n\n((49063, 14), (14018, 14), (7010, 14))\n\n\n\ntrain_df.head()\n\n\n    \n\n\n\n\n\n\np (mbar)\nT (degC)\nTpot (K)\nTdew (degC)\nrh (%)\nVPmax (mbar)\nVPact (mbar)\nVPdef (mbar)\nsh (g/kg)\nH2OC (mmol/mol)\nrho (g/m**3)\nwv (m/s)\nmax. wv (m/s)\nwd (deg)\n\n\n\n\n5\n996.50\n-8.05\n265.38\n-8.78\n94.4\n3.33\n3.14\n0.19\n1.96\n3.15\n1307.86\n0.21\n0.63\n192.7\n\n\n11\n996.62\n-8.88\n264.54\n-9.77\n93.2\n3.12\n2.90\n0.21\n1.81\n2.91\n1312.25\n0.25\n0.63\n190.3\n\n\n17\n996.84\n-8.81\n264.59\n-9.66\n93.5\n3.13\n2.93\n0.20\n1.83\n2.94\n1312.18\n0.18\n0.63\n167.2\n\n\n23\n996.99\n-9.05\n264.34\n-10.02\n92.6\n3.07\n2.85\n0.23\n1.78\n2.85\n1313.61\n0.10\n0.38\n240.0\n\n\n29\n997.46\n-9.63\n263.72\n-10.65\n92.2\n2.94\n2.71\n0.23\n1.69\n2.71\n1317.19\n0.40\n0.88\n157.0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\n\ntrain_df, val_df, test_df = normalizacion_datos(train_df, val_df, test_df)\ntrain_df.describe()\n\n\n    \n\n\n\n\n\n\np (mbar)\nT (degC)\nTpot (K)\nTdew (degC)\nrh (%)\nVPmax (mbar)\nVPact (mbar)\nVPdef (mbar)\nsh (g/kg)\nH2OC (mmol/mol)\nrho (g/m**3)\nwv (m/s)\nmax. wv (m/s)\nwd (deg)\n\n\n\n\ncount\n4.906300e+04\n4.906300e+04\n4.906300e+04\n4.906300e+04\n4.906300e+04\n49063.000000\n4.906300e+04\n49063.000000\n4.906300e+04\n4.906300e+04\n4.906300e+04\n4.906300e+04\n4.906300e+04\n4.906300e+04\n\n\nmean\n2.027515e-16\n-1.946415e-16\n9.685730e-16\n1.853728e-17\n-6.719765e-16\n0.000000\n-2.270817e-16\n0.000000\n-1.506154e-16\n1.807385e-16\n-2.321795e-15\n1.540912e-16\n-1.616219e-16\n-2.178131e-16\n\n\nstd\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000\n1.000000e+00\n1.000000\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n\n\nmin\n-9.045695e+00\n-3.682079e+00\n-3.707266e+00\n-4.216645e+00\n-3.746587e+00\n-1.609554\n-2.030996e+00\n-0.829861\n-2.022853e+00\n-2.031986e+00\n-3.846513e+00\n-1.403684e+00\n-1.535423e+00\n-1.977937e+00\n\n\n25%\n-6.093840e-01\n-7.069026e-01\n-6.939982e-01\n-6.697392e-01\n-6.581569e-01\n-0.750526\n-7.786971e-01\n-0.657581\n-7.762466e-01\n-7.761335e-01\n-7.116941e-01\n-7.390786e-01\n-7.595612e-01\n-6.326198e-01\n\n\n50%\n5.467421e-02\n9.450477e-03\n1.318575e-02\n5.168967e-02\n1.989686e-01\n-0.222892\n-1.561120e-01\n-0.383594\n-1.548152e-01\n-1.540757e-01\n-7.847992e-02\n-2.308512e-01\n-2.250786e-01\n2.674928e-01\n\n\n75%\n6.548575e-01\n7.200265e-01\n7.123465e-01\n7.530390e-01\n8.150841e-01\n0.533469\n6.684569e-01\n0.268164\n6.650251e-01\n6.651626e-01\n6.442168e-01\n4.793641e-01\n5.206108e-01\n6.932912e-01\n\n\nmax\n2.913378e+00\n3.066661e+00\n3.041354e+00\n2.647686e+00\n1.455361e+00\n5.846190\n4.489514e+00\n7.842254\n4.550843e+00\n4.524268e+00\n4.310438e+00\n7.724863e+00\n8.593884e+00\n2.131645e+00"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2b_Resolviendo_desafios_con_RNNs_usando_keras.html#creaciÃ³n-de-ventanas",
    "href": "semana_2/notebooks/Nb_2b_Resolviendo_desafios_con_RNNs_usando_keras.html#creaciÃ³n-de-ventanas",
    "title": "Resolviendo diversidad de tareas con RNNs",
    "section": "CreaciÃ³n de ventanas",
    "text": "CreaciÃ³n de ventanas\nEjemplo para un problema que dado las Ãºltimas 6 mediciones de temperatura va a predecir la siguiente hora de temperatura.\n\n\n\nsplit_window.png\n\n\n\nEjemplo creaciÃ³n dataset ventanas\n\n# Crear un array de 10 minutos y 3 caracterÃ­sticas (features=2), con valores consecutivos para facilitar el entendimiento\ndatae = np.arange(10 * 3).reshape(10, 3)\n\n# Crear un DataFrame para mostrar los datos de manera mÃ¡s clara\ndfe = pd.DataFrame(datae, columns=['Feature_1', 'Feature_2', 'Target'])\ndfe.index.name = 'Minuto'\n\n# Mostrar el DataFrame\ndfe\n\n\n    \n\n\n\n\n\n\nFeature_1\nFeature_2\nTarget\n\n\nMinuto\n\n\n\n\n\n\n\n0\n0\n1\n2\n\n\n1\n3\n4\n5\n\n\n2\n6\n7\n8\n\n\n3\n9\n10\n11\n\n\n4\n12\n13\n14\n\n\n5\n15\n16\n17\n\n\n6\n18\n19\n20\n\n\n7\n21\n22\n23\n\n\n8\n24\n25\n26\n\n\n9\n27\n28\n29\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n# La variable a predecir es la Ãºltima columna (columna final)\ntargets = datae[2:, -1]  # Los targets son el valor de la columna final (variable a predecir) desplazados por 2 pasos\ntargets\n\narray([ 8, 11, 14, 17, 20, 23, 26, 29])\n\n\n\n# Crear el dataset de series temporales\nds = tf.keras.utils.timeseries_dataset_from_array(\n    data=datae[:-1],  # Todas las filas menos la Ãºltima, porque no hay target para la Ãºltima fila\n    targets=targets,  # Los valores a predecir son la columna final de la siguiente fila\n    sequence_length=2,  # Usamos secuencias de 2 minutos\n    sequence_stride=1,  # Stride de 1 para obtener todas las posibles ventanas\n    shuffle=False,  # Barajamos las secuencias\n    batch_size=2  # Agrupamos en lotes de 2 secuencias\n)\n\n\n# Mostrar los primeros 5 lotes de inputs y labels, con un formato mejorado\nfor batch_num, batch in enumerate(ds.take(5), 1):\n    inputs, labels = batch\n    print(f\"Batch {batch_num}:\")\n    print(f\"Inputs shape: {inputs.shape}\")\n    print(\"Inputs:\")\n\n    # Imprimir cada secuencia de inputs con su respectiva etiqueta al final\n    for i, (input_sequence, label) in enumerate(zip(inputs.numpy(), labels.numpy()), 1):\n        print(f\"  Sequence {i}:\")\n        print(f\" {input_sequence} -&gt; Target: {label}\")\n\n    print(\"\\n\" + \"-\"*50 + \"\\n\")\n\nBatch 1:\nInputs shape: (2, 2, 3)\nInputs:\n  Sequence 1:\n [[0 1 2]\n [3 4 5]] -&gt; Target: 8\n  Sequence 2:\n [[3 4 5]\n [6 7 8]] -&gt; Target: 11\n\n--------------------------------------------------\n\nBatch 2:\nInputs shape: (2, 2, 3)\nInputs:\n  Sequence 1:\n [[ 6  7  8]\n [ 9 10 11]] -&gt; Target: 14\n  Sequence 2:\n [[ 9 10 11]\n [12 13 14]] -&gt; Target: 17\n\n--------------------------------------------------\n\nBatch 3:\nInputs shape: (2, 2, 3)\nInputs:\n  Sequence 1:\n [[12 13 14]\n [15 16 17]] -&gt; Target: 20\n  Sequence 2:\n [[15 16 17]\n [18 19 20]] -&gt; Target: 23\n\n--------------------------------------------------\n\nBatch 4:\nInputs shape: (2, 2, 3)\nInputs:\n  Sequence 1:\n [[18 19 20]\n [21 22 23]] -&gt; Target: 26\n  Sequence 2:\n [[21 22 23]\n [24 25 26]] -&gt; Target: 29\n\n--------------------------------------------------\n\n\n\n\n\nRetomando\n\nMAX_EPOCHS = 20\n\ndef compile_and_fit(model, window, patience=2):\n  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                                    patience=patience,\n                                                    mode='min')\n\n  model.compile(loss=tf.keras.losses.MeanSquaredError(),\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[tf.keras.metrics.MeanAbsoluteError()])\n\n  history = model.fit(window.train, epochs=MAX_EPOCHS,\n                      validation_data=window.val,\n                      callbacks=[early_stopping])\n  return history\n\n\nWIDTH_TEMP = 6\nwindow = WindowGenerator(\n    input_width=WIDTH_TEMP,\n    label_width=1,\n    shift=1,\n    train_df = train_df,\n    val_df = val_df,\n    test_df=test_df,\n    label_columns=['T (degC)'])\n\nwindow\n\nTotal window size: 7\nInput indices: [0 1 2 3 4 5]\nLabel indices: [6]\nLabel column name(s): ['T (degC)']\n\n\n\n# Stack three slices, the length of the total window.\nexample_window = tf.stack([np.array(train_df[:window.total_window_size]),\n                           np.array(train_df[100:100+window.total_window_size]),\n                           np.array(train_df[200:200+window.total_window_size])])\n\nexample_inputs, example_labels = window.split_window(example_window)\n\nprint('All shapes are: (batch, time, features)')\nprint(f'Window shape: {example_window.shape}')\nprint(f'Inputs shape: {example_inputs.shape}')\nprint(f'Labels shape: {example_labels.shape}')\n\nAll shapes are: (batch, time, features)\nWindow shape: (3, 7, 14)\nInputs shape: (3, 6, 14)\nLabels shape: (3, 1, 1)\n\n\n\n# graficar los datos en las ventanas\nwindow.plot()\n\n\n\n\n\n\n\n\n\nlstm_model = tf.keras.models.Sequential([\n    # Shape [batch, time, features] =&gt; [batch, time, lstm_units]\n    tf.keras.layers.LSTM(32, return_sequences=True),\n    # Shape =&gt; [batch, time, features]\n    tf.keras.layers.Dense(units=1)\n])\n\n\nprint('Input shape:', window.example[0].shape)\nprint('Output shape:', lstm_model(window.example[0]).shape)\n\nInput shape: (32, 6, 14)\nOutput shape: (32, 6, 1)\n\n\n\nhistory = compile_and_fit(lstm_model, window)\n\nval_performance = {}\nperformance = {}\nval_performance['LSTM'] = lstm_model.evaluate(window.val, return_dict=True)\nperformance['LSTM'] = lstm_model.evaluate(window.test, verbose=0, return_dict=True)\n\n\nEpoch 1/20\n\n1534/1534 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 8ms/step - loss: 0.1676 - mean_absolute_error: 0.2821 - val_loss: 0.0809 - val_mean_absolute_error: 0.1999\n\nEpoch 2/20\n\n1534/1534 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 12s 8ms/step - loss: 0.0787 - mean_absolute_error: 0.1963 - val_loss: 0.0758 - val_mean_absolute_error: 0.1910\n\nEpoch 3/20\n\n1534/1534 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 12s 8ms/step - loss: 0.0735 - mean_absolute_error: 0.1877 - val_loss: 0.0735 - val_mean_absolute_error: 0.1872\n\nEpoch 4/20\n\n1534/1534 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 18s 6ms/step - loss: 0.0716 - mean_absolute_error: 0.1845 - val_loss: 0.0726 - val_mean_absolute_error: 0.1848\n\nEpoch 5/20\n\n1534/1534 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 11s 7ms/step - loss: 0.0704 - mean_absolute_error: 0.1826 - val_loss: 0.0723 - val_mean_absolute_error: 0.1830\n\nEpoch 6/20\n\n1534/1534 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 20s 7ms/step - loss: 0.0696 - mean_absolute_error: 0.1809 - val_loss: 0.0715 - val_mean_absolute_error: 0.1817\n\nEpoch 7/20\n\n1534/1534 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 20s 7ms/step - loss: 0.0692 - mean_absolute_error: 0.1806 - val_loss: 0.0704 - val_mean_absolute_error: 0.1806\n\nEpoch 8/20\n\n1534/1534 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 20s 7ms/step - loss: 0.0685 - mean_absolute_error: 0.1791 - val_loss: 0.0708 - val_mean_absolute_error: 0.1817\n\nEpoch 9/20\n\n1534/1534 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 23s 8ms/step - loss: 0.0681 - mean_absolute_error: 0.1784 - val_loss: 0.0709 - val_mean_absolute_error: 0.1836\n\n438/438 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 4ms/step - loss: 0.0694 - mean_absolute_error: 0.1819\n\n\n\n\n\nperformance\n\n{'LSTM': {'loss': 0.06843273341655731,\n  'mean_absolute_error': 0.1830073744058609}}\n\n\n\nval_performance\n\n{'LSTM': {'loss': 0.07088509947061539,\n  'mean_absolute_error': 0.18363438546657562}}"
  },
  {
    "objectID": "semana_2/index.html",
    "href": "semana_2/index.html",
    "title": "Semana 2: Arquitecturas de Redes Neuronales",
    "section": "",
    "text": "Expandimos nuestro conocimiento explorando diversas arquitecturas de redes neuronales y sus aplicaciones.\nTemas Clave:\n\nDiversas arquitecturas de redes neuronales (DNN, CNN, RNN).\nAplicaciones y ventajas de cada arquitectura.\nIntroducciÃ³n a arquitecturas avanzadas (Transformers, GANs, Redes de Grafos).\n\nEnfoque PrÃ¡ctico: AprenderÃ¡s a identificar y seleccionar la arquitectura mÃ¡s adecuada para diferentes tipos de problemas. ImplementarÃ¡s modelos DNN, CNN y RNN en proyectos prÃ¡cticos para experimentar con su desempeÃ±o.\nMateriales de la Semana:\n\nSlides de la Semana 2 (PrÃ³ximamente)\nNotebook: Implementando una CNN usando Keras\nNotebook: Resolviendo desafÃ­os con RNNs usando Keras\nNotebook: TraducciÃ³n usando Transformers y Keras"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html",
    "href": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html",
    "title": "Implementado un PerceptrÃ³n multi-capa usando frameworks y MLFlow",
    "section": "",
    "text": "Open In Colab\n#@title Importar librerÃ­as\n#importar librerÃ­as necesarias\nimport json\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Input\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.datasets import mnist\n\n!pip install mlflow --quiet\n\nimport mlflow\nimport mlflow.sklearn\nimport mlflow.tensorflow\nfrom mlflow.tracking import MlflowClient\n\n\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 26.7/26.7 MB 19.7 MB/s eta 0:00:00\n\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5.7/5.7 MB 32.5 MB/s eta 0:00:00\n\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 233.5/233.5 kB 8.5 MB/s eta 0:00:00\n\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 147.8/147.8 kB 5.6 MB/s eta 0:00:00\n\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 114.7/114.7 kB 3.8 MB/s eta 0:00:00\n\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 85.0/85.0 kB 2.3 MB/s eta 0:00:00\n\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 569.1/569.1 kB 10.8 MB/s eta 0:00:00\n\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 203.2/203.2 kB 3.6 MB/s eta 0:00:00\n\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 78.6/78.6 kB 4.4 MB/s eta 0:00:00\n#@title Configurando MLFlow\n# configurar que el servidor de trackin sea localhost con una BD sqlite como el almacenamiento para almacenar el proceso de tracking\n\nlocal_registry = \"sqlite:///mlruns.db\"\nprint(f\"Ejecutando registro en modo local={local_registry}\")\nmlflow.set_tracking_uri(local_registry)\n\nEjecutando registro en modo local=sqlite:///mlruns.db\n#@title Funciones complementarias\ndef plot_samples_dataset(X, y):\n    # Convertir las etiquetas a enteros\n    y = y.astype(int)\n\n    # Crear la grilla de 4x4\n    fig, axes = plt.subplots(4, 4, figsize=(6, 6))\n    fig.suptitle('Grilla de imÃ¡genes del dataset MNIST')\n\n    # Iterar para mostrar las primeras 16 imÃ¡genes con sus etiquetas\n    for i, ax in enumerate(axes.flat):\n        img = X.iloc[i].values.reshape(28, 28)\n        label = y[i]\n        ax.imshow(img, cmap='gray')\n        ax.set_title(f'Label: {label}')\n        ax.axis('off')\n\n    plt.show()\n\ndef plot_curva_aprendizaje(mlp):\n    plt.figure(figsize=(8, 5))\n    plt.plot(mlp.loss_curve_, marker='o')\n    plt.title('PÃ©rdida durante el entrenamiento del MLP por iteraciÃ³n')\n    plt.xlabel('IteraciÃ³n')\n    plt.ylabel('PÃ©rdida (Loss)')\n    plt.grid()\n    plt.show()\n\ndef plot_matriz_confusion(cm, nombre):\n    # Visualizar la matriz de confusiÃ³n usando Seaborn\n    plt.figure(figsize=(10, 7))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n    plt.xlabel('Etiqueta predicha')\n    plt.ylabel('Etiqueta real')\n    plt.title('Matriz de ConfusiÃ³n para el MLP en el dataset MNIST')\n    plt.show()\n    plt.savefig(f\"{nombre}.png\")\n\ndef encontrar_dim_imagen(n_neurons):\n    \"\"\"\n    Encuentra la mejor forma cuadrada (filas, columnas) para una cantidad dada de neuronas.\n    \"\"\"\n    side_length = int(np.sqrt(n_neurons))  # Calcular la raÃ­z cuadrada del nÃºmero de neuronas\n    if side_length * side_length == n_neurons:\n        return (side_length, side_length)  # Si es un cuadrado perfecto\n    else:\n        # Si no es un cuadrado perfecto, buscamos la mejor aproximaciÃ³n (filas, columnas)\n        for i in range(side_length, 0, -1):\n            if n_neurons % i == 0:\n                return (i, n_neurons // i)  # Devolver filas y columnas\n    return (n_neurons, 1)  # Si no encuentra, retornar en forma de vector (n_neurons, 1)\n\ndef visualizacion_pesos_mlp(mlp):\n    # Definir la figura con 3 filas y 5 columnas\n    fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n\n    # Asignar las dimensiones para visualizar cada capa\n    layer_shapes = [encontrar_dim_imagen(layer.shape[0]) for layer in mlp.coefs_]\n\n    # Recorrer cada capa de coeficientes del MLP\n    for layer_index, (layer_coefs, ax_row) in enumerate(zip(mlp.coefs_, axes)):\n        # Seleccionar aleatoriamente 5 neuronas de la capa actual\n        num_neurons = layer_coefs.shape[1]\n        random_neurons = random.sample(range(num_neurons), 5)\n\n        # Obtener la forma de visualizaciÃ³n para esta capa\n        layer_shape = layer_shapes[layer_index]\n\n        vmin, vmax = layer_coefs.min(), layer_coefs.max()\n\n        # Visualizar las neuronas seleccionadas\n        for neuron_index, ax in zip(random_neurons, ax_row):\n            # Seleccionar los pesos de la neurona especÃ­fica y reestructurarlos en una matriz 2D\n            neuron_weights = layer_coefs[:, neuron_index].reshape(layer_shape)\n            # Dibujar la imagen de los pesos de la neurona\n            ax.matshow(neuron_weights, cmap=plt.cm.gray, vmin=0.5 * vmin, vmax=0.5 * vmax)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.set_title(f'Capa {layer_index+1}, Neurona {neuron_index}')\n\n    plt.suptitle('VisualizaciÃ³n de Pesos de las Neuronas en las Capas Ocultas')\n    plt.tight_layout()\n    plt.show()\n\ndef plot_loss_historia_keras(history):\n    # Graficar el histÃ³rico de pÃ©rdida durante el entrenamiento\n    plt.plot(history.history['loss'], label='PÃ©rdida de Entrenamiento')\n    plt.plot(history.history['val_loss'], label='PÃ©rdida de ValidaciÃ³n')\n    plt.title('PÃ©rdida durante el Entrenamiento')\n    plt.xlabel('Ã‰poca')\n    plt.ylabel('PÃ©rdida')\n    plt.legend()\n    plt.show()\n\ndef plot_acc_historia_keras(history):\n    # Graficar la precisiÃ³n durante el entrenamiento\n    plt.plot(history.history['accuracy'], label='PrecisiÃ³n de Entrenamiento')\n    plt.plot(history.history['val_accuracy'], label='PrecisiÃ³n de ValidaciÃ³n')\n    plt.title('PrecisiÃ³n durante el Entrenamiento')\n    plt.xlabel('Ã‰poca')\n    plt.ylabel('PrecisiÃ³n')\n    plt.legend()\n    plt.show()\n\ndef visualizacion_pesos_mlp_keras(model):\n    # Obtener los pesos del modelo (par de listas [pesos, biases] para cada capa)\n    weights = model.get_weights()\n\n    # Extraer solo los pesos de cada capa oculta, ignorando los bias\n    layer_weights = [weights[i] for i in range(0, len(weights), 2)]  # Solo los pesos, no los sesgos\n\n    # Definir la figura con 3 filas (una por cada capa) y 5 columnas (5 neuronas al azar)\n    fig, axes = plt.subplots(len(layer_weights), 5, figsize=(15, 9))\n\n    # Calcular las formas de cada capa de manera dinÃ¡mica\n    layer_shapes = [encontrar_dim_imagen(layer.shape[0]) for layer in layer_weights]\n\n    # Recorrer cada capa y sus pesos\n    for layer_index, (layer_coefs, ax_row) in enumerate(zip(layer_weights, axes)):\n        # Seleccionar aleatoriamente 5 neuronas de la capa actual\n        num_neurons = layer_coefs.shape[1]\n        random_neurons = random.sample(range(num_neurons), 5)\n\n        # Obtener la forma de visualizaciÃ³n para esta capa\n        layer_shape = layer_shapes[layer_index]\n        vmin, vmax = layer_coefs.min(), layer_coefs.max()\n\n\n        # Visualizar las neuronas seleccionadas\n        for neuron_index, ax in zip(random_neurons, ax_row):\n            # Seleccionar los pesos de la neurona especÃ­fica y reestructurarlos en una matriz 2D\n            neuron_weights = layer_coefs[:, neuron_index].reshape(layer_shape)\n            # Dibujar la imagen de los pesos de la neurona\n            ax.matshow(neuron_weights, cmap=plt.cm.gray, vmin=0.5 * vmin, vmax=0.5 * vmax)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.set_title(f'Capa {layer_index+1}, Neurona {neuron_index}')\n\n    plt.suptitle('VisualizaciÃ³n de Pesos de las Neuronas en las Capas Ocultas de Keras')\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html#evaluaciÃ³n-completa",
    "href": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html#evaluaciÃ³n-completa",
    "title": "Implementado un PerceptrÃ³n multi-capa usando frameworks y MLFlow",
    "section": "EvaluaciÃ³n completa",
    "text": "EvaluaciÃ³n completa\nRealizaremos una evaluaciÃ³n completa revisando el rendimiento en ambos conjuntos, seguidamente generaremos el reporte de clasificaciÃ³n y la matriz de confusiÃ³n.\n\nprint(f\"Training set score: {mlp.score(X_train, y_train):.3f}\")\nprint(f\"Test set score: {mlp.score(X_test, y_test):.3f}\")\n\nTraining set score: 1.000\nTest set score: 0.982\n\n\n\n# re abrir un run anterior para registrar mÃ¡s datos\nmlflow.start_run(run_id=run_id)\n\n# Realizar predicciones\ny_pred = mlp.predict(X_test)\n\n# Imprimir el reporte de mÃ©tricas\nprint(\"Reporte de ClasificaciÃ³n del MLP en MNIST:\\n\")\nreport = classification_report(y_test, y_pred)\nprint(report)\n\n# registrar artefacto\nwith open(\"classification_report.json\", \"w\") as f:\n    json.dump(report, f)\nmlflow.log_artifact(\"classification_report.json\")\n\n# Generar la matriz de confusiÃ³n\ncm = confusion_matrix(y_test, y_pred)\n\n# visualizar la matriz de confusiÃ³n\nplot_matriz_confusion(cm, nombre='confusion_matrix')\n# Guardar la visualizaciÃ³n como imagen y registrarla en MLflow\nmlflow.log_artifact(\"confusion_matrix.png\")\n\n# Finalizar el `run` si ya no se va a registrar nada mÃ¡s\nmlflow.end_run()\n\nReporte de ClasificaciÃ³n del MLP en MNIST:\n\n              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99      2058\n           1       0.99      0.99      0.99      2364\n           2       0.98      0.98      0.98      2133\n           3       0.98      0.98      0.98      2176\n           4       0.98      0.98      0.98      1936\n           5       0.98      0.98      0.98      1915\n           6       0.98      0.99      0.99      2088\n           7       0.98      0.98      0.98      2248\n           8       0.98      0.97      0.97      1992\n           9       0.98      0.97      0.98      2090\n\n    accuracy                           0.98     21000\n   macro avg       0.98      0.98      0.98     21000\nweighted avg       0.98      0.98      0.98     21000\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\nEjecutar MLFlow UI\n\n# Ejecutar tracking UI en background\nget_ipython().system_raw(\"mlflow ui --backend-store-uri sqlite:///mlruns.db --port 5000 &\")\n\n\n# Crear un tunel remoto usando ngrok.com\n!pip install -U pyngrok --quiet\nfrom pyngrok import ngrok\n\n# Terminate open tunnels if exist\nngrok.kill()\n\n# Colocar el token\n# Coloque su authtoken obtenido de https://dashboard.ngrok.com/auth\nNGROK_AUTH_TOKEN = \"2oR7ctXTRQ7kJl8csoQXXoxsb6V_72fsChgD1kFQN5MkVr7U3\"\nngrok.set_auth_token(NGROK_AUTH_TOKEN)\n\n# Abrir el tunel http en el puerto 5000 para http://localhost:5000\npublic_url = ngrok.connect(\"http://localhost:5000\", proto='http')\nprint(\"MLflow Tracking UI:\", public_url)\n\nMLflow Tracking UI: NgrokTunnel: \"https://3ec3-34-125-33-68.ngrok-free.app\" -&gt; \"http://localhost:5000\""
  },
  {
    "objectID": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html#visualizaciÃ³n-de-pesos",
    "href": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html#visualizaciÃ³n-de-pesos",
    "title": "Implementado un PerceptrÃ³n multi-capa usando frameworks y MLFlow",
    "section": "VisualizaciÃ³n de pesos",
    "text": "VisualizaciÃ³n de pesos\n\nvisualizacion_pesos_mlp(mlp)\n\n\n\n\n\n\n\n\n\nTutoriales relacionados\n\nAnÃ¡lisis de la variaciÃ³n del parametro de regularizaciÃ³n alpha\nComparaciÃ³n de las diferentes estrategias de aprendizaje"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html#evaluaciÃ³n-completa-1",
    "href": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html#evaluaciÃ³n-completa-1",
    "title": "Implementado un PerceptrÃ³n multi-capa usando frameworks y MLFlow",
    "section": "EvaluaciÃ³n completa",
    "text": "EvaluaciÃ³n completa\n\ny_test_categorical = to_categorical(y_test)\n\nscore = mlp_keras.evaluate(X_test.values.astype(float), y_test_categorical, batch_size=128)\n\nscore\n\n\n165/165 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - accuracy: 0.9481 - loss: 0.1761\n\n\n\n\n[0.17650671303272247, 0.9479047656059265]\n\n\n\n# Realizar predicciones en el conjunto de prueba\ny_pred = mlp_keras.predict(X_test.values.astype(float))\n\n# Convertir las predicciones en etiquetas (la clase con mayor probabilidad)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true = y_test.values.astype(int)  # Las etiquetas reales del conjunto de prueba\n\n\n657/657 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 4s 6ms/step\n\n\n\n\n\n# Generar el reporte de clasificaciÃ³n\nprint(\"Reporte de ClasificaciÃ³n para el MLP en MNIST:\\n\")\nprint(classification_report(y_true, y_pred_classes))\n\n# Crear la matriz de confusiÃ³n\ncm = confusion_matrix(y_true, y_pred_classes)\n\n# Visualizar la matriz de confusiÃ³n usando Seaborn\nplot_matriz_confusion(cm)\n\nReporte de ClasificaciÃ³n para el MLP en MNIST:\n\n              precision    recall  f1-score   support\n\n           0       0.96      0.97      0.97      2058\n           1       0.96      0.98      0.97      2364\n           2       0.96      0.93      0.95      2133\n           3       0.93      0.93      0.93      2176\n           4       0.94      0.96      0.95      1936\n           5       0.95      0.93      0.94      1915\n           6       0.96      0.97      0.96      2088\n           7       0.95      0.95      0.95      2248\n           8       0.95      0.92      0.93      1992\n           9       0.93      0.93      0.93      2090\n\n    accuracy                           0.95     21000\n   macro avg       0.95      0.95      0.95     21000\nweighted avg       0.95      0.95      0.95     21000"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html#visualizaciÃ³n-de-pesos-1",
    "href": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html#visualizaciÃ³n-de-pesos-1",
    "title": "Implementado un PerceptrÃ³n multi-capa usando frameworks y MLFlow",
    "section": "VisualizaciÃ³n de pesos",
    "text": "VisualizaciÃ³n de pesos\n\nvisualizacion_pesos_mlp_keras(mlp_keras)"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html#actividad-extra-clase",
    "href": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html#actividad-extra-clase",
    "title": "Implementado un PerceptrÃ³n multi-capa usando frameworks y MLFlow",
    "section": "Actividad extra clase",
    "text": "Actividad extra clase\nUsando la siguiente documentaciÃ³n: https://mlflow.org/docs/latest/deep-learning/tensorflow/guide/index.html realizar un seguimiento y registro del MLP construido en keras/tensorflow."
  },
  {
    "objectID": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html",
    "href": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html",
    "title": "Implementando un PerceptrÃ³n Multi-capa (MLP) desde CERO usando python",
    "section": "",
    "text": "Open In Colab\n#@title Importar librerÃ­as\n#importar librerÃ­as necesarias\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_moons\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\n#@title Funciones complementarias\ndef plot_dataset(X_train, y_train, X_test, y_test):\n    # TamaÃ±o de paso en la grilla de valores\n    # (para la visualizaciÃ³n del espacio de caracterÃ­sticas)\n    h = 0.02\n\n    # Definir los lÃ­mites del grÃ¡fico en el eje x e y basados\n    # en los datos de entrenamiento\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n\n    # Crear una malla de puntos para cubrir el espacio de caracterÃ­sticas\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n    # CreaciÃ³n del lienzo para visualizar los datos\n    fig, ax = plt.subplots(1,1, figsize=(8, 5))\n\n    # Agregar titulo a la grafica\n    ax.set_title(\"Dataset linealmente no separable\")\n\n    # Agregar nombres a cada eje de caracteristica\n    ax.set_xlabel(\"CaracterÃ­stica x_1\")\n    ax.set_ylabel(\"CaracterÃ­stica x_2\")\n\n    # Puntos de entrenamiento\n    ax.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1],\n               c=\"#FF0000\", edgecolors=\"k\", label='Clase de entrenamiento 1')\n    ax.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1],\n               c=\"#0000FF\", edgecolors=\"k\", label='Clase de entrenamiento 2')\n\n    # Puntos de prueba\n    ax.scatter(X_test[y_test == 0, 0], X_test[y_test == 0, 1],\n               c=\"#FF0000\", edgecolors=\"k\", alpha=0.6, label='Clase de prueba 1')\n    ax.scatter(X_test[y_test == 1, 0], X_test[y_test == 1, 1],\n               c=\"#0000FF\", edgecolors=\"k\", alpha=0.6, label='Clase de prueba 2')\n\n    # Establecer los lÃ­mites del grÃ¡fico para asegurar que todos los puntos sean visibles\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n\n    # Eliminar las marcas en los ejes x e y para un grÃ¡fico mÃ¡s limpio\n    ax.set_xticks(())\n    ax.set_yticks(())\n\n    # AÃ±adir una leyenda para identificar las clases de los\n    # puntos de entrenamiento y prueba\n    ax.legend()\n\n    # mostrar el grafico\n    plt.show()\n\ndef plot_decision_boundary(mlp, X, y, h=0.02):\n    # Crear una malla de puntos para el espacio de caracterÃ­sticas\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # por cada punto de la grilla, hacer una predicciÃ³n del MLP\n    Z = np.array([mlp.prediccion([np.array([xx.ravel()[i], yy.ravel()[i]])])\n                  for i in range(len(xx.ravel()))])\n\n    # redimensionar para que tenga el mismo shape de la grilla\n    Z = Z.reshape(xx.shape)\n\n    # crear una figura de dos subplots\n    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n\n    # Graficar los puntos originales\n    ax[0].set_title('Puntos originales')\n    ax[0].scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu,\n                  edgecolors='k', alpha=0.6)\n\n    # Graficar los puntos de entrenamiento\n    ax[1].set_title('Frontera de decisiÃ³n generada por el MLP')\n    ax[1].scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.RdBu)\n    # Graficar la frontera de decisiÃ³n con un contorno\n    ax[1].contourf(xx, yy, Z, alpha=0.6, cmap=plt.cm.RdBu)\n\n    # mejorar la visualizaciÃ³n\n    for i in range(2):\n        ax[i].set_xlim(xx.min(), xx.max())\n        ax[i].set_ylim(yy.min(), yy.max())\n        ax[i].set_xticks(())\n        ax[i].set_yticks(())\n        ax[i].set_xlabel(\"CaracterÃ­stica x_1\")\n        ax[i].set_ylabel(\"CaracterÃ­stica x_2\")\n\n    plt.show()\n\ndef plot_cost_history(costo_historia):\n    # Grafica el cambio del costo en el entrenamiento\n    plt.figure(figsize=(8, 4))\n    plt.plot(costo_historia, label='Costo')\n    plt.title('Historia del Costo')\n    plt.xlabel('Ã‰pocas')\n    plt.ylabel('Costo')\n    plt.legend()\n    plt.grid(True)\n    plt.show()"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#inicializaciÃ³n",
    "href": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#inicializaciÃ³n",
    "title": "Implementando un PerceptrÃ³n Multi-capa (MLP) desde CERO usando python",
    "section": "1. InicializaciÃ³n",
    "text": "1. InicializaciÃ³n\nAsignaciÃ³n de atributos e inicializaciÃ³n de pesos y biases\n\nclass PerceptronMulticapa():\n    def __init__(self, params=None):\n        # AsignaciÃ³n de hiperparÃ¡metros\n        self.capa_entrada = params['capa_entrada']\n        self.capa_oculta = params['capa_oculta']\n        self.capa_salida = params['capa_salida']\n        self.epochs = params['epochs']\n        self.lr = params['lr']\n        self.relu = (lambda x: x*(x &gt; 0))\n        self.derivada_relu = (lambda x: 1 * (x&gt;0))\n        self.sigmoide = (lambda x: 1/(1 + np.exp(-x)))\n        self.derivada_sigmoide = (lambda x: x*(1-x))\n\n        # inicializaciÃ³n de pesos y bias\n        self.inicializacion()\n\n    def inicializacion(self):\n        # inicializaciÃ³n de pesos y bias aleatoria\n        np.random.seed(42) # fijar una semilla para reproducir resultados\n\n        # Capa Oculta\n        self.pesos_capa_oculta = np.random.rand(self.capa_oculta, self.capa_entrada)\n        self.bias_capa_oculta = np.ones((self.capa_oculta, 1))\n\n        # Capa de salida\n        self.pesos_capa_salida = np.random.rand(self.capa_salida, self.capa_oculta)\n        self.bias_capa_salida = np.ones((self.capa_salida, 1))\n\n\n# Instanciamos nuestro perceptrÃ³n multicapa\nmlp =  PerceptronMulticapa(params)\n\n\nprint('DimensiÃ³n pesos capa oculta: {}'.format(mlp.pesos_capa_oculta.shape))\nprint('DimensiÃ³n biases capa oculta: {}'.format(mlp.bias_capa_oculta.shape))\nprint('DimensiÃ³n pesos capa salida: {}'.format(mlp.pesos_capa_salida.shape))\nprint('DimensiÃ³n bias capa salida: {}'.format(mlp.bias_capa_salida.shape))\n\nDimensiÃ³n pesos capa oculta: (50, 2)\nDimensiÃ³n biases capa oculta: (50, 1)\nDimensiÃ³n pesos capa salida: (1, 50)\nDimensiÃ³n bias capa salida: (1, 1)\n\n\n\n# ejemplo pesos capa salida\nmlp.pesos_capa_salida\n\narray([[0.03142919, 0.63641041, 0.31435598, 0.50857069, 0.90756647,\n        0.24929223, 0.41038292, 0.75555114, 0.22879817, 0.07697991,\n        0.28975145, 0.16122129, 0.92969765, 0.80812038, 0.63340376,\n        0.87146059, 0.80367208, 0.18657006, 0.892559  , 0.53934224,\n        0.80744016, 0.8960913 , 0.31800347, 0.11005192, 0.22793516,\n        0.42710779, 0.81801477, 0.86073058, 0.00695213, 0.5107473 ,\n        0.417411  , 0.22210781, 0.11986537, 0.33761517, 0.9429097 ,\n        0.32320293, 0.51879062, 0.70301896, 0.3636296 , 0.97178208,\n        0.96244729, 0.2517823 , 0.49724851, 0.30087831, 0.28484049,\n        0.03688695, 0.60956433, 0.50267902, 0.05147875, 0.27864646]])"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#propagaciÃ³n-hacia-adelante-forward",
    "href": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#propagaciÃ³n-hacia-adelante-forward",
    "title": "Implementando un PerceptrÃ³n Multi-capa (MLP) desde CERO usando python",
    "section": "2. PropagaciÃ³n hacia adelante (forward)",
    "text": "2. PropagaciÃ³n hacia adelante (forward)\nMÃ©todo de propagaciÃ³n hacia adelante\n\ndef forward_pass(self, x):\n    # Realizar la operacion Wx + b de la capa oculta, x = x_0\n    z = np.matmul(self.pesos_capa_oculta, x) + self.bias_capa_oculta\n    # Aplicar funciÃ³n de activaciÃ³n\n    h = self.relu(z) # z = x_1, h = x_2\n\n    # Aplicar la operaciÃ³n Wh + b para generar la salida, y = x_3\n    y = np.matmul(self.pesos_capa_salida, h) + self.bias_capa_salida\n    # Aplicar funciÃ³n de activaciÃ³n softmax para la clasificaciÃ³n\n    y_pred = self.sigmoide(y) # y = x_4\n\n    return z, h, y_pred\n\n\n# AÃ±adimos nuestro nuevo mÃ©todo\nsetattr(PerceptronMulticapa, 'forward_pass', forward_pass)\n\n\n# seleccionamos una muestra del dataset\n# por ser solo uno se redimensiona para que tenga la estructura de entrada propia\nx_i = X_train[0,:].reshape((-1, 1))\nz, h, y_pred = mlp.forward_pass(x_i)\n\n\nprint('DimensiÃ³n biases capa oculta: {}'.format(z.shape))\nprint('DimensiÃ³n de la capa oculta: {}'.format(h.shape))\nprint('PredicciÃ³n: {}'.format(y_pred))\nprint('Capa oculta: {}'.format(h))\n\nDimensiÃ³n biases capa oculta: (50, 1)\nDimensiÃ³n de la capa oculta: (50, 1)\nPredicciÃ³n: [[1.]]\nCapa oculta: [[0.72633077]\n [1.01761811]\n [0.99129875]\n [0.64226347]\n [0.91951467]\n [0.58240149]\n [1.22554416]\n [0.98915428]\n [0.88627666]\n [1.03760999]\n [1.17304912]\n [0.95112959]\n [0.83016191]\n [0.85085938]\n [1.20642355]\n [1.1577875 ]\n [0.60864823]\n [1.01505623]\n [1.07377182]\n [1.06886654]\n [0.82949748]\n [0.61426555]\n [0.80843687]\n [0.89119274]\n [1.12821114]\n [1.03116189]\n [0.96713639]\n [0.82449353]\n [0.94790496]\n [0.87459926]\n [1.02976633]\n [1.1607742 ]\n [0.86948377]\n [0.70204454]\n [0.59561443]\n [1.20847428]\n [0.64438828]\n [0.95081357]\n [1.26279179]\n [1.08640592]\n [1.05700326]\n [1.09879949]\n [0.97640557]\n [0.99963981]\n [1.13251031]\n [0.73290002]\n [1.0450389 ]\n [1.0785398 ]\n [1.0125701 ]\n [0.96240177]]"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#funciÃ³n-para-calcular-el-error",
    "href": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#funciÃ³n-para-calcular-el-error",
    "title": "Implementando un PerceptrÃ³n Multi-capa (MLP) desde CERO usando python",
    "section": "3. FunciÃ³n para calcular el error",
    "text": "3. FunciÃ³n para calcular el error\nMÃ©todo que permite conocer el error de una predicciÃ³n con respecto a la etiqueta real.\n\ndef calcular_perdida_entropia_cruzada(self, y_real, y_pred):\n    epsilon = 1e-12\n    # asegura que los valores de las predicciones esten en un rango\n    # seguro para evitar logaritmos de 0 y 1\n    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n    # calculo de la perdida\n    perdida = -(((1 - y_real) * np.log(1 - y_pred + epsilon)) + (y_real * np.log(y_pred + epsilon)))\n\n    return perdida\n\n\n# AÃ±adimos nuestro nuevo mÃ©todo\nsetattr(PerceptronMulticapa, 'calcular_perdida_entropia_cruzada', calcular_perdida_entropia_cruzada)\n\n\n# Probamos nuestra funciÃ³n de error\ny_real = y_train[0]\nerror = mlp.calcular_perdida_entropia_cruzada(y_real, y_pred)\nprint(f'Error: {error}')\n\nError: [[6.67165212e-11]]"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#propagaciÃ³n-hacÃ­a-atrÃ¡s-backward",
    "href": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#propagaciÃ³n-hacÃ­a-atrÃ¡s-backward",
    "title": "Implementando un PerceptrÃ³n Multi-capa (MLP) desde CERO usando python",
    "section": "4. PropagaciÃ³n hacÃ­a atrÃ¡s (backward)",
    "text": "4. PropagaciÃ³n hacÃ­a atrÃ¡s (backward)\nMÃ©todo para propagar los errores hacÃ­a atrÃ¡s.\n\ndef backward_pass(self, x, z, y_real, h, y_pred):\n    # PropagaciÃ³n de error en la capa de salida\n    # Calculo de error en la capa de salida g_out\n    #error_salida =  (y_pred - y_real) * self.derivada_sigmoide(y_pred)\n    error_salida =  y_pred - y_real\n\n    # gradiente de los pesos respecto a la capa de salida\n    # X_in * g_out = error_salida * h.T\n    # X_in = h es la entrada a la capa de salida\n    self.gradiente_pesos_capa_salida = np.matmul(error_salida, h.T)\n    # gradiente de los bias respecto a la capa de salida\n    self.gradiente_bias_capa_salida = error_salida\n\n    # PropagaciÃ³n de error en la capa oculta\n    # gradiente respecto a la capa oculta\n    # (g_out * W) * relu'(X_in)\n    # X_in en esta capa es la salida de aplicar la primera transformaciÃ³n\n    error_oculta = np.matmul(self.pesos_capa_salida.T, error_salida) * self.derivada_relu(z)\n    # gradientes con respecto a la capa oculta, de nuevo g_out * X_in\n    self.gradiente_pesos_capa_oculta = np.matmul(error_oculta, x.T)\n    self.gradiente_bias_capa_oculta = error_oculta\n\n\n# AÃ±adimos nuestro nuevo mÃ©todo\nsetattr(PerceptronMulticapa, 'backward_pass', backward_pass)\n\n\n# calcular propagaciÃ³n de errores\nmlp.backward_pass(x_i, z, y_real, h, y_pred)\n\n\nprint('DimensiÃ³n gradientes capa oculta: {}'.format(mlp.gradiente_pesos_capa_oculta.shape))\nprint('Gradientes capa oculta: {}'.format(mlp.gradiente_pesos_capa_oculta))\n\nDimensiÃ³n gradientes capa oculta: (50, 2)\nGradientes capa oculta: [[-8.14789923e-13  9.33629314e-13]\n [-1.64987027e-11  1.89050846e-11]\n [-8.14956162e-12  9.33819800e-12]\n [-1.31845056e-11  1.51075026e-11]\n [-2.35283225e-11  2.69599942e-11]\n [-6.46280811e-12  7.40542677e-12]\n [-1.06390243e-11  1.21907558e-11]\n [-1.95873816e-11  2.24442561e-11]\n [-5.93150715e-12  6.79663407e-12]\n [-1.99567547e-12  2.28675032e-12]\n [-7.51169840e-12  8.60730064e-12]\n [-4.17960177e-12  4.78920839e-12]\n [-2.41020651e-11  2.76174187e-11]\n [-2.09502196e-11  2.40058678e-11]\n [-1.64207563e-11  1.88157695e-11]\n [-2.25922909e-11  2.58874398e-11]\n [-2.08348990e-11  2.38737273e-11]\n [-4.83675923e-12  5.54221410e-12]\n [-2.31392593e-11  2.65141851e-11]\n [-1.39822466e-11  1.60215964e-11]\n [-2.09325850e-11  2.39856612e-11]\n [-2.32308329e-11  2.66191149e-11]\n [-8.24412154e-12  9.44654975e-12]\n [-2.85305512e-12  3.26918119e-12]\n [-5.90913412e-12  6.77099787e-12]\n [-1.10726102e-11  1.26875814e-11]\n [-2.12067279e-11  2.42997885e-11]\n [-2.23141195e-11  2.55686963e-11]\n [-1.80231392e-13  2.06518646e-13]\n [-1.32409334e-11  1.51721606e-11]\n [-1.08212247e-11  1.23995306e-11]\n [-5.75806219e-12  6.59789168e-12]\n [-3.10746496e-12  3.56069743e-12]\n [-8.75254746e-12  1.00291310e-11]\n [-2.44445826e-11  2.80098934e-11]\n [-8.37891553e-12  9.60100382e-12]\n [-1.34494535e-11  1.54110939e-11]\n [-1.82255044e-11  2.08837453e-11]\n [-9.42696189e-12  1.08019107e-11]\n [-2.51930882e-11  2.88675707e-11]\n [-2.49510873e-11  2.85902733e-11]\n [-6.52736216e-12  7.47939620e-12]\n [-1.28909822e-11  1.47711680e-11]\n [-7.80015802e-12  8.93783290e-12]\n [-7.38438363e-12  8.46141666e-12]\n [-9.56280359e-13  1.09575653e-12]\n [-1.58027281e-11  1.81076003e-11]\n [-1.30317663e-11  1.49324859e-11]\n [-1.33456744e-12  1.52921783e-12]\n [-7.22380571e-12  8.27741800e-12]]"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#entrenamiento",
    "href": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#entrenamiento",
    "title": "Implementando un PerceptrÃ³n Multi-capa (MLP) desde CERO usando python",
    "section": "5. Entrenamiento",
    "text": "5. Entrenamiento\nMÃ©todo para iterar sobre todo el conjunto de datos y entrenar la red. Antes se deberÃ¡ generar otro mÃ©todo que haga la respectiva actualizaciÃ³n de pesos, una vez ya los gradientes son calculados.\n\ndef actualizar_pesos(self):\n    # actualizar pesos aplicando gradiente descendiente\n    self.pesos_capa_salida -= self.lr * self.gradiente_pesos_capa_salida\n    self.bias_capa_salida -= self.lr * self.gradiente_bias_capa_salida\n    self.pesos_capa_oculta -= self.lr * self.gradiente_pesos_capa_oculta\n    self.bias_capa_oculta -= self.lr * self.gradiente_bias_capa_oculta\n\ndef entrenar(self, X, y):\n    # almacenar el costo de cada iteraciÃ³n\n    self.costo_historia = []\n\n    # iterar sobre el nÃºmero de Ã©pocas\n    for iteracion in tqdm(range(self.epochs), desc='Iteraciones'):\n        # iterar sobre los datos de entrenamiento\n        error_total = 0 # error para la iteraciÃ³n i\n        # iterar sobre todo el conjunto de datos\n        for i, (x_i, y_i) in tqdm(enumerate(zip(X,y)), desc='Datos', leave=False):\n            # asegurar de que la entrada y la salida solo tenga una columna\n            x_i = x_i.reshape(-1, 1)\n            y_i = y_i.reshape(-1, 1)\n            # aplicar propagaciÃ³n hacia adelante\n            z, h, y_pred = self.forward_pass(x_i)\n            # calcular la perdida de entropia cruzada\n            perdida = self.calcular_perdida_entropia_cruzada(y_i, y_pred)\n            error_total += perdida\n            # aplicar propagaciÃ³n hacia atras\n            self.backward_pass(x_i, z,  y_i, h, y_pred)\n            # actualizar pesos y bias usando gradiente descendiente\n            self.actualizar_pesos()\n\n        # almacenar los costos de cada iteraciÃ³n\n        self.costo = error_total / len(X)\n        self.costo_historia.append(self.costo)\n\n    #print(f'costo: {self.costo[0][0]} en la iteraciÃ³n: {iteracion}')\n\n\n# AÃ±adimos nuestros nuevos mÃ©todos\nsetattr(PerceptronMulticapa, 'actualizar_pesos', actualizar_pesos)\nsetattr(PerceptronMulticapa, 'entrenar', entrenar)\n\n\n# realizamos el respectivo entrenamiento\nmlp.entrenar(X_train, y_train)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint(f'Costo final: {mlp.costo}')\n\nCosto final: [[0.44965122]]\n\n\n\n# Visualizar el cambio del costo durante el entrenamiento\nplot_cost_history(np.array(mlp.costo_historia).ravel())"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#evaluaciÃ³n",
    "href": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#evaluaciÃ³n",
    "title": "Implementando un PerceptrÃ³n Multi-capa (MLP) desde CERO usando python",
    "section": "EvaluaciÃ³n",
    "text": "EvaluaciÃ³n\nPara evaluar nuestro modelo con la informaciÃ³n de test, creamos primero la funciÃ³n de predicciÃ³n y seguidamente evaluamos el rendimiento en test.\n\ndef prediccion(self, X):\n    predicciones = []\n    for x_i in X:\n        x_i = x_i.reshape(-1, 1)  # Asegurar que x_i sea una columna\n        z, _, y_pred = self.forward_pass(x_i)\n        predicciones.append(y_pred)\n    return np.array(predicciones).flatten()\n\ndef evaluar(self, X, y, umbral):\n    # generar predicciones\n    y_pred = self.prediccion(X)\n    # convertir a 0 y 1 bajo un umbral\n    y_pred = np.where(y_pred &gt;= umbral, 1, 0)\n    # generar reporte de clasificaciÃ³n y matriz de confusiÃ³n\n    print(classification_report(y, y_pred))\n    cm = confusion_matrix(y, y_pred)\n\n    # visualizacion de la matriz de confusion\n    ConfusionMatrixDisplay(cm).plot()\n    plt.show()\n\n    return y_pred, cm\n\n\n# AÃ±adimos nuestros nuevos mÃ©todos\nsetattr(PerceptronMulticapa, 'prediccion', prediccion)\nsetattr(PerceptronMulticapa, 'evaluar', evaluar)\n\n\n# evaluar y visualizar la matriz de confusion usando sklearn\ny_pred, cm = mlp.evaluar(X_test, y_test, umbral=0.5)\n\n              precision    recall  f1-score   support\n\n           0       0.81      0.81      0.81        26\n           1       0.74      0.74      0.74        19\n\n    accuracy                           0.78        45\n   macro avg       0.77      0.77      0.77        45\nweighted avg       0.78      0.78      0.78        45"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#interpretabilidad",
    "href": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#interpretabilidad",
    "title": "Implementando un PerceptrÃ³n Multi-capa (MLP) desde CERO usando python",
    "section": "7. Interpretabilidad",
    "text": "7. Interpretabilidad\nGrÃ¡ficamos algunos aspectos de interpretabilidad como la frontera de decisiÃ³n generada.\n\nplot_decision_boundary(mlp, X, y, h=0.02)"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#clase-python-perceptronmulticapa-completo",
    "href": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#clase-python-perceptronmulticapa-completo",
    "title": "Implementando un PerceptrÃ³n Multi-capa (MLP) desde CERO usando python",
    "section": "Clase python PerceptronMulticapa Completo",
    "text": "Clase python PerceptronMulticapa Completo\n\nclass PerceptronMulticapa():\n    def __init__(self, params=None):\n        # AsignaciÃ³n de hiperparÃ¡metros\n        self.capa_entrada = params['capa_entrada']\n        self.capa_oculta = params['capa_oculta']\n        self.capa_salida = params['capa_salida']\n        self.epochs = params['epochs']\n        self.lr = params['lr']\n        self.relu = (lambda x: x*(x &gt; 0))\n        self.derivada_relu = (lambda x: 1 * (x&gt;0))\n        self.sigmoide = (lambda x: 1/(1 + np.exp(-x)))\n        self.derivada_sigmoide = (lambda x: x*(1-x))\n\n        # inicializaciÃ³n de pesos y bias\n        self.inicializacion()\n\n    def inicializacion(self):\n        # inicializaciÃ³n de pesos y bias aleatoria\n        np.random.seed(42) # fijar una semilla para reproducir resultados\n\n        # Capa Oculta\n        self.pesos_capa_oculta = np.random.rand(self.capa_oculta, self.capa_entrada)\n        self.bias_capa_oculta = np.ones((self.capa_oculta, 1))\n\n        # Capa de salida\n        self.pesos_capa_salida = np.random.rand(self.capa_salida, self.capa_oculta)\n        self.bias_capa_salida = np.ones((self.capa_salida, 1))\n\n    def forward_pass(self, x):\n        # Realizar la operacion Wx + b de la capa oculta, x = x_0\n        z = np.matmul(self.pesos_capa_oculta, x) + self.bias_capa_oculta\n        # Aplicar funciÃ³n de activaciÃ³n\n        h = self.relu(z) # z = x_1, h = x_2\n\n        # Aplicar la operaciÃ³n Wh + b para generar la salida, y = x_3\n        y = np.matmul(self.pesos_capa_salida, h) + self.bias_capa_salida\n        # Aplicar funciÃ³n de activaciÃ³n softmax para la clasificaciÃ³n\n        y_pred = self.sigmoide(y) # y = x_4\n\n        return z, h, y_pred\n\n    def actualizar_pesos(self):\n        # actualizar pesos aplicando gradiente descendiente\n        self.pesos_capa_salida -= self.lr * self.gradiente_pesos_capa_salida\n        self.bias_capa_salida -= self.lr * self.gradiente_bias_capa_salida\n        self.pesos_capa_oculta -= self.lr * self.gradiente_pesos_capa_oculta\n        self.bias_capa_oculta -= self.lr * self.gradiente_bias_capa_oculta\n\n    def backward_pass(self, x, z, y_real, h, y_pred):\n        # PropagaciÃ³n de error en la capa de salida\n        # Calculo de error en la capa de salida g_out\n        #error_salida =  (y_pred - y_real) * self.derivada_sigmoide(y_pred)\n        error_salida =  y_pred - y_real\n\n        # gradiente de los pesos respecto a la capa de salida\n        # X_in * g_out = error_salida * h.T\n        # X_in = h es la entrada a la capa de salida\n        self.gradiente_pesos_capa_salida = np.matmul(error_salida, h.T)\n        # gradiente de los bias respecto a la capa de salida\n        self.gradiente_bias_capa_salida = error_salida\n\n        # PropagaciÃ³n de error en la capa oculta\n        # gradiente respecto a la capa oculta\n        # (g_out * W) * relu'(X_in)\n        # X_in en esta capa es la salida de aplicar la primera transformaciÃ³n\n        error_oculta = np.matmul(self.pesos_capa_salida.T, error_salida) * self.derivada_relu(z)\n        # gradientes con respecto a la capa oculta, de nuevo g_out * X_in\n        self.gradiente_pesos_capa_oculta = np.matmul(error_oculta, x.T)\n        self.gradiente_bias_capa_oculta = error_oculta\n\n    def entrenar(self, X, y):\n        # almacenar el costo de cada iteraciÃ³n\n        self.costo_historia = []\n\n        # iterar sobre el nÃºmero de Ã©pocas\n        for iteracion in tqdm(range(self.epochs), desc='Iteraciones'):\n            # iterar sobre los datos de entrenamiento\n            error_total = 0 # error para la iteraciÃ³n i\n            # iterar sobre todo el conjunto de datos\n            for i, (x_i, y_i) in tqdm(enumerate(zip(X,y)), desc='Datos', leave=False):\n                # asegurar de que la entrada y la salida solo tenga una columna\n                x_i = x_i.reshape(-1, 1)\n                y_i = y_i.reshape(-1, 1)\n                # aplicar propagaciÃ³n hacia adelante\n                z, h, y_pred = self.forward_pass(x_i)\n                # calcular la perdida de entropia cruzada\n                perdida = self.calcular_perdida_entropia_cruzada(y_i, y_pred)\n                error_total += perdida\n                # aplicar propagaciÃ³n hacia atras\n                self.backward_pass(x_i, z,  y_i, h, y_pred)\n                # actualizar pesos y bias usando gradiente descendiente\n                self.actualizar_pesos()\n\n            # almacenar los costos de cada iteraciÃ³n\n            self.costo = error_total / len(X)\n            self.costo_historia.append(self.costo)\n\n        #print(f'costo: {self.costo[0][0]} en la iteraciÃ³n: {iteracion}')\n\n    def calcular_perdida_entropia_cruzada(self, y_real, y_pred):\n        epsilon = 1e-12\n        # asegura que los valores de las predicciones esten en un rango\n        # seguro para evitar logaritmos de 0 y 1\n        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n        # calculo de la perdida\n        perdida = -(((1 - y_real) * np.log(1 - y_pred + epsilon)) + (y_real * np.log(y_pred + epsilon)))\n\n        return perdida\n\n    def prediccion(self, X):\n        predicciones = []\n        for x_i in X:\n            x_i = x_i.reshape(-1, 1)  # Asegurar que x_i sea una columna\n            z, _, y_pred = self.forward_pass(x_i)\n            predicciones.append(y_pred)\n        return np.array(predicciones).flatten()\n\n    def evaluar(self, X, y, umbral):\n        # generar predicciones\n        y_pred = self.prediccion(X)\n        # convertir a 0 y 1 bajo un umbral\n        y_pred = np.where(y_pred &gt;= umbral, 1, 0)\n        # generar reporte de clasificaciÃ³n y matriz de confusiÃ³n\n        print(classification_report(y, y_pred))\n        cm = confusion_matrix(y, y_pred)\n\n        # visualizacion de la matriz de confusion\n        ConfusionMatrixDisplay(cm).plot()\n        plt.show()\n\n        return y_pred, cm"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Curso PrÃ¡ctico: Neural Networks y Deep Learning",
    "section": "",
    "text": "Â¡Bienvenido al sitio web del curso prÃ¡ctico intensivo de Neural Networks y Deep Learning!\nEste curso estÃ¡ diseÃ±ado para proporcionarte una comprensiÃ³n sÃ³lida y aplicada de los conceptos fundamentales de las redes neuronales y su aplicaciÃ³n en el campo de la inteligencia artificial, con un enfoque marcado en la implementaciÃ³n prÃ¡ctica y la resoluciÃ³n de problemas reales.\nA lo largo de tres semanas intensivas, explorarÃ¡s desde los principios bÃ¡sicos hasta arquitecturas avanzadas y tÃ©cnicas de vanguardia, preparÃ¡ndote para enfrentar desafÃ­os complejos en el mundo del Deep Learning."
  },
  {
    "objectID": "index.html#quÃ©-exploraremos",
    "href": "index.html#quÃ©-exploraremos",
    "title": "Curso PrÃ¡ctico: Neural Networks y Deep Learning",
    "section": "Â¿QuÃ© Exploraremos?",
    "text": "Â¿QuÃ© Exploraremos?\nEl curso se estructura en tres unidades clave:\n\nFundamentos de Redes Neuronales: Introduce los conceptos esenciales, la estructura y el entrenamiento de modelos, abordando el perceptrÃ³n, la retropropagaciÃ³n y mÃ©todos de optimizaciÃ³n como el gradiente descendente. SentarÃ¡s las bases para entender cÃ³mo aprenden las redes mientras aplicas de manera prÃ¡ctica los conceptos.\nArquitecturas de Redes Neuronales: ExplorarÃ¡s diversas arquitecturas, incluyendo Redes Neuronales Densas (DNN), Convolucionales (CNN) y Recurrentes (RNN). AmpliarÃ¡s tu visiÃ³n con una introducciÃ³n a arquitecturas avanzadas como transformers y desarrollarÃ¡s la habilidad para seleccionar la mÃ¡s adecuada segÃºn el problema.\nTÃ©cnicas Avanzadas y Robustez: Te centrarÃ¡s en la construcciÃ³n de modelos avanzados, cubriendo la generalizaciÃ³n, la transferencia de aprendizaje, optimizaciÃ³n de hiperparÃ¡metros y la explicabilidad. Se introducirÃ¡ el entrenamiento adversarial y tÃ©cnicas para crear modelos robustos y transferibles que funcionen bien en entornos de datos diversos."
  },
  {
    "objectID": "index.html#enfoque-prÃ¡ctico",
    "href": "index.html#enfoque-prÃ¡ctico",
    "title": "Curso PrÃ¡ctico: Neural Networks y Deep Learning",
    "section": "Enfoque PrÃ¡ctico",
    "text": "Enfoque PrÃ¡ctico\nEste curso es eminentemente prÃ¡ctico. A travÃ©s de Implementaciones en Python y trabajo directo con notebooks, aplicarÃ¡s los conceptos teÃ³ricos de inmediato. El objetivo es que no solo comprendas cÃ³mo funcionan las redes neuronales, sino que tambiÃ©n ganes experiencia prÃ¡ctica en cÃ³mo construirlas y utilizarlas para resolver problemas reales.\nCada semana incluye componentes prÃ¡cticos diseÃ±ados para consolidar tu aprendizaje y permitirte experimentar con diferentes arquitecturas y tÃ©cnicas."
  },
  {
    "objectID": "index.html#estructura-del-sitio",
    "href": "index.html#estructura-del-sitio",
    "title": "Curso PrÃ¡ctico: Neural Networks y Deep Learning",
    "section": "Estructura del Sitio",
    "text": "Estructura del Sitio\n\nSemana 1: Fundamentos y MLP\nSemana 2: Arquitecturas neuronales profundas\nSemana 3: TÃ©cnicas Avanzadas y Robustez\n\nExplora cada secciÃ³n para encontrar los materiales de la semana, los notebooks (renderizados para visualizaciÃ³n web) y las slides correspondientes."
  },
  {
    "objectID": "semana_1/index.html",
    "href": "semana_1/index.html",
    "title": "Semana 1: Fundamentos de Redes Neuronales",
    "section": "",
    "text": "Esta semana sentamos las bases del Deep Learning. Cubriremos los conceptos esenciales, la estructura y el entrenamiento de las redes neuronales.\nTemas Clave:\n\nEstructura y entrenamiento de modelos en Deep Learning\nEl PerceptrÃ³n\nProceso de RetropropagaciÃ³n\nMÃ©todos de optimizaciÃ³n (Gradiente Descendente)\n\nEnfoque PrÃ¡ctico: ComprenderÃ¡s el funcionamiento interno de una red y aplicarÃ¡s tÃ©cnicas de entrenamiento en modelos simples. RealizarÃ¡s implementaciones prÃ¡cticas para observar cÃ³mo aprenden los modelos.\nMateriales de la Semana:\n\nSlides de la Semana 1 (PrÃ³ximamente)\nNotebook: Implementando PerceptrÃ³n Multicapa desde CERO\nNotebook: MLP usando Frameworks"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1b_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks.html",
    "href": "semana_1/notebooks/Nb_1b_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks.html",
    "title": "Implementado un PerceptrÃ³n multi-capa usando frameworks",
    "section": "",
    "text": "Open In Colab\n#@title Importar librerÃ­as\n#importar librerÃ­as necesarias\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn import set_config\nset_config(display='diagram')\nimport seaborn as sns\nimport random\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Input\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.utils import plot_model\n#@title Funciones complementarias\ndef plot_samples_dataset(X, y):\n    # Convertir las etiquetas a enteros\n    y = y.astype(int)\n\n    # Crear la grilla de 4x4\n    fig, axes = plt.subplots(4, 4, figsize=(6, 6))\n    fig.suptitle('Grilla de imÃ¡genes del dataset MNIST')\n\n    # Iterar para mostrar las primeras 16 imÃ¡genes con sus etiquetas\n    for i, ax in enumerate(axes.flat):\n        img = X.iloc[i].values.reshape(28, 28)\n        label = y[i]\n        ax.imshow(img, cmap='gray')\n        ax.set_title(f'Label: {label}')\n        ax.axis('off')\n\n    plt.show()\n\ndef plot_curva_aprendizaje(mlp):\n    plt.figure(figsize=(8, 5))\n    plt.plot(mlp.loss_curve_, marker='o')\n    plt.title('PÃ©rdida durante el entrenamiento del MLP por iteraciÃ³n')\n    plt.xlabel('IteraciÃ³n')\n    plt.ylabel('PÃ©rdida (Loss)')\n    plt.grid()\n    plt.show()\n\ndef plot_matriz_confusion(cm):\n    # Visualizar la matriz de confusiÃ³n usando Seaborn\n    plt.figure(figsize=(10, 7))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n    plt.xlabel('Etiqueta predicha')\n    plt.ylabel('Etiqueta real')\n    plt.title('Matriz de ConfusiÃ³n para el MLP en el dataset MNIST')\n    plt.show()\n\ndef encontrar_dim_imagen(n_neurons):\n    \"\"\"\n    Encuentra la mejor forma cuadrada (filas, columnas) para una cantidad dada de neuronas.\n    \"\"\"\n    side_length = int(np.sqrt(n_neurons))  # Calcular la raÃ­z cuadrada del nÃºmero de neuronas\n    if side_length * side_length == n_neurons:\n        return (side_length, side_length)  # Si es un cuadrado perfecto\n    else:\n        # Si no es un cuadrado perfecto, buscamos la mejor aproximaciÃ³n (filas, columnas)\n        for i in range(side_length, 0, -1):\n            if n_neurons % i == 0:\n                return (i, n_neurons // i)  # Devolver filas y columnas\n    return (n_neurons, 1)  # Si no encuentra, retornar en forma de vector (n_neurons, 1)\n\ndef visualizacion_pesos_mlp(mlp):\n    # Definir la figura con 3 filas y 5 columnas\n    fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n\n    # Asignar las dimensiones para visualizar cada capa\n    layer_shapes = [encontrar_dim_imagen(layer.shape[0]) for layer in mlp.coefs_]\n\n    # Recorrer cada capa de coeficientes del MLP\n    for layer_index, (layer_coefs, ax_row) in enumerate(zip(mlp.coefs_, axes)):\n        # Seleccionar aleatoriamente 5 neuronas de la capa actual\n        num_neurons = layer_coefs.shape[1]\n        random_neurons = random.sample(range(num_neurons), 5)\n\n        # Obtener la forma de visualizaciÃ³n para esta capa\n        layer_shape = layer_shapes[layer_index]\n\n        vmin, vmax = layer_coefs.min(), layer_coefs.max()\n\n        # Visualizar las neuronas seleccionadas\n        for neuron_index, ax in zip(random_neurons, ax_row):\n            # Seleccionar los pesos de la neurona especÃ­fica y reestructurarlos en una matriz 2D\n            neuron_weights = layer_coefs[:, neuron_index].reshape(layer_shape)\n            # Dibujar la imagen de los pesos de la neurona\n            ax.matshow(neuron_weights, cmap=plt.cm.gray, vmin=0.5 * vmin, vmax=0.5 * vmax)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.set_title(f'Capa {layer_index+1}, Neurona {neuron_index}')\n\n    plt.suptitle('VisualizaciÃ³n de Pesos de las Neuronas en las Capas Ocultas')\n    plt.tight_layout()\n    plt.show()\n\ndef plot_loss_historia_keras(history):\n    # Graficar el histÃ³rico de pÃ©rdida durante el entrenamiento\n    plt.plot(history.history['loss'], label='PÃ©rdida de Entrenamiento')\n    plt.plot(history.history['val_loss'], label='PÃ©rdida de ValidaciÃ³n')\n    plt.title('PÃ©rdida durante el Entrenamiento')\n    plt.xlabel('Ã‰poca')\n    plt.ylabel('PÃ©rdida')\n    plt.legend()\n    plt.show()\n\ndef plot_acc_historia_keras(history):\n    # Graficar la precisiÃ³n durante el entrenamiento\n    plt.plot(history.history['accuracy'], label='PrecisiÃ³n de Entrenamiento')\n    plt.plot(history.history['val_accuracy'], label='PrecisiÃ³n de ValidaciÃ³n')\n    plt.title('PrecisiÃ³n durante el Entrenamiento')\n    plt.xlabel('Ã‰poca')\n    plt.ylabel('PrecisiÃ³n')\n    plt.legend()\n    plt.show()\n\ndef visualizacion_pesos_mlp_keras(model):\n    # Obtener los pesos del modelo (par de listas [pesos, biases] para cada capa)\n    weights = model.get_weights()\n\n    # Extraer solo los pesos de cada capa oculta, ignorando los bias\n    layer_weights = [weights[i] for i in range(0, len(weights), 2)]  # Solo los pesos, no los sesgos\n\n    # Definir la figura con 3 filas (una por cada capa) y 5 columnas (5 neuronas al azar)\n    fig, axes = plt.subplots(len(layer_weights), 5, figsize=(15, 9))\n\n    # Calcular las formas de cada capa de manera dinÃ¡mica\n    layer_shapes = [encontrar_dim_imagen(layer.shape[0]) for layer in layer_weights]\n\n    # Recorrer cada capa y sus pesos\n    for layer_index, (layer_coefs, ax_row) in enumerate(zip(layer_weights, axes)):\n        # Seleccionar aleatoriamente 5 neuronas de la capa actual\n        num_neurons = layer_coefs.shape[1]\n        random_neurons = random.sample(range(num_neurons), 5)\n\n        # Obtener la forma de visualizaciÃ³n para esta capa\n        layer_shape = layer_shapes[layer_index]\n        vmin, vmax = layer_coefs.min(), layer_coefs.max()\n\n\n        # Visualizar las neuronas seleccionadas\n        for neuron_index, ax in zip(random_neurons, ax_row):\n            # Seleccionar los pesos de la neurona especÃ­fica y reestructurarlos en una matriz 2D\n            neuron_weights = layer_coefs[:, neuron_index].reshape(layer_shape)\n            # Dibujar la imagen de los pesos de la neurona\n            ax.matshow(neuron_weights, cmap=plt.cm.gray, vmin=0.5 * vmin, vmax=0.5 * vmax)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.set_title(f'Capa {layer_index+1}, Neurona {neuron_index}')\n\n    plt.suptitle('VisualizaciÃ³n de Pesos de las Neuronas en las Capas Ocultas de Keras')\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1b_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks.html#evaluaciÃ³n-completa",
    "href": "semana_1/notebooks/Nb_1b_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks.html#evaluaciÃ³n-completa",
    "title": "Implementado un PerceptrÃ³n multi-capa usando frameworks",
    "section": "EvaluaciÃ³n completa",
    "text": "EvaluaciÃ³n completa\nRealizaremos una evaluaciÃ³n completa revisando el rendimiento en ambos conjuntos, seguidamente generaremos el reporte de clasificaciÃ³n y la matriz de confusiÃ³n.\n\nprint(f\"Training set score: {mlp.score(X_train, y_train):.3f}\")\nprint(f\"Test set score: {mlp.score(X_test, y_test):.3f}\")\n\nTraining set score: 1.000\nTest set score: 0.982\n\n\n\n# Realizar predicciones\ny_pred = mlp.predict(X_test)\n\n# Imprimir el reporte de mÃ©tricas\nprint(\"Reporte de ClasificaciÃ³n del MLP en MNIST:\\n\")\nprint(classification_report(y_test, y_pred))\n\n# Generar la matriz de confusiÃ³n\ncm = confusion_matrix(y_test, y_pred)\n\n# visualizar la matriz de confusiÃ³n\nplot_matriz_confusion(cm)\n\nReporte de ClasificaciÃ³n del MLP en MNIST:\n\n              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99      2058\n           1       0.99      0.99      0.99      2364\n           2       0.98      0.98      0.98      2133\n           3       0.98      0.98      0.98      2176\n           4       0.98      0.98      0.98      1936\n           5       0.98      0.98      0.98      1915\n           6       0.98      0.99      0.99      2088\n           7       0.98      0.98      0.98      2248\n           8       0.98      0.97      0.97      1992\n           9       0.98      0.97      0.98      2090\n\n    accuracy                           0.98     21000\n   macro avg       0.98      0.98      0.98     21000\nweighted avg       0.98      0.98      0.98     21000"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1b_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks.html#visualizaciÃ³n-de-pesos",
    "href": "semana_1/notebooks/Nb_1b_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks.html#visualizaciÃ³n-de-pesos",
    "title": "Implementado un PerceptrÃ³n multi-capa usando frameworks",
    "section": "VisualizaciÃ³n de pesos",
    "text": "VisualizaciÃ³n de pesos\n\n# pesos de la primera capa oculta. Todas las neuronas conectadas con cada pixel\nprint('DimensiÃ³n de la primera capa oculta: {}'.format(mlp.coefs_[0].shape))\nprint('DimensiÃ³n de la segunda capa oculta: {}'.format(mlp.coefs_[1].shape))\nprint('DimensiÃ³n de la tercera capa oculta: {}'.format(mlp.coefs_[2].shape))\n\nDimensiÃ³n de la primera capa oculta: (784, 225)\nDimensiÃ³n de la segunda capa oculta: (225, 100)\nDimensiÃ³n de la tercera capa oculta: (100, 10)\n\n\n\nvisualizacion_pesos_mlp(mlp)\n\n\n\n\n\n\n\n\n\nTutoriales relacionados\n\nAnÃ¡lisis de la variaciÃ³n del parametro de regularizaciÃ³n alpha\nComparaciÃ³n de las diferentes estrategias de aprendizaje\nAccelaraciÃ³n de Sklearn (GPU) usando la extensiÃ³n sklearnex\nAccelaraciÃ³n de Sklearn (GPU) usando CuML"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1b_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks.html#evaluaciÃ³n-completa-1",
    "href": "semana_1/notebooks/Nb_1b_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks.html#evaluaciÃ³n-completa-1",
    "title": "Implementado un PerceptrÃ³n multi-capa usando frameworks",
    "section": "EvaluaciÃ³n completa",
    "text": "EvaluaciÃ³n completa\n\ny_test_categorical = to_categorical(y_test)\n\nscore = mlp_keras.evaluate(X_test.values.astype(float), y_test_categorical, batch_size=128)\n\nscore\n\n\n165/165 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - accuracy: 0.9484 - loss: 0.1750\n\n\n\n\n[0.175654336810112, 0.9481428861618042]\n\n\n\n# Realizar predicciones en el conjunto de prueba\ny_pred = mlp_keras.predict(X_test.values.astype(float))\n\n# Convertir las predicciones en etiquetas (la clase con mayor probabilidad)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true = y_test.values.astype(int)  # Las etiquetas reales del conjunto de prueba\n\n\n657/657 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 2ms/step\n\n\n\n\n\n# Generar el reporte de clasificaciÃ³n\nprint(\"Reporte de ClasificaciÃ³n para el MLP en MNIST:\\n\")\nprint(classification_report(y_true, y_pred_classes))\n\n# Crear la matriz de confusiÃ³n\ncm = confusion_matrix(y_true, y_pred_classes)\n\n# Visualizar la matriz de confusiÃ³n usando Seaborn\nplot_matriz_confusion(cm)\n\nReporte de ClasificaciÃ³n para el MLP en MNIST:\n\n              precision    recall  f1-score   support\n\n           0       0.97      0.98      0.97      2058\n           1       0.96      0.98      0.97      2364\n           2       0.95      0.94      0.94      2133\n           3       0.93      0.94      0.93      2176\n           4       0.94      0.95      0.94      1936\n           5       0.95      0.93      0.94      1915\n           6       0.96      0.97      0.97      2088\n           7       0.95      0.96      0.95      2248\n           8       0.95      0.92      0.93      1992\n           9       0.93      0.92      0.93      2090\n\n    accuracy                           0.95     21000\n   macro avg       0.95      0.95      0.95     21000\nweighted avg       0.95      0.95      0.95     21000"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1b_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks.html#visualizaciÃ³n-de-pesos-1",
    "href": "semana_1/notebooks/Nb_1b_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks.html#visualizaciÃ³n-de-pesos-1",
    "title": "Implementado un PerceptrÃ³n multi-capa usando frameworks",
    "section": "VisualizaciÃ³n de pesos",
    "text": "VisualizaciÃ³n de pesos\n\nvisualizacion_pesos_mlp_keras(mlp_keras)\n\n\n\n\n\n\n\n\n\nâœ… CÃ³mo transformar tus datos para usar un MLP estÃ¡ndar (Keras o scikit-learn)\n\n\n\n\n\n\n\n\nTipo de dato o problema\nÂ¿QuÃ© debes hacer para usar un MLP?\nEjemplo sencillo\n\n\n\n\nğŸ“ Texto\nConvertir el texto a vectores. Usa tÃ©cnicas como Bag of Words (BoW), TF-IDF.\nClasificaciÃ³n de sentimientos: convertir cada comentario en un vector TF-IDF\n\n\nâ± Series temporales\nDividir en ventanas de tiempo fijas y calcular caracterÃ­sticas estadÃ­sticas (media, std, min, max, energÃ­a, etc.) por ventana.\nPredicciÃ³n de fallas: usar estadÃ­sticas de 10s de datos de sensores como entrada al MLP\n\n\nâ— AnomalÃ­as\nEtiquetar datos anÃ³malos (si puedes).\nDetecciÃ³n de fraude: marcar transacciones normales y anÃ³malas y entrenar un clasificador\n\n\nğŸ–¼ ImÃ¡genes\nExtraer caracterÃ­sticas manuales (como color, textura, tamaÃ±o, etc.) o redimensionar las imÃ¡genes y vectorizarlas.\nClasificaciÃ³n de imÃ¡genes de zapatos: usar un modelo CNN preentrenado para extraer features\n\n\nğŸ”Š Audio\nExtraer features de audio como MFCCs, espectrogramas, energÃ­a, pitch, etc., y construir vectores con esas estadÃ­sticas.\nDetecciÃ³n de emociones en voz: usar MFCCs y energÃ­a para representar cada audio\n\n\n\n\n\nğŸ§­ GuÃ­a paso a paso para construir tu baseline con un MLP\nSigue estos pasos para convertir tu idea o proyecto en un experimento funcional con un PerceptrÃ³n Multicapa (MLP), usando scikit-learn o Keras.\n\n\n1ï¸âƒ£ Define el objetivo de predicciÃ³n\n\nÂ¿Tu problema es de clasificaciÃ³n o regresiÃ³n?\nÂ¿CuÃ¡l es la variable que quieres predecir?\nEjemplos:\n\nClasificaciÃ³n: Â¿Este artÃ­culo es de biologÃ­a o matemÃ¡ticas?\nRegresiÃ³n: Â¿CuÃ¡l serÃ¡ el consumo energÃ©tico el prÃ³ximo mes?\n\n\n\n\n\n2ï¸âƒ£ Identifica tu tipo de datos\n\nÂ¿QuÃ© tipo de datos tienes?\n\nTexto\nSeries temporales\nDatos tabulares\nImÃ¡genes\nAudio\n\n\nğŸ” Revisa la tabla anterior para ver cÃ³mo transformar tus datos para usarlos con un MLP.\n\n\n\n3ï¸âƒ£ Preprocesa y vectoriza tus datos\n\nNormaliza tus valores si son numÃ©ricos (por ejemplo, entre 0 y 1).\nSi tienes texto, usa TF-IDF o BoW.\nSi tienes imÃ¡genes, vectorÃ­zalas (flatten).\nSi tienes secuencias, divide en ventanas y calcula estadÃ­sticas (media, std, etc.).\n\nğŸ“Œ AsegÃºrate de que cada fila sea un ejemplo y cada columna una caracterÃ­stica.\n\n\n\n4ï¸âƒ£ Define el tipo de salida y la funciÃ³n de pÃ©rdida\n\nClasificaciÃ³n binaria â†’ sigmoid + binary_crossentropy\nClasificaciÃ³n multiclase â†’ softmax + categorical_crossentropy\nRegresiÃ³n â†’ linear + mean_squared_error o mean_absolute_error\n\nâš ï¸ Si usas Keras, recuerda convertir las etiquetas con to_categorical() si usas softmax.\n\n\n\n5ï¸âƒ£ Construye tu MLP\n\nDecide cuÃ¡ntas capas ocultas y neuronas usar (ej. 2 capas de 128 y 64).\nUsa relu como activaciÃ³n oculta y softmax o sigmoid segÃºn el caso.\nAÃ±ade regularizaciÃ³n (Dropout, L2) si hay riesgo de sobreajuste."
  },
  {
    "objectID": "semana_1/slides/slides_semana_1.html#perceptrÃ³n-como-clasificador-bÃ¡sico",
    "href": "semana_1/slides/slides_semana_1.html#perceptrÃ³n-como-clasificador-bÃ¡sico",
    "title": "Semana 1: Fundamentos de Redes Neuronales",
    "section": "PerceptrÃ³n como Clasificador bÃ¡sico",
    "text": "PerceptrÃ³n como Clasificador bÃ¡sico"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2a_Implementando_una_CNN_usando_Keras.html",
    "href": "semana_2/notebooks/Nb_2a_Implementando_una_CNN_usando_Keras.html",
    "title": "Implementado una Red Neuronal Convolucional",
    "section": "",
    "text": "Ãšltima actualizaciÃ³n 09/05/2025\n#@title Importar librerÃ­as\n#importar librerÃ­as necesarias\nimport random\nfrom random import randint\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport random\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Input\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.datasets import mnist\n#@title Funciones complementarias\ndef plot_samples_dataset(X, y):\n    # Convertir las etiquetas a enteros\n    y = y.astype(int)\n\n    # Crear la grilla de 4x4\n    fig, axes = plt.subplots(4, 4, figsize=(6, 6))\n    fig.suptitle('Grilla de imÃ¡genes del dataset MNIST')\n\n    # Iterar para mostrar las primeras 16 imÃ¡genes con sus etiquetas\n    for i, ax in enumerate(axes.flat):\n        img = X.iloc[i].values.reshape(28, 28)\n        label = y[i]\n        ax.imshow(img, cmap='gray')\n        ax.set_title(f'Label: {label}')\n        ax.axis('off')\n\n    plt.show()\n\ndef plot_curva_aprendizaje(mlp):\n    plt.figure(figsize=(8, 5))\n    plt.plot(mlp.loss_curve_, marker='o')\n    plt.title('PÃ©rdida durante el entrenamiento del MLP por iteraciÃ³n')\n    plt.xlabel('IteraciÃ³n')\n    plt.ylabel('PÃ©rdida (Loss)')\n    plt.grid()\n    plt.show()\n\ndef plot_matriz_confusion(cm):\n    # Visualizar la matriz de confusiÃ³n usando Seaborn\n    plt.figure(figsize=(10, 7))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n    plt.xlabel('Etiqueta predicha')\n    plt.ylabel('Etiqueta real')\n    plt.title('Matriz de ConfusiÃ³n para el MLP en el dataset MNIST')\n    plt.show()\n\ndef encontrar_dim_imagen(n_neurons):\n    \"\"\"\n    Encuentra la mejor forma cuadrada (filas, columnas) para una cantidad dada de neuronas.\n    \"\"\"\n    side_length = int(np.sqrt(n_neurons))  # Calcular la raÃ­z cuadrada del nÃºmero de neuronas\n    if side_length * side_length == n_neurons:\n        return (side_length, side_length)  # Si es un cuadrado perfecto\n    else:\n        # Si no es un cuadrado perfecto, buscamos la mejor aproximaciÃ³n (filas, columnas)\n        for i in range(side_length, 0, -1):\n            if n_neurons % i == 0:\n                return (i, n_neurons // i)  # Devolver filas y columnas\n    return (n_neurons, 1)  # Si no encuentra, retornar en forma de vector (n_neurons, 1)\n\ndef plot_loss_historia_keras(history):\n    # Graficar el histÃ³rico de pÃ©rdida durante el entrenamiento\n    plt.plot(history.history['loss'], label='PÃ©rdida de Entrenamiento')\n    plt.plot(history.history['val_loss'], label='PÃ©rdida de ValidaciÃ³n')\n    plt.title('PÃ©rdida durante el Entrenamiento')\n    plt.xlabel('Ã‰poca')\n    plt.ylabel('PÃ©rdida')\n    plt.legend()\n    plt.show()\n\ndef plot_acc_historia_keras(history):\n    # Graficar la precisiÃ³n durante el entrenamiento\n    plt.plot(history.history['accuracy'], label='PrecisiÃ³n de Entrenamiento')\n    plt.plot(history.history['val_accuracy'], label='PrecisiÃ³n de ValidaciÃ³n')\n    plt.title('PrecisiÃ³n durante el Entrenamiento')\n    plt.xlabel('Ã‰poca')\n    plt.ylabel('PrecisiÃ³n')\n    plt.legend()\n    plt.show()\n\ndef visualizacion_filtros_cnn_keras(model):\n    # Obtener las capas convolucionales del modelo\n    conv_layers = [layer for layer in model.layers if 'conv' in layer.name]\n\n    # Definir la figura con una fila por capa convolucional y varias columnas (5 filtros al azar por capa)\n    fig, axes = plt.subplots(len(conv_layers), 5, figsize=(10, len(conv_layers) * 3))\n\n    # Recorrer cada capa convolucional y sus pesos\n    for layer_index, (layer, ax_row) in enumerate(zip(conv_layers, axes)):\n        # Obtener los pesos de la capa (solo el primer tensor, ignorar bias)\n        layer_weights = layer.get_weights()[0]  # shape: (filter_height, filter_width, input_channels, num_filters)\n\n        # Seleccionar 5 filtros de la capa actual\n        num_filters = layer_weights.shape[-1]\n        random_filters = random.sample(range(num_filters), 5)\n\n        # Obtener los lÃ­mites para la normalizaciÃ³n de las imÃ¡genes\n        vmin, vmax = layer_weights.min(), layer_weights.max()\n\n        # Dibujar cada filtro seleccionado\n        for filter_index, ax in zip(random_filters, ax_row):\n            # Extraer el filtro correspondiente (shape: filter_height, filter_width, input_channels)\n            filter_weights = layer_weights[..., filter_index]\n\n            # Promediar los canales para visualizar como imagen en escala de grises\n            if filter_weights.shape[-1] &gt; 1:\n                filter_weights = np.mean(filter_weights, axis=-1)  # Promedio sobre los canales de entrada\n\n            # Dibujar la imagen del filtro\n            ax.matshow(filter_weights, cmap=plt.cm.gray, vmin=0.5 * vmin, vmax=0.5 * vmax)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.set_title(f'Capa {layer_index + 1}, Filtro {filter_index}')\n\n    plt.suptitle('VisualizaciÃ³n de Filtros de las Capas Convolucionales de Keras')\n    plt.tight_layout()\n    plt.show()\n\ndef visualizacion_feature_maps(model, image):\n    # Crear un nuevo modelo que toma la misma entrada pero cuya salida son los mapas de caracterÃ­sticas de cada capa convolucional\n    layer_outputs = [layer.output for layer in model.layers if 'conv' in layer.name]\n    feature_map_model = tf.keras.Model(inputs=model.input, outputs=layer_outputs)\n\n    # Obtener los mapas de caracterÃ­sticas al pasar la imagen a travÃ©s del modelo\n    feature_maps = feature_map_model.predict(np.expand_dims(image, axis=0))  # AÃ±adir batch dimension\n\n    # Recorrer cada capa convolucional y sus feature maps correspondientes\n    for layer_index, feature_map in enumerate(feature_maps):\n        # NÃºmero de filtros en la capa actual\n        num_filters = feature_map.shape[-1]\n\n        # Definir la figura con una fila por cada filtro (limitado a 6 para evitar grÃ¡ficos muy grandes)\n        fig, axes = plt.subplots(1, min(6, num_filters), figsize=(20, 5))\n\n        # Mostrar cada filtro como imagen en escala de grises\n        for i in range(min(6, num_filters)):  # Mostrar un mÃ¡ximo de 6 filtros\n            ax = axes[i]\n            # Extraer el feature map del filtro `i` y mostrarlo\n            ax.matshow(feature_map[0, :, :, i], cmap='viridis')\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.set_title(f'Filtro {i+1}')\n\n        # TÃ­tulo de la capa y ajuste de la visualizaciÃ³n\n        plt.suptitle(f'VisualizaciÃ³n de Feature Maps - Capa {layer_index+1}')\n        plt.tight_layout()\n        plt.show()"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2a_Implementando_una_CNN_usando_Keras.html#componentes-base-complementarios",
    "href": "semana_2/notebooks/Nb_2a_Implementando_una_CNN_usando_Keras.html#componentes-base-complementarios",
    "title": "Implementado una Red Neuronal Convolucional",
    "section": "Componentes base complementarios",
    "text": "Componentes base complementarios\n\nCapas de Pooling\nCapas de normalizaciÃ³n\n\n\n1. Capas de Pooling\n\n\n\n1_vKYHxr5oI9cBw_hGhjQCrA.webp\n\n\n\n\n2. Capas de normalizaciÃ³n\nEn nuestro ejemplo, no es necesario normalizar ya que nuestras entradas estan entre 0 y 1 y no tienen un alta complejidad. Pero en otros escenarios, con imÃ¡genes mÃ¡s complejas y en formato RGB la normalizaciÃ³n se hace mÃ¡s comÃºn.\n\n\n\n1_dsl93qeGPteT3Zt7mBy1dQ-1.webp\n\n\n\n# Asumiendo que X_train y y_train ya estÃ¡n definidos como en el ejemplo anterior\n# Preprocesar las etiquetas para que sean categÃ³ricas (one-hot encoding)\ny_train_categorical = to_categorical(y_train)\ny_train_categorical\n\narray([[1., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 1., 0., ..., 0., 0., 0.],\n       [1., 0., 0., ..., 0., 0., 0.],\n       [1., 0., 0., ..., 0., 0., 0.]])\n\n\n\n# crear modelo usando el API funcional\ndef cnn_model(input_shape, num_classes):\n    # Definir la entrada\n    inputs = tf.keras.Input(shape=input_shape)\n\n    # Primera capa convolucional y de pooling\n    x = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\",\n                               strides=(1, 1),\n                               padding=\"valid\")(inputs)\n    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n\n    # Segunda capa convolucional y de pooling\n    x = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\",\n                               strides=(1, 1),\n                               padding=\"valid\")(x)\n    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n\n    # Aplanar y aÃ±adir Dropout\n    x = tf.keras.layers.Flatten()(x)\n    x = tf.keras.layers.Dropout(0.5)(x)\n\n    # Capa de salida\n    outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n\n    # Crear el modelo usando la API funcional\n    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='cnn_keras')\n\n    return model\n\n\n# Ahora se definen los input shape y el numero de clases\ninput_shape = (dim_imagen[0], dim_imagen[1], n_canales)\nnum_classes = 10\n\n# Crear el modelo cnn\ncnn_keras = cnn_model(input_shape, num_classes)\n\n# Visualizar el resumen del modelo\ncnn_keras.summary()\n\nModel: \"cnn_keras\"\n\n\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ Layer (type)                    â”ƒ Output Shape           â”ƒ       Param # â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ input_layer (InputLayer)        â”‚ (None, 28, 28, 1)      â”‚             0 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d (Conv2D)                 â”‚ (None, 26, 26, 32)     â”‚           320 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling2d (MaxPooling2D)    â”‚ (None, 13, 13, 32)     â”‚             0 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_1 (Conv2D)               â”‚ (None, 11, 11, 64)     â”‚        18,496 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling2d_1 (MaxPooling2D)  â”‚ (None, 5, 5, 64)       â”‚             0 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ flatten (Flatten)               â”‚ (None, 1600)           â”‚             0 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout (Dropout)               â”‚ (None, 1600)           â”‚             0 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (Dense)                   â”‚ (None, 10)             â”‚        16,010 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n Total params: 34,826 (136.04 KB)\n\n\n\n Trainable params: 34,826 (136.04 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# tambien es posible visualizar nuestra red en formato de grafo\ntf.keras.utils.plot_model(cnn_keras, rankdir='LR',show_dtype=True)\n\n\n\n\n\n\n\n\n\n# Compilar el modelo\ncnn_keras.compile(loss='categorical_crossentropy',\n            optimizer=SGD(),\n            metrics=['accuracy'])\n\n# Entrenar el modelo\nhistory = cnn_keras.fit(X_train, y_train_categorical,\n                    epochs=30,\n                    batch_size=128,\n                    validation_split=0.2,\n                    verbose=1)\n\n\nEpoch 1/30\n\n307/307 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 32s 103ms/step - accuracy: 0.2604 - loss: 2.1329 - val_accuracy: 0.7763 - val_loss: 0.8651\n\nEpoch 2/30\n\n307/307 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 40s 100ms/step - accuracy: 0.7170 - loss: 0.8741 - val_accuracy: 0.8841 - val_loss: 0.4328\n\nEpoch 3/30\n\n307/307 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 41s 102ms/step - accuracy: 0.8282 - loss: 0.5398 - val_accuracy: 0.9079 - val_loss: 0.3207\n\nEpoch 4/30\n\n307/307 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 30s 98ms/step - accuracy: 0.8755 - loss: 0.4041 - val_accuracy: 0.9270 - val_loss: 0.2540\n\nEpoch 5/30\n\n307/307 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 42s 100ms/step - accuracy: 0.8992 - loss: 0.3252 - val_accuracy: 0.9372 - val_loss: 0.2167\n\nEpoch 6/30\n\n307/307 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 41s 101ms/step - accuracy: 0.9138 - loss: 0.2829 - val_accuracy: 0.9444 - val_loss: 0.1931\n\nEpoch 7/30\n\n307/307 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 41s 101ms/step - accuracy: 0.9231 - loss: 0.2526 - val_accuracy: 0.9512 - val_loss: 0.1733\n\nEpoch 8/30\n\n307/307 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 41s 99ms/step - accuracy: 0.9292 - loss: 0.2346 - val_accuracy: 0.9535 - val_loss: 0.1600\n\nEpoch 9/30\n\n307/307 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 30s 96ms/step - accuracy: 0.9350 - loss: 0.2114 - val_accuracy: 0.9570 - val_loss: 0.1478\n\nEpoch 10/30\n\n307/307 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 31s 100ms/step - accuracy: 0.9376 - loss: 0.2025 - val_accuracy: 0.9600 - val_loss: 0.1391\n\nEpoch 11/30\n\n307/307 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 42s 102ms/step - accuracy: 0.9452 - loss: 0.1841 - val_accuracy: 0.9620 - val_loss: 0.1314\n\nEpoch 12/30\n\n307/307 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 40s 99ms/step - accuracy: 0.9466 - loss: 0.1766 - val_accuracy: 0.9627 - val_loss: 0.1269\n\nEpoch 13/30\n\n307/307 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 30s 99ms/step - accuracy: 0.9497 - loss: 0.1683 - val_accuracy: 0.9649 - val_loss: 0.1202\n\nEpoch 14/30\n\n307/307 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 43s 105ms/step - accuracy: 0.9497 - loss: 0.1671 - val_accuracy: 0.9664 - val_loss: 0.1161\n\nEpoch 15/30\n\n307/307 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 30s 96ms/step - accuracy: 0.9530 - loss: 0.1590 - val_accuracy: 0.9683 - val_loss: 0.1109\n\nEpoch 16/30\n\n307/307 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 42s 99ms/step - accuracy: 0.9527 - loss: 0.1602 - val_accuracy: 0.9686 - val_loss: 0.1073\n\nEpoch 17/30\n\n307/307 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 41s 99ms/step - accuracy: 0.9549 - loss: 0.1468 - val_accuracy: 0.9685 - val_loss: 0.1040\n\nEpoch 18/30\n\n307/307 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 42s 102ms/step - accuracy: 0.9561 - loss: 0.1420 - val_accuracy: 0.9698 - val_loss: 0.1018\n\nEpoch 19/30\n\n307/307 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 39s 96ms/step - accuracy: 0.9587 - loss: 0.1382 - val_accuracy: 0.9705 - val_loss: 0.0985\n\nEpoch 20/30\n\n307/307 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 32s 103ms/step - accuracy: 0.9579 - loss: 0.1386 - val_accuracy: 0.9718 - val_loss: 0.0948\n\nEpoch 21/30\n\n307/307 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 40s 99ms/step - accuracy: 0.9611 - loss: 0.1306 - val_accuracy: 0.9710 - val_loss: 0.0932\n\nEpoch 22/30\n\n307/307 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 42s 103ms/step - accuracy: 0.9593 - loss: 0.1307 - val_accuracy: 0.9719 - val_loss: 0.0905\n\nEpoch 23/30\n\n307/307 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 40s 99ms/step - accuracy: 0.9597 - loss: 0.1280 - val_accuracy: 0.9717 - val_loss: 0.0887\n\nEpoch 24/30\n\n307/307 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 30s 99ms/step - accuracy: 0.9629 - loss: 0.1202 - val_accuracy: 0.9738 - val_loss: 0.0870\n\nEpoch 25/30\n\n307/307 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 31s 100ms/step - accuracy: 0.9644 - loss: 0.1226 - val_accuracy: 0.9735 - val_loss: 0.0851\n\nEpoch 26/30\n\n307/307 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 42s 104ms/step - accuracy: 0.9641 - loss: 0.1221 - val_accuracy: 0.9745 - val_loss: 0.0839\n\nEpoch 27/30\n\n307/307 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 39s 99ms/step - accuracy: 0.9659 - loss: 0.1155 - val_accuracy: 0.9742 - val_loss: 0.0822\n\nEpoch 28/30\n\n307/307 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 41s 99ms/step - accuracy: 0.9650 - loss: 0.1146 - val_accuracy: 0.9745 - val_loss: 0.0809\n\nEpoch 29/30\n\n307/307 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 41s 99ms/step - accuracy: 0.9648 - loss: 0.1158 - val_accuracy: 0.9746 - val_loss: 0.0815\n\nEpoch 30/30\n\n307/307 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 31s 100ms/step - accuracy: 0.9655 - loss: 0.1112 - val_accuracy: 0.9764 - val_loss: 0.0780\n\n\n\n\n\nplot_loss_historia_keras(history)\n\n\n\n\n\n\n\n\n\nplot_acc_historia_keras(history)"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2a_Implementando_una_CNN_usando_Keras.html#evaluaciÃ³n-completa",
    "href": "semana_2/notebooks/Nb_2a_Implementando_una_CNN_usando_Keras.html#evaluaciÃ³n-completa",
    "title": "Implementado una Red Neuronal Convolucional",
    "section": "EvaluaciÃ³n completa",
    "text": "EvaluaciÃ³n completa\n\ny_test_categorical = to_categorical(y_test)\n\nscore = cnn_keras.evaluate(X_test, y_test_categorical, batch_size=128)\n\nscore\n\n\n165/165 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 4s 24ms/step - accuracy: 0.9740 - loss: 0.0853\n\n\n\n\n[0.08383515477180481, 0.9749523997306824]\n\n\n\n# Realizar predicciones en el conjunto de prueba\ny_pred = cnn_keras.predict(X_test)\n\n# Convertir las predicciones en etiquetas (la clase con mayor probabilidad)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true = y_test.values.astype(int)  # Las etiquetas reales del conjunto de prueba\n\n\n657/657 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 8ms/step\n\n\n\n\n\n# Generar el reporte de clasificaciÃ³n\nprint(\"Reporte de ClasificaciÃ³n para la red CNN en MNIST:\\n\")\nprint(classification_report(y_true, y_pred_classes))\n\n# Crear la matriz de confusiÃ³n\ncm = confusion_matrix(y_true, y_pred_classes)\n\n# Visualizar la matriz de confusiÃ³n usando Seaborn\nplot_matriz_confusion(cm)\n\nReporte de ClasificaciÃ³n para la red CNN en MNIST:\n\n              precision    recall  f1-score   support\n\n           0       0.98      0.99      0.99      2058\n           1       0.98      0.99      0.98      2364\n           2       0.96      0.97      0.96      2133\n           3       0.98      0.96      0.97      2176\n           4       0.98      0.98      0.98      1936\n           5       0.98      0.98      0.98      1915\n           6       0.99      0.98      0.99      2088\n           7       0.97      0.97      0.97      2248\n           8       0.96      0.97      0.96      1992\n           9       0.97      0.96      0.97      2090\n\n    accuracy                           0.97     21000\n   macro avg       0.97      0.97      0.97     21000\nweighted avg       0.97      0.97      0.97     21000"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2a_Implementando_una_CNN_usando_Keras.html#visualizaciÃ³n-de-filtros-convolucionales",
    "href": "semana_2/notebooks/Nb_2a_Implementando_una_CNN_usando_Keras.html#visualizaciÃ³n-de-filtros-convolucionales",
    "title": "Implementado una Red Neuronal Convolucional",
    "section": "VisualizaciÃ³n de filtros convolucionales",
    "text": "VisualizaciÃ³n de filtros convolucionales\n\nvisualizacion_filtros_cnn_keras(cnn_keras)"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2a_Implementando_una_CNN_usando_Keras.html#visualizaciÃ³n-de-los-features-maps",
    "href": "semana_2/notebooks/Nb_2a_Implementando_una_CNN_usando_Keras.html#visualizaciÃ³n-de-los-features-maps",
    "title": "Implementado una Red Neuronal Convolucional",
    "section": "VisualizaciÃ³n de los features maps",
    "text": "VisualizaciÃ³n de los features maps\n\nvisualizacion_feature_maps(cnn_keras, image=X_test[randint(0, 100),:,:,:])\n\n\n1/1 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 70ms/step"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html",
    "href": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html",
    "title": "Explicando la Mejora de los Transformers sobre las RNNs",
    "section": "",
    "text": "Ãšltima actualizaciÃ³n 07/05/2025\n#@title Importar librerÃ­as\n#importar librerÃ­as necesarias\nimport os\nimport logging\nimport time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\n\nimport tensorflow_text\n\nimport warnings\nwarnings.filterwarnings('ignore')\n#@title Funciones complementarias\ndef plot_similaridad_positional_encodings(pos_encoding):\n  # normalizaciÃ³n de los vectores a 1\n  pos_encoding/=tf.norm(pos_encoding, axis=1, keepdims=True)\n  # seleccionamos el vector de la posiciÃ³n 1000\n  p = pos_encoding[1000]\n  # cÃ¡lculo de la similitud del producto punto\n  dots = tf.einsum('pd,d -&gt; p', pos_encoding, p)\n\n  # visualizaciÃ³n de la relaciÃ³n de los vectores con sus palabras\n  # vecinas, por definiciÃ³n tendran mucha similaridad.\n  plt.subplot(2,1,1)\n  plt.plot(dots)\n  plt.ylim([0,1])\n  plt.plot([950, 950, float('nan'), 1050, 1050],\n          [0,1,float('nan'),0,1], color='k', label='Zoom')\n  plt.legend()\n  plt.subplot(2,1,2)\n  plt.plot(dots)\n  plt.xlim([950, 1050])\n  plt.ylim([0,1])\n\ndef plot_distribucion_longitudes_tokens(all_lengths):\n  plt.hist(all_lengths, np.linspace(0, 500, 101))\n  plt.ylim(plt.ylim())\n  max_length = max(all_lengths)\n  plt.plot([max_length, max_length], plt.ylim())\n  plt.title(f'NÃºmero mÃ¡ximo de tokens por muestra: {max_length}');\n\ndef plot_positional_encodings(pos_encoding):\n  # GrÃ¡ficar las dimensiones\n  plt.pcolormesh(pos_encoding.numpy().T, cmap='RdBu')\n  plt.ylabel('Profundidad')\n  plt.xlabel('PosiciÃ³n')\n  plt.colorbar()\n  plt.show()\n\ndef plot_lr_planificador(learning_rate):\n  plt.plot(learning_rate(tf.range(40000, dtype=tf.float32)))\n  plt.ylabel('Learning Rate');\n  plt.xlabel('Paso de entrenamiento');\n\ndef plot_attention_head(in_tokens, translated_tokens, attention):\n  # Saltar el token start.\n  translated_tokens = translated_tokens[1:]\n\n  ax = plt.gca()\n  ax.matshow(attention)\n  ax.set_xticks(range(len(in_tokens)))\n  ax.set_yticks(range(len(translated_tokens)))\n\n  labels = [label.decode('utf-8') for label in in_tokens.numpy()]\n  ax.set_xticklabels(\n      labels, rotation=90)\n\n  labels = [label.decode('utf-8') for label in translated_tokens.numpy()]\n  ax.set_yticklabels(labels)\n\ndef plot_attention_weights(sentence, translated_tokens, attention_heads):\n  in_tokens = tf.convert_to_tensor([sentence])\n  in_tokens = tokenizers.pt.tokenize(in_tokens).to_tensor()\n  in_tokens = tokenizers.pt.lookup(in_tokens)[0]\n\n  fig = plt.figure(figsize=(12, 15))\n\n  for h, head in enumerate(attention_heads):\n    ax = fig.add_subplot(2, 4, h+1)\n\n    plot_attention_head(in_tokens, translated_tokens, head)\n\n    ax.set_xlabel(f'Head {h+1}')\n\n  plt.tight_layout()\n  plt.show()\n\ndef plot_traduccion(sentence, tokens, ground_truth):\n  print(f'{\"Input:\":15s}: {sentence}')\n  print(f'{\"PredicciÃ³n\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n  print(f'{\"Ground truth\":15s}: {ground_truth}')\nLos Transformers: Una Nueva Era en el Procesamiento de Secuencias\nLas Redes Neuronales Recurrentes (RNNs) han sido una herramienta fundamental para el procesamiento de datos secuenciales, como el lenguaje natural. Sin embargo, los Transformers, propuestos en el artÃ­culo â€œAttention is all you needâ€, han revolucionado este campo, ofreciendo ventajas significativas sobre las RNNs. Los Transformers son redes neuronales profundas que reemplazan las CNNs y las RNNs. Estos introducen la auto-atenciÃ³n (self-attention) permite a los Transformers transmitir informaciÃ³n fÃ¡cilmente a travÃ©s de las secuencias de entrada.\nÂ¿Por quÃ© los Transformers son importantes?\nEn este notebook, exploraremos en profundidad los componentes clave de los Transformers y compararemos su funcionamiento con el de las RNNs, destacando las razones de su superioridad."
  },
  {
    "objectID": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#esquema-de-trabajo",
    "href": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#esquema-de-trabajo",
    "title": "Explicando la Mejora de los Transformers sobre las RNNs",
    "section": "Esquema de trabajo",
    "text": "Esquema de trabajo\nCon lo anterior en mente, y una vez visto un poco las diferencias entre RNNs y Transformers, vamos a abordar los siguientes contenidos de manera detallada:\n\nPrepararÃ¡s los datos.\nImplementarÃ¡s los componentes necesarios:\n\nEmbeddings posicionales.\nCapas de atenciÃ³n.\nEl codificador y el decodificador.\n\nConstruirÃ¡s y entrenarÃ¡s el Transformer.\nGenerarÃ¡s traducciones.\nExportarÃ¡s el modelo.\n\n\nCarga y Preprocesamiento de Datos\nCargaremos un conjunto de datos de traducciÃ³n PortuguÃ©s-InglÃ©s y utilizaremos un tokenizador para preparar el texto para el modelo. Este conjunto de datos contiene aproximadamente 52.000 ejemplos de entrenamiento, 1.200 de validaciÃ³n y 1.800 de prueba.\n\nexamples, metadata = tfds.load('ted_hrlr_translate/pt_to_en',\n                               with_info=True,\n                               as_supervised=True)\n\ntrain_examples, val_examples = examples['train'], examples['validation']\n\n\n# Mostrar algunos ejemplos de parejas de oraciones\nfor pt_examples, en_examples in train_examples.batch(3).take(1):\n  print('-&gt; Ejemplos en portuguÃ©s:')\n  for pt in pt_examples.numpy():\n    print(pt.decode('utf-8'))\n  print()\n\n  print('-&gt; TraducciÃ³n al inglÃ©s:')\n  for en in en_examples.numpy():\n    print(en.decode('utf-8'))\n\n-&gt; Ejemplos en portuguÃ©s:\ne quando melhoramos a procura , tiramos a Ãºnica vantagem da impressÃ£o , que Ã© a serendipidade .\nmas e se estes fatores fossem ativos ?\nmas eles nÃ£o tinham a curiosidade de me testar .\n\n-&gt; TraducciÃ³n al inglÃ©s:\nand when you improve searchability , you actually take away the one advantage of print , which is serendipity .\nbut what if it were active ?\nbut they did n't test for curiosity .\n\n\n\nTokenizadores\nEn nuestro ejemplo vamos a usar los tokenizadores construidos en el tutorial subword tokenizer. Ese tutorial optimiza dos objetos text.BertTokenizer (uno para inglÃ©s, otro para portuguÃ©s) para este conjunto de datos y los exporta en formato saved_model de TensorFlow.\n\nmodel_name = 'ted_hrlr_translate_pt_en_converter'\ntf.keras.utils.get_file(\n    f'{model_name}.zip',\n    f'https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip',\n    cache_dir='.', cache_subdir='', extract=True\n)\n\n'./ted_hrlr_translate_pt_en_converter_extracted'\n\n\n\n# cargar los tokenizadores\ntokenizers = tf.saved_model.load(f'{model_name}_extracted/{model_name}')\n\n\n# ambos tokenizadores tienen los mismo mÃ©todos\n[item for item in dir(tokenizers.en) if not item.startswith('_')]\n\n['detokenize',\n 'get_reserved_tokens',\n 'get_vocab_path',\n 'get_vocab_size',\n 'lookup',\n 'tokenize',\n 'tokenizer',\n 'vocab']\n\n\n\n# por ejemplo revisemos los vocab\ntokenizers.en.vocab.shape, tokenizers.pt.vocab.shape\n\n(TensorShape([7010]), TensorShape([7765]))\n\n\nEl mÃ©todo tokenize convierte un grupo de oraciones en un grupo de identificadores de tokens de una misma longitud (padding). Este mÃ©todo separa los signos de puntuaciÃ³n, las minÃºsculas y normaliza el texto de entrada antes de la tokenizaciÃ³n. Esta normalizaciÃ³n no es visible aquÃ­ porque los datos de entrada ya estÃ¡n normalizados.\n\nprint('-&gt; Esto es un grupo de cadenas:')\nfor en in en_examples.numpy():\n  print(en.decode('utf-8'))\n\n-&gt; Esto es un grupo de cadenas:\nand when you improve searchability , you actually take away the one advantage of print , which is serendipity .\nbut what if it were active ?\nbut they did n't test for curiosity .\n\n\n\nencoded = tokenizers.en.tokenize(en_examples)\n\nprint('-&gt; Este es el grupo de cedena de ID de tokens (con padding)')\nfor row in encoded.to_list():\n  print(row)\n\n-&gt; Este es el grupo de cedena de ID de tokens (con padding)\n[2, 72, 117, 79, 1259, 1491, 2362, 13, 79, 150, 184, 311, 71, 103, 2308, 74, 2679, 13, 148, 80, 55, 4840, 1434, 2423, 540, 15, 3]\n[2, 87, 90, 107, 76, 129, 1852, 30, 3]\n[2, 87, 83, 149, 50, 9, 56, 664, 85, 2512, 15, 3]\n\n\nEl mÃ©todo detokenize convierte los ID de token de nuevo en texto legible normal:\n\nround_trip = tokenizers.en.detokenize(encoded)\n\nprint('-&gt; Correspondiente texto:')\nfor line in round_trip.numpy():\n  print(line.decode('utf-8'))\n\n-&gt; Correspondiente texto:\nand when you improve searchability , you actually take away the one advantage of print , which is serendipity .\nbut what if it were active ?\nbut they did n ' t test for curiosity .\n\n\nEl mÃ©todo lookup convierte de token-IDs a token-texto:\n\nprint('-&gt; Este es el texto dividido en tokens:')\ntokens = tokenizers.en.lookup(encoded)\ntokens\n\n-&gt; Este es el texto dividido en tokens:\n\n\n&lt;tf.RaggedTensor [[b'[START]', b'and', b'when', b'you', b'improve', b'search', b'##ability',\n  b',', b'you', b'actually', b'take', b'away', b'the', b'one', b'advantage',\n  b'of', b'print', b',', b'which', b'is', b's', b'##ere', b'##nd', b'##ip',\n  b'##ity', b'.', b'[END]']                                                 ,\n [b'[START]', b'but', b'what', b'if', b'it', b'were', b'active', b'?',\n  b'[END]']                                                           ,\n [b'[START]', b'but', b'they', b'did', b'n', b\"'\", b't', b'test', b'for',\n  b'curiosity', b'.', b'[END]']                                          ]&gt;\n\n\nLa salida demuestra el aspecto de \"subword\" de la tokenizaciÃ³n de subpalabras.\nPor ejemplo, la palabra 'searchability' se descompone en 'search' y '##ability', y la palabra 'serendipity' en 's', '##ere', '##nd', '##ip' e '##ity'.\nTen en cuenta que el texto tokenizado incluye los tokens '[START]' y '[END]'.\nLa distribuciÃ³n de tokens por ejemplo en el conjunto de datos es la siguiente:\n\nlengths = []\n\nfor pt_examples, en_examples in train_examples.batch(1024):\n  pt_tokens = tokenizers.pt.tokenize(pt_examples)\n  lengths.append(pt_tokens.row_lengths())\n\n  en_tokens = tokenizers.en.tokenize(en_examples)\n  lengths.append(en_tokens.row_lengths())\n  print('.', end='', flush=True)\n\n...................................................\n\n\n\nall_lengths = np.concatenate(lengths)\n\nplot_distribucion_longitudes_tokens(all_lengths)\n\n\n\n\n\n\n\n\n\n\nConfiguraciÃ³n del pipeline de datos usando tf.data\nLa siguiente funciÃ³n toma batches de texto como entrada y los convierte a un formato adecuado para el entrenamiento.\n\nLos tokeniza en lotes de diferentes dimensiones (ragged).\nRecorta cada uno para que no tenga mÃ¡s de MAX_TOKENS.\nDivide los tokens objetivo (inglÃ©s) en entradas y etiquetas. Estos se desplazan un paso de modo que en cada ubicaciÃ³n de entrada, la etiqueta es el ID del siguiente token.\nConvierte los RaggedTensor en Tensor densos con padding.\nDevuelve un par (entradas, etiquetas).\n\n\nMAX_TOKENS=64\ndef prepare_batch(pt, en):\n    pt = tokenizers.pt.tokenize(pt) # la sÃ¡lida tiene diferentes longitudes\n    pt = pt[:, :MAX_TOKENS]    # Truncar al max # de tokens\n    pt = pt.to_tensor()  # Convertir a un tensor denso sin padding\n\n    en = tokenizers.en.tokenize(en)\n    en = en[:, :(MAX_TOKENS+1)] # para poder hacer el shift\n    en_inputs = en[:, :-1].to_tensor()  # Elimina los tokens [END]\n    en_labels = en[:, 1:].to_tensor()   # Eliminar los tokens [START]\n\n    return (pt, en_inputs), en_labels\n\nLa siguiente funciÃ³n convierte un conjunto de datos de ejemplos de texto en datos de lotes para el entrenamiento.\n\nTokeniza el texto y filtra las secuencias que son demasiado largas.\nEl mÃ©todo cache asegura que ese trabajo solo se ejecute una vez.\nLuego, shuffle y preparar el batch.\nFinalmente, prefetch ejecuta el conjunto de datos en paralelo con el modelo para asegurar que los datos estÃ©n disponibles cuando se necesiten. Consulta Mejorar rendimiento con tf.data para mÃ¡s detalles.\n\n\nBUFFER_SIZE = 10000\nBATCH_SIZE = 64\n\n\ndef make_batches(ds):\n  return (\n      ds\n      .shuffle(BUFFER_SIZE)\n      .batch(BATCH_SIZE)\n      .map(prepare_batch, tf.data.AUTOTUNE)\n      .prefetch(buffer_size=tf.data.AUTOTUNE))\n\n\n\nProbar nuestro pipeline de datos\n\n# Crear los conjuntos de entrenamiento y validaciÃ³n\ntrain_batches = make_batches(train_examples)\nval_batches = make_batches(val_examples)\n\nComo se verÃ­an las entradas y sÃ¡lidas en nuestro pipeline de datos?\n\n\n\nInputs en la parte inferior, labels en la parte superior.\n\n\n\n\n\n\n\n\nEsta configuraciÃ³n se llama â€œteacher forcingâ€ porque, independientemente de la salida del modelo en cada paso de tiempo, recibe el valor verdadero como entrada para el siguiente paso de tiempo. Esta es una forma simple y eficiente de entrenar un modelo de generaciÃ³n de texto. Es eficiente porque no necesitas ejecutar el modelo secuencialmente; las salidas en las diferentes ubicaciones de la secuencia se pueden calcular en paralelo.\n\n# visualizar un ejemplo de nuestros datos para entrenar\nfor (pt, en), en_labels in train_batches.take(1):\n  break\n\nprint(pt.shape)\nprint(en.shape)\nprint(en_labels.shape)\n\n(64, 64)\n(64, 64)\n(64, 64)\n\n\nLas etiquetas en y en_labels son las mismas, sÃ³lo que desplazadas en 1:\n\nprint(en[0][:10])\nprint(en_labels[0][:10])\n\ntf.Tensor([ 2 90 80 81 85 30  0  0  0  0], shape=(10,), dtype=int64)\ntf.Tensor([90 80 81 85 30  3  0  0  0  0], shape=(10,), dtype=int64)\n\n\n\n\n\nDefinir los componentes del Transformer\nDentro de un Transformer pasan muchas cosas. Las cosas importantes que hay que recordar son:\n\nSigue el mismo patrÃ³n general que un modelo estÃ¡ndar secuencia-a-secuencia con un codificador y un decodificador.\nSi trabajas paso a paso, todo tendrÃ¡ sentido.\n\n\n\n\nDiagrama original del Transformer\n\n\nRepresentaciÃ³n de un Transformer de 4 capas\n\n\n\n\n\n\n\n\n\n\n\n\nLa capa para la codificaciÃ³n de la posiciÃ³n\nLas entradas tanto del codificador como del descodificador utilizan la misma lÃ³gica de incrustaciÃ³n y codificaciÃ³n posicional\n\n\n\nThe embedding and positional encoding layer\n\n\n\n\n\n\n\n\nDada una secuencia de tokens, tanto los tokens de entrada (portuguÃ©s) como los tokens objetivo (inglÃ©s) deben convertirse en vectores utilizando una capa tf.keras.layers.Embedding.\nLas capas de atenciÃ³n utilizadas en todo el modelo ven su entrada como un conjunto de vectores, sin ningÃºn orden. Dado que el modelo no contiene ninguna capa recurrente o convolucional, necesita alguna forma de identificar el orden de las palabras; de lo contrario, verÃ­a la secuencia de entrada como una instancia de bolsa de palabras, cÃ³mo estÃ¡s, cÃ³mo tÃº estÃ¡s, tÃº cÃ³mo estÃ¡s, y asÃ­ sucesivamente, son indistinguibles.\nPor lo tanto, el Transformer agrega una â€œCodificaciÃ³n Posicionalâ€ a los vectores de embedding. Utiliza un conjunto de senos y cosenos en diferentes frecuencias (a lo largo de la secuencia). Por definiciÃ³n, los elementos cercanos tendrÃ¡n codificaciones posicionales similares.\nEl paper original usa la siguiente formÃºla para la codificaciÃ³n posicional:\n\\[\\Large{PE_{(pos, 2i)} = \\sin(pos / 10000^{2i / d_{model}})} \\] \\[\\Large{PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i / d_{model}})} \\]\nNota: El cÃ³digo a continuaciÃ³n lo implementa, pero en lugar de intercalar los senos y cosenos, los vectores de senos y cosenos simplemente se concatenan. Permutar los canales de esta manera es funcionalmente equivalente, y un poco mÃ¡s fÃ¡cil de implementar y mostrar en las grÃ¡ficas siguientes.\nExplicaciÃ³n Paso a Paso: Imaginemos que tenemos una frase: â€œEl gato estÃ¡ en la alfombraâ€.\nY supongamos que d_model = 4. Esto significa que cada posiciÃ³n (cada palabra) se representarÃ¡ con un vector de 4 nÃºmeros.\n\nPosiciones:\n\n\nâ€œElâ€ estÃ¡ en la posiciÃ³n 0.\nâ€œgatoâ€ estÃ¡ en la posiciÃ³n 1.\nâ€œestÃ¡â€ estÃ¡ en la posiciÃ³n 2.\nâ€œenâ€ estÃ¡ en la posiciÃ³n 3.\nâ€œlaâ€ estÃ¡ en la posiciÃ³n 4.\nâ€œalfombraâ€ estÃ¡ en la posiciÃ³n 5.\n\n\nDimensiones:\n\nComo d_model = 4, tendremos dimensiones 0, 1, 2, y 3 en nuestro vector de codificaciÃ³n posicional.\n\nAplicando la FÃ³rmula (Ejemplo: la palabra â€œgatoâ€ en la posiciÃ³n 1):\n\n\nPara la dimensiÃ³n 0 (2i = 0, entonces i = 0):\n\nPE(1, 0) = sin(1 / 10000^(2*0 / 4)) = sin(1 / 10000^0) = sin(1 / 1) = sin(1) â‰ˆ 0.841\n\nPara la dimensiÃ³n 1 (2i + 1 = 1, entonces i = 0):\n\nPE(1, 1) = cos(1 / 10000^(2*0 / 4)) = cos(1 / 10000^0) = cos(1 / 1) = cos(1) â‰ˆ 0.540\n\nPara la dimensiÃ³n 2 (2i = 2, entonces i = 1):\n\nPE(1, 2) = sin(1 / 10000^(2*1 / 4)) = sin(1 / 10000^(1/2)) = sin(1 / 100) = sin(0.01) â‰ˆ 0.01\n\nPara la dimensiÃ³n 3 (2i + 1 = 3, entonces i = 1):\n\nPE(1, 3) = cos(1 / 10000^(2*1 / 4)) = cos(1 / 10000^(1/2)) = cos(1 / 100) = cos(0.01) â‰ˆ 0.99995\nEntonces, la codificaciÃ³n posicional para la palabra â€œgatoâ€ (posiciÃ³n 1) serÃ­a aproximadamente el vector: [0.841, 0.540, 0.01, 0.99995].\n\ndef positional_encoding(length, depth):\n  depth = depth/2 # mitad de las dimensiones para cada funciÃ³n\n\n  positions = np.arange(length)[:, np.newaxis]     # shape = (seq, 1)\n  depths = np.arange(depth)[np.newaxis, :]/depth   # shape = (1, depth)\n\n  angle_rates = 1 / (10000**depths)         # (1, depth)\n  angle_rads = positions * angle_rates      # (pos, depth)\n\n  pos_encoding = np.concatenate(\n      [np.sin(angle_rads), np.cos(angle_rads)],\n      axis=-1)\n\n  return tf.cast(pos_encoding, dtype=tf.float32)\n\nLa funciÃ³n de codificaciÃ³n de posiciÃ³n es una pila de senos y cosenos que vibran a distintas frecuencias segÃºn su ubicaciÃ³n a lo largo de la profundidad del vector de incrustaciÃ³n. Vibran a travÃ©s del eje de posiciÃ³n.\n\n# para el ejemplo de nuestra frase: El gato esta en la alfombra\n# en este caso se concatenaron las funciones\npositional_encoding(length=6, depth=4)\n\n&lt;tf.Tensor: shape=(6, 4), dtype=float32, numpy=\narray([[ 0.        ,  0.        ,  1.        ,  1.        ],\n       [ 0.84147096,  0.00999983,  0.5403023 ,  0.99995   ],\n       [ 0.9092974 ,  0.01999867, -0.41614684,  0.9998    ],\n       [ 0.14112   ,  0.0299955 , -0.9899925 ,  0.99955004],\n       [-0.7568025 ,  0.03998933, -0.6536436 ,  0.9992001 ],\n       [-0.9589243 ,  0.04997917,  0.2836622 ,  0.99875027]],\n      dtype=float32)&gt;\n\n\n\npos_encoding = positional_encoding(length=2048, depth=512)\n\n# Revisar la dimensiÃ³n\nprint(pos_encoding.shape)\n\nplot_positional_encodings(pos_encoding)\n\n(2048, 512)\n\n\n\n\n\n\n\n\n\nPor definiciÃ³n, estos vectores se alinean bien con los vectores cercanos a lo largo del eje de posiciÃ³n. A continuaciÃ³n, los vectores de codificaciÃ³n de posiciÃ³n se normalizan y el vector de la posiciÃ³n 1000 se compara, por producto punto, con todos los demÃ¡s:\n\nplot_similaridad_positional_encodings(pos_encoding)\n\n\n\n\n\n\n\n\nAhora creemos la capa: PositionEmbedding\n\nclass PositionalEmbedding(tf.keras.layers.Layer):\n  def __init__(self, vocab_size, d_model):\n    super().__init__()\n    self.d_model = d_model\n    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n\n  def compute_mask(self, *args, **kwargs):\n    return self.embedding.compute_mask(*args, **kwargs)\n\n  def call(self, x):\n    length = tf.shape(x)[1]\n    x = self.embedding(x)\n    # Este factor establece la escala relativa de la incrustaciÃ³n y la codificaciÃ³n_positonal.\n    # Es decir asegurar que tengan escalas similares\n    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n    # Se suma las posiciones a los embeddings de los tokens\n    x = x + self.pos_encoding[tf.newaxis, :length, :]\n    return x\n\n\nembed_pt = PositionalEmbedding(vocab_size=tokenizers.pt.get_vocab_size().numpy(), d_model=512)\nembed_en = PositionalEmbedding(vocab_size=tokenizers.en.get_vocab_size().numpy(), d_model=512)\n\npt_emb = embed_pt(pt)\nen_emb = embed_en(en)\n\n\n# la mÃ¡scara de cada oraciÃ³n, recordar que las oraciones no tiene la misma\n# longitud, asi que se aplica la mÃ¡scara\nen_emb._keras_mask\n\n&lt;tf.Tensor: shape=(64, 64), dtype=bool, numpy=\narray([[ True,  True,  True, ..., False, False, False],\n       [ True,  True,  True, ..., False, False, False],\n       [ True,  True,  True, ..., False, False, False],\n       ...,\n       [ True,  True,  True, ..., False, False, False],\n       [ True,  True,  True, ..., False, False, False],\n       [ True,  True,  True, ..., False, False, False]])&gt;\n\n\n\npt_emb\n\n&lt;tf.Tensor: shape=(64, 64, 512), dtype=float32, numpy=\narray([[[ 0.81105447, -0.7717942 ,  0.5229695 , ...,  0.02146667,\n          0.24027777,  0.26449662],\n        [ 1.8670975 ,  1.6279981 ,  0.97901094, ...,  1.3392618 ,\n          0.8188071 ,  0.21744752],\n        [-0.11729413,  0.5008898 ,  0.32620972, ...,  0.27159345,\n          1.1697826 ,  1.4658518 ],\n        ...,\n        [-1.3773707 ,  0.633837  ,  1.1719698 , ...,  1.6278234 ,\n          0.05931401,  0.90110606],\n        [-1.1504335 , -0.23321328,  1.8688756 , ...,  1.6278226 ,\n          0.0593133 ,  0.90110534],\n        [-0.24389721, -0.9982976 ,  1.8318357 , ...,  1.6278219 ,\n          0.05931258,  0.9011047 ]],\n\n       [[ 0.81105447, -0.7717942 ,  0.5229695 , ...,  0.02146667,\n          0.24027777,  0.26449662],\n        [-0.03075731,  0.79514253,  1.7891788 , ...,  0.0534128 ,\n          0.01318026,  0.86922586],\n        [ 1.0960544 ,  0.75714344,  0.7072108 , ...,  0.3786983 ,\n          1.0271425 ,  1.3102653 ],\n        ...,\n        [-1.3773707 ,  0.633837  ,  1.1719698 , ...,  1.6278234 ,\n          0.05931401,  0.90110606],\n        [-1.1504335 , -0.23321328,  1.8688756 , ...,  1.6278226 ,\n          0.0593133 ,  0.90110534],\n        [-0.24389721, -0.9982976 ,  1.8318357 , ...,  1.6278219 ,\n          0.05931258,  0.9011047 ]],\n\n       [[ 0.81105447, -0.7717942 ,  0.5229695 , ...,  0.02146667,\n          0.24027777,  0.26449662],\n        [ 1.7584805 ,  1.4909694 , -0.1760881 , ...,  2.1180818 ,\n          0.4953122 ,  0.23008263],\n        [ 0.55767405,  1.4934946 ,  0.02770001, ...,  0.84077555,\n          0.9217277 ,  1.9566159 ],\n        ...,\n        [-1.3773707 ,  0.633837  ,  1.1719698 , ...,  1.6278234 ,\n          0.05931401,  0.90110606],\n        [-1.1504335 , -0.23321328,  1.8688756 , ...,  1.6278226 ,\n          0.0593133 ,  0.90110534],\n        [-0.24389721, -0.9982976 ,  1.8318357 , ...,  1.6278219 ,\n          0.05931258,  0.9011047 ]],\n\n       ...,\n\n       [[ 0.81105447, -0.7717942 ,  0.5229695 , ...,  0.02146667,\n          0.24027777,  0.26449662],\n        [ 0.7150625 ,  1.6491615 ,  1.213108  , ...,  1.0735046 ,\n          0.3283956 ,  0.01745564],\n        [ 1.1696244 ,  1.2925339 ,  1.6480486 , ...,  0.24500364,\n          1.4749924 ,  1.9941585 ],\n        ...,\n        [-1.3773707 ,  0.633837  ,  1.1719698 , ...,  1.6278234 ,\n          0.05931401,  0.90110606],\n        [-1.1504335 , -0.23321328,  1.8688756 , ...,  1.6278226 ,\n          0.0593133 ,  0.90110534],\n        [-0.24389721, -0.9982976 ,  1.8318357 , ...,  1.6278219 ,\n          0.05931258,  0.9011047 ]],\n\n       [[ 0.81105447, -0.7717942 ,  0.5229695 , ...,  0.02146667,\n          0.24027777,  0.26449662],\n        [-0.23110986,  0.5054298 , -0.25015187, ...,  1.1486163 ,\n          1.1958522 ,  1.1460018 ],\n        [ 1.4417007 , -0.06407106,  0.7872464 , ...,  0.72261333,\n          0.1937148 ,  2.0049632 ],\n        ...,\n        [-1.3773707 ,  0.633837  ,  1.1719698 , ...,  1.6278234 ,\n          0.05931401,  0.90110606],\n        [-1.1504335 , -0.23321328,  1.8688756 , ...,  1.6278226 ,\n          0.0593133 ,  0.90110534],\n        [-0.24389721, -0.9982976 ,  1.8318357 , ...,  1.6278219 ,\n          0.05931258,  0.9011047 ]],\n\n       [[ 0.81105447, -0.7717942 ,  0.5229695 , ...,  0.02146667,\n          0.24027777,  0.26449662],\n        [ 1.7306108 ,  1.4167533 ,  1.2637417 , ..., -0.04352736,\n          0.04924804,  0.62587   ],\n        [ 0.03706914,  0.90970105,  1.9453614 , ...,  0.0534128 ,\n          0.01318026,  0.86922586],\n        ...,\n        [-1.3773707 ,  0.633837  ,  1.1719698 , ...,  1.6278234 ,\n          0.05931401,  0.90110606],\n        [-1.1504335 , -0.23321328,  1.8688756 , ...,  1.6278226 ,\n          0.0593133 ,  0.90110534],\n        [-0.24389721, -0.9982976 ,  1.8318357 , ...,  1.6278219 ,\n          0.05931258,  0.9011047 ]]], dtype=float32)&gt;\n\n\n\n\nCapas de AdiciÃ³n y normalizaciÃ³n\n\n\n\nAdd y normalize\n\n\n\n\n\n\n\n\nEstos bloques de â€œAdd & Normâ€ se encuentran distribuidos a lo largo de todo el modelo Transformer. Cada uno combina una conexiÃ³n residual y pasa el resultado a travÃ©s de una capa de LayerNormalization.\nLa manera mÃ¡s sencilla de organizar el cÃ³digo es estructurÃ¡ndolo alrededor de estos bloques residuales. En las siguientes secciones, definiremos clases de capas personalizadas para cada uno de ellos.\nLos bloques residuales â€œAdd & Normâ€ se incluyen para que el entrenamiento sea eficiente. La conexiÃ³n residual proporciona una ruta directa para el gradiente (y asegura que los vectores sean actualizados por las capas de atenciÃ³n en lugar de ser reemplazados), mientras que la normalizaciÃ³n mantiene una escala razonable para las salidas.\nNota: Las implementaciones que se muestran a continuaciÃ³n utilizan la capa Add para asegurar que las mÃ¡scaras de Keras se propaguen correctamente (el operador + no lo hace).\n\n\nBases de la capa de atenciÃ³n\nLas capas de atenciÃ³n se utilizan a lo largo de todo el modelo Transformer. Todas ellas son idÃ©nticas, excepto por cÃ³mo se configura la atenciÃ³n. Cada una contiene una capa layers.MultiHeadAttention, una capa layers.LayerNormalization y una capa layers.Add.\n\n\n\nCapa de atenciÃ³n bÃ¡sica\n\n\n\n\n\n\n\n\nPara implementar estas capas de atenciÃ³n, comenzaremos con una clase base simple que sÃ³lo contenga definidos los componentes. Cada caso de uso se implementarÃ¡ como una subclase (framework).\n\nclass BaseAttention(tf.keras.layers.Layer):\n  def __init__(self, **kwargs):\n    super().__init__()\n    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n    self.layernorm = tf.keras.layers.LayerNormalization()\n    self.add = tf.keras.layers.Add()\n\n\nAtenciÃ³n, cÃ³mo funciona?\nAntes de entrar en los detalles de cada uso, aquÃ­ tienes un breve repaso de cÃ³mo funciona la atenciÃ³n:\n\n\n\nCapa de atenciÃ³n bÃ¡sica\n\n\n\n\n\n\n\n\nHay dos entradas: 1. La secuencia de consulta (query sequence): la secuencia que se estÃ¡ procesando; la secuencia que â€œatiendeâ€ (abajo). 2. La secuencia de contexto (context sequence): la secuencia a la que se estÃ¡ â€œatendiendoâ€ (izquierda).\nLa salida tiene la misma forma que la secuencia de consulta.\nUna analogÃ­a comÃºn es que esta operaciÃ³n se asemeja a una bÃºsqueda en un diccionario. Una bÃºsqueda en un diccionario difuso, diferenciable y vectorizado.\nAquÃ­ tienes un diccionario regular de Python, con 3 claves y 3 valores al que se le pasa una Ãºnica consulta.\nd = {'color': 'azul', 'edad': 22, 'tipo': 'pickup'}\nresult = d['color']\n\nEl query es lo que se esta tratando de encontrar.\nThe key es el tipo de informaciÃ³n que tiene el diccionario\nThe value es la informaciÃ³n\n\nCuando buscas una query (consulta) en un diccionario normal, el diccionario encuentra la key (clave) coincidente y devuelve su value (valor) asociado. La query tiene una key coincidente o no la tiene.\nPuedes imaginar un diccionario difuso donde las claves no tienen que coincidir perfectamente. Si buscaras d[\"especie\"] en el diccionario de arriba, quizÃ¡s querrÃ­as que devolviera \"pickup\" ya que es la mejor coincidencia para la consulta.\nUna capa de atenciÃ³n realiza una bÃºsqueda difusa como esta, pero no solo busca la mejor clave. Combina los values basÃ¡ndose en quÃ© tan bien la query coincide con cada key.\nÂ¿CÃ³mo funciona esto? En una capa de atenciÃ³n, la query, la key y el value son cada uno vectores. En lugar de realizar una bÃºsqueda de asignaciÃ³n, la capa de atenciÃ³n combina los vectores de la query y la key para determinar quÃ© tan bien coinciden, obteniendo una â€œpuntuaciÃ³n de atenciÃ³nâ€ (attention score). La capa devuelve el promedio de todos los values, ponderado por las â€œpuntuaciones de atenciÃ³nâ€.\nCada posiciÃ³n en la secuencia de consulta (query sequence) proporciona un vector de query. La secuencia de contexto (context sequence) actÃºa como el diccionario. Cada posiciÃ³n en la secuencia de contexto proporciona un vector de key y un vector de value.\nLos vectores de entrada no se utilizan directamente; la capa layers.MultiHeadAttention incluye capas layers.Dense para proyectar los vectores de entrada antes de utilizarlos.\n\n\nCapa de atenciÃ³n cruzada\nEn el centro literal del Transformer estÃ¡ la capa de atenciÃ³n cruzada. Esta capa conecta el codificador y el decodificador. Esta capa es el uso mÃ¡s directo de la atenciÃ³n en el modelo, realiza la misma tarea que el bloque de atenciÃ³n en el Tutorial NMT con atenciÃ³n.\n\n\n\nAtenciÃ³n cruzada\n\n\n\n\n\n\n\n\nPara implementar esto, pasas la secuencia objetivo x como la query (consulta) y la secuencia de context (contexto) como la key/value (clave/valor) al llamar a la capa mha:\n\nclass CrossAttention(BaseAttention):\n  def call(self, x, context):\n    attn_output, attn_scores = self.mha(\n        query=x,\n        key=context,\n        value=context,\n        return_attention_scores=True)\n\n    # Almacenar los scores de atenciÃ³n para\n    # visualizarlos mÃ¡s adelante\n    self.last_attn_scores = attn_scores\n\n    x = self.add([x, attn_output])\n    x = self.layernorm(x)\n\n    return x\n\nLa siguiente caricatura muestra cÃ³mo fluye la informaciÃ³n a travÃ©s de esta capa. Las columnas representan la suma ponderada de la secuencia contextual.\nPara simplificar, no se muestran las conexiones residuales.\n\n\n\nAtenciÃ³n cruzada\n\n\n\n\n\n\n\n\nLa longitud de la salida es la longitud de la secuencia de query (consulta), y no la longitud de la secuencia de key/value (clave/valor) del contexto.\nEl diagrama se simplifica aÃºn mÃ¡s a continuaciÃ³n. No es necesario dibujar la matriz completa de â€œpesos de atenciÃ³nâ€.\nEl punto clave es que cada ubicaciÃ³n en la query puede â€œverâ€ todos los pares de key/value en el contexto, pero no se intercambia informaciÃ³n entre las diferentes consultas.\n\n\n\nCada elemento del query puede ver todo el contexto.\n\n\n\n\n\n\n\n\nEjemplo:\n\nsample_ca = CrossAttention(num_heads=2, key_dim=512)\n\nprint(pt_emb.shape)\nprint(en_emb.shape)\n# cada token de la frase en inglÃ©s\n# serÃ¡ operados con todos los tokens de\n# la oraciÃ³n en portuguÃ©s, esto pasa asÃ­\n# solo en entrenamiento\nprint(sample_ca(en_emb, pt_emb).shape)\n\n(64, 64, 512)\n(64, 64, 512)\n(64, 64, 512)\n\n\n\n\nCapa global de auto-atenciÃ³n\nEsta capa se encarga de procesar la secuencia contextual y de propagar la informaciÃ³n a lo largo de la misma:\n\n\n\nLa capa global de auto-atenciÃ³n\n\n\n\n\n\n\n\n\nDado que la secuencia contextual es fija mientras se genera la traducciÃ³n (entrenamiento), se permite que la informaciÃ³n fluya en ambas direcciones.\nAntes de los Transformers y la autoatenciÃ³n, los modelos solÃ­an utilizar RNNs o CNNs para realizar esta tarea (bidireccionalidad):\n\n\n\nRNNs y CNNs bidireccionales\n\n\n\n\n\n\n\n\n\n\n\n\n\nRNNs and CNNs have their limitations.\nLas RNNs y las CNNs tienen sus limitaciones:\n\nLa RNN permite que la informaciÃ³n fluya a lo largo de toda la secuencia, pero atraviesa muchos pasos de procesamiento para llegar allÃ­ (limitando el flujo del gradiente). Estos pasos de la RNN deben ejecutarse secuencialmente, por lo que la RNN es menos capaz de aprovechar los dispositivos paralelos modernos.\nEn la CNN, cada ubicaciÃ³n puede procesarse en paralelo, pero solo proporciona un campo receptivo limitado. El campo receptivo solo crece linealmente con el nÃºmero de capas CNN. Se necesita apilar varias capas convolucionales para transmitir informaciÃ³n a travÃ©s de la secuencia.\n\nPor otro lado, la capa de auto-atenciÃ³n global permite que cada elemento de la secuencia acceda directamente a todos los demÃ¡s elementos de la secuencia, con solo unas pocas operaciones, y todas las salidas se pueden calcular en paralelo.\nPara implementar esta capa, solo necesitas pasar la secuencia objetivo, x, como los argumentos query (consulta) y value (valor) a la capa mha:\n\nclass GlobalSelfAttention(BaseAttention):\n  def call(self, x):\n    attn_output = self.mha(\n        query=x,\n        value=x,\n        key=x)\n    x = self.add([x, attn_output])\n    x = self.layernorm(x)\n    return x\n\n\nsample_gsa = GlobalSelfAttention(num_heads=2, key_dim=512)\n\nprint(pt_emb.shape)\nprint(sample_gsa(pt_emb).shape)\n\n(64, 64, 512)\n(64, 64, 512)\n\n\nSiguiendo con el mismo estilo de antes, podriamos dibujarlo asÃ­:\n\n\n\nLa capa global de auto-atenciÃ³n\n\n\n\n\n\n\n\n\nDe nuevo, las conexiones residuales se omiten para mayor claridad.\nEs mÃ¡s compacto e igual de preciso dibujarlo asÃ­:\n\n\n\nLa capa de auto-atenciÃ³n global\n\n\n\n\n\n\n\n\n\n\nCapa de auto-atenciÃ³n causal\nEsta capa realiza un trabajo similar al de la capa de autoatenciÃ³n global, para la secuencia de salida:\n\n\n\nCapa causal de auto-atenciÃ³n\n\n\n\n\n\n\n\n\nThis needs to be handled differently from the encoderâ€™s global self-attention layer.\nLike the text generation tutorial, and the NMT with attention tutorial, Transformers are an â€œautoregressiveâ€ model: They generate the text one token at a time and feed that output back to the input. To make this efficient, these models ensure that the output for each sequence element only depends on the previous sequence elements; the models are â€œcausalâ€.\nUna RNN unidireccional es causal por definiciÃ³n. Para hacer una convoluciÃ³n causal, solo necesitas rellenar (pad) la entrada y desplazar la salida para que se alinee correctamente (puedes usar layers.Conv1D(padding='causal')).\n\n\n\nCausalidad en RNNs y CNNs\n\n\n\n\n\n\n\n\n\n\n\n\n\nUn modelo causal es eficiente de dos maneras:\n\nEn el entrenamiento, te permite calcular la pÃ©rdida para cada posiciÃ³n en la secuencia de salida ejecutando el modelo una sola vez.\nDurante la inferencia, para cada nuevo token generado, solo necesitas calcular sus salidas; las salidas de los elementos de la secuencia anterior se pueden reutilizar.\n\nPara una RNN, solo necesitas el estado de la RNN para tener en cuenta los cÃ¡lculos previos (pasa return_state=True al constructor de la capa RNN).\nPara una CNN, necesitarÃ­as seguir el enfoque de Fast Wavenet.\n\n\nPara construir una capa de auto-atenciÃ³n causal, necesitas usar una mÃ¡scara apropiada al calcular las puntuaciones de atenciÃ³n y sumar los values de atenciÃ³n.\nEsto se gestiona automÃ¡ticamente si pasas use_causal_mask = True a la capa MultiHeadAttention cuando la llamas:\n\nclass CausalSelfAttention(BaseAttention):\n  def call(self, x):\n    attn_output = self.mha(\n        query=x,\n        value=x,\n        key=x,\n        use_causal_mask = True)\n    x = self.add([x, attn_output])\n    x = self.layernorm(x)\n    return x\n\nLa mÃ¡scara causal garantiza que cada lugar sÃ³lo tenga acceso a los lugares que le preceden:\n\n\n\nCapa de auto-atenciÃ³n causal.\n\n\n\n\n\n\n\n\nDe nuevo, las conexiones residuales se omiten por simplicidad.\nLa representaciÃ³n mÃ¡s compacta de esta capa serÃ­a:\n\n\n\nCapa de auto-atenciÃ³n causal.\n\n\n\n\n\n\n\n\nEjemplo:\n\nsample_csa = CausalSelfAttention(num_heads=2, key_dim=512)\n\nprint(en_emb.shape)\nprint(sample_csa(en_emb).shape)\n\n(64, 64, 512)\n(64, 64, 512)\n\n\nLa salida de los primeros elementos de la secuencia no depende de los elementos posteriores, por lo que no deberÃ­a importar si recorta los elementos antes o despuÃ©s de aplicar la capa:\n\n# ejemplo donde la sÃ¡lida debe ser cercana a cero\n# ya que no deberia influir informaciÃ³n posterior\nout1 = sample_csa(embed_en(en[:, :3]))\nout2 = sample_csa(embed_en(en))[:, :3]\n\ntf.reduce_max(abs(out1 - out2)).numpy()\n# nota omitir el warning por ahora\n\nnp.float32(7.1525574e-07)\n\n\nNota: Cuando se utilizan mÃ¡scaras Keras, los valores de salida en lugares no vÃ¡lidos no estÃ¡n bien definidos. Por lo tanto, lo anterior puede no ser vÃ¡lido para las regiones enmascaradas.\n\n\n\nLa red feed forward (MLP)\nEl Transformer tambiÃ©n incluye esta red neuronal feed-forward point-wise tanto en el codificador como en el decodificador:\n\n\n\nLa red feed forward (MLP)\n\n\n\n\n\n\n\n\nLa red consiste en dos capas lineales (tf.keras.layers.Dense) con una funciÃ³n de activaciÃ³n ReLU entre ellas, y una capa de dropout. Al igual que con las capas de atenciÃ³n, el cÃ³digo aquÃ­ tambiÃ©n incluye la conexiÃ³n residual y la normalizaciÃ³n:\n\nclass FeedForward(tf.keras.layers.Layer):\n  def __init__(self, d_model, dff, dropout_rate=0.1):\n    super().__init__()\n    self.seq = tf.keras.Sequential([\n      tf.keras.layers.Dense(dff, activation='relu'),\n      tf.keras.layers.Dense(d_model),\n      tf.keras.layers.Dropout(dropout_rate)\n    ])\n    self.add = tf.keras.layers.Add()\n    self.layer_norm = tf.keras.layers.LayerNormalization()\n\n  def call(self, x):\n    x = self.add([x, self.seq(x)])\n    x = self.layer_norm(x)\n    return x\n\nPrueba:\n\nsample_ffn = FeedForward(512, 2048)\n\nprint(en_emb.shape)\nprint(sample_ffn(en_emb).shape)\n\n(64, 64, 512)\n(64, 64, 512)\n\n\n\n\nLa capa encoder\nEl codificador contiene una pila de N capas de codificador. Donde cada EncoderLayer contiene una capa GlobalSelfAttention y una capa FeedForward:\n\n\n\nLa capa encoder\n\n\n\n\n\n\n\n\nAquÃ­ la estructura de la capa EncoderLayer:\n\nclass EncoderLayer(tf.keras.layers.Layer):\n  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n    super().__init__()\n\n    self.self_attention = GlobalSelfAttention(\n        num_heads=num_heads,\n        key_dim=d_model,\n        dropout=dropout_rate)\n\n    self.ffn = FeedForward(d_model, dff)\n\n  def call(self, x):\n    x = self.self_attention(x)\n    x = self.ffn(x)\n    return x\n\nPrueba:\n\nsample_encoder_layer = EncoderLayer(d_model=512, num_heads=8, dff=2048)\n\nprint(pt_emb.shape)\nprint(sample_encoder_layer(pt_emb).shape)\n\n(64, 64, 512)\n(64, 64, 512)\n\n\n\n\nEl mÃ³dulo Encoder\nConstruyamos el encoder, agregandole la parte de embedings y la codificaciÃ³n posicional.\n\n\n\nEl encoder\n\n\n\n\n\n\n\n\nEl codificador consiste en:\n\nUna capa PositionalEmbedding en la entrada.\nUna pila de capas EncoderLayer.\n\n\nclass Encoder(tf.keras.layers.Layer):\n  def __init__(self, *, num_layers, d_model, num_heads,\n               dff, vocab_size, dropout_rate=0.1):\n    super().__init__()\n\n    self.d_model = d_model\n    self.num_layers = num_layers\n\n    self.pos_embedding = PositionalEmbedding(\n        vocab_size=vocab_size, d_model=d_model)\n\n    self.enc_layers = [\n        EncoderLayer(d_model=d_model,\n                     num_heads=num_heads,\n                     dff=dff,\n                     dropout_rate=dropout_rate)\n        for _ in range(num_layers)]\n    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n\n  def call(self, x):\n    # `x` es token-IDs shape: (batch, seq_len)\n    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n\n    # aÃ±adir dropout.\n    x = self.dropout(x)\n\n    for i in range(self.num_layers):\n      x = self.enc_layers[i](x)\n\n    return x  # Shape `(batch_size, seq_len, d_model)`.\n\nProbar:\n\n# Instanciar el Encoder.\nsample_encoder = Encoder(num_layers=4,\n                         d_model=512,\n                         num_heads=8,\n                         dff=2048,\n                         vocab_size=8500)\n# Fijar training en false\nsample_encoder_output = sample_encoder(pt, training=False)\n\n# Dimensiones\nprint(pt.shape)\nprint(sample_encoder_output.shape)  # Shape `(batch_size, input_seq_len, d_model)`.\n# ignorar los warnings\n\n(64, 64)\n(64, 64, 512)\n\n\n\n\nLa capa decoder\nEl decodificador es ligeramente mÃ¡s compleja, con cada DecoderLayer conteniendo una capa CausalSelfAttention, una capa CrossAttention y una capa FeedForward:\n\n\n\nLa capa decoder\n\n\n\n\n\n\n\n\n\nclass DecoderLayer(tf.keras.layers.Layer):\n  def __init__(self,\n               *,\n               d_model,\n               num_heads,\n               dff,\n               dropout_rate=0.1):\n    super(DecoderLayer, self).__init__()\n\n    self.causal_self_attention = CausalSelfAttention(\n        num_heads=num_heads,\n        key_dim=d_model,\n        dropout=dropout_rate)\n\n    self.cross_attention = CrossAttention(\n        num_heads=num_heads,\n        key_dim=d_model,\n        dropout=dropout_rate)\n\n    self.ffn = FeedForward(d_model, dff)\n\n  def call(self, x, context):\n    x = self.causal_self_attention(x=x)\n    x = self.cross_attention(x=x, context=context)\n\n    # Solo para efectos de visualizaciÃ³n posterior\n    self.last_attn_scores = self.cross_attention.last_attn_scores\n\n    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n    return x\n\nProbar la capa del decoder:\n\nsample_decoder_layer = DecoderLayer(d_model=512, num_heads=8, dff=2048)\n\nsample_decoder_layer_output = sample_decoder_layer(\n    x=en_emb, context=pt_emb)\n\nprint(en_emb.shape)\nprint(pt_emb.shape)\nprint(sample_decoder_layer_output.shape)  # `(batch_size, seq_len, d_model)`\n\n(64, 64, 512)\n(64, 64, 512)\n(64, 64, 512)\n\n\n\n\nEl mÃ³dulo decoder\nDe forma similar al Codificador, el Decodificador consiste en un PositionalEmbedding y una pila de DecoderLayerâ€™s:\n\n\n\nCapa Decoder + Embedding + PE\n\n\n\n\n\n\n\n\nDefinimos el Decoder extendiendo tf.keras.layers.Layer:\n\nclass Decoder(tf.keras.layers.Layer):\n  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n               dropout_rate=0.1):\n    super(Decoder, self).__init__()\n\n    self.d_model = d_model\n    self.num_layers = num_layers\n\n    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n                                             d_model=d_model)\n    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n    self.dec_layers = [\n        DecoderLayer(d_model=d_model, num_heads=num_heads,\n                     dff=dff, dropout_rate=dropout_rate)\n        for _ in range(num_layers)]\n\n    self.last_attn_scores = None\n\n  def call(self, x, context):\n    # `x` es token-IDs shape (batch, target_seq_len)\n    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n\n    x = self.dropout(x)\n\n    for i in range(self.num_layers):\n      x  = self.dec_layers[i](x, context)\n\n    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n\n    # EL shape de x es (batch_size, target_seq_len, d_model).\n    return x\n\nProbar:\n\n# Instanciar el decoder\nsample_decoder = Decoder(num_layers=4,\n                         d_model=512,\n                         num_heads=8,\n                         dff=2048,\n                         vocab_size=8000)\n\noutput = sample_decoder(\n    x=en,\n    context=pt_emb)\n\n# Shapes.\nprint(en.shape)\nprint(pt_emb.shape)\nprint(output.shape)\n\n(64, 64)\n(64, 64, 512)\n(64, 64, 512)\n\n\n\nsample_decoder.last_attn_scores.shape  # (batch, heads, target_seq, input_seq)\n\nTensorShape([64, 8, 64, 64])\n\n\nUna vez creados el codificador y el decodificador Transformer, es hora de construir el modelo Transformer y entrenarlo."
  },
  {
    "objectID": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#el-transformer",
    "href": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#el-transformer",
    "title": "Explicando la Mejora de los Transformers sobre las RNNs",
    "section": "El Transformer",
    "text": "El Transformer\nYou now have Encoder and Decoder. To complete the Transformer model, you need to put them together and add a final linear (Dense) layer which converts the resulting vector at each location into output token probabilities.\nThe output of the decoder is the input to this final linear layer.\n\n\n\nEl transformer\n\n\n\n\n\n\n\n\nUn Transformer con una capa tanto en el Codificador como en el Decodificador se parece casi exactamente al modelo del tutorial de RNN+atenciÃ³n. Un Transformer multi-capa tiene mÃ¡s capas, pero fundamentalmente estÃ¡ haciendo lo mismo.\n\n\n\nTransformer de una capa\n\n\nTransformer de 4 capas\n\n\n\n\n\n\n\n\n\n\n\n\nRNN + Modelo de atenciÃ³n\n\n\n\n\n\n\n\n\nCrea el Transformer extendiendo tf.keras.Model:\n\nNota: El artÃ­culo original, secciÃ³n 3.4, comparte la matriz de pesos entre la capa de embedding y la capa lineal final. Para mantener las cosas simples, este tutorial utiliza dos matrices de pesos separadas.\n\n\nclass Transformer(tf.keras.Model):\n  def __init__(self, *, num_layers, d_model, num_heads, dff,\n               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n    super().__init__()\n    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n                           num_heads=num_heads, dff=dff,\n                           vocab_size=input_vocab_size,\n                           dropout_rate=dropout_rate)\n\n    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n                           num_heads=num_heads, dff=dff,\n                           vocab_size=target_vocab_size,\n                           dropout_rate=dropout_rate)\n\n    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n\n  def call(self, inputs):\n    # Para usar el `.fit` del modelo keras usted debe pasar\n    # todas las inputs como el primer argumento\n    context, x  = inputs\n\n    context = self.encoder(context)  # (batch_size, context_len, d_model)\n\n    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n\n    # Capa final densa lineal\n    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n\n    try:\n      # Eliminar las mÃ¡scaras para que no afecten el cÃ¡lculo del loss y mÃ©tricas\n      # b/250038731 --&gt; relacionado con este bug que fue reportado\n      del logits._keras_mask\n    except AttributeError:\n      pass\n\n    # retornar la salida final\n    return logits\n\n\nHiperparÃ¡metros\nPara mantener este ejemplo pequeÃ±o y relativamente rÃ¡pido, el nÃºmero de capas (num_layers), la dimensionalidad de los embeddings (d_model) y la dimensionalidad interna de la capa FeedForward (dff) se han reducido.\nEl modelo base descrito en el artÃ­culo original del Transformer utilizaba num_layers=6, d_model=512 y dff=2048.\nEl nÃºmero de cabezas de auto-atenciÃ³n serÃ¡ (num_heads=4).\n\nnum_layers = 2\nd_model = 128\ndff = 256\nnum_heads = 4\ndropout_rate = 0.1\n\n\n\nProbemos el transformer\nInstanciar el modelo Transformer:\n\ntransformer = Transformer(\n    num_layers=num_layers,\n    d_model=d_model,\n    num_heads=num_heads,\n    dff=dff,\n    input_vocab_size=tokenizers.pt.get_vocab_size().numpy(),\n    target_vocab_size=tokenizers.en.get_vocab_size().numpy(),\n    dropout_rate=dropout_rate)\n\nProbar\n\noutput = transformer((pt, en))\n\nprint(en.shape)\nprint(pt.shape)\nprint(output.shape)\n\n(64, 64)\n(64, 64)\n(64, 64, 7010)\n\n\n\n# acceder a los scores de atenciÃ³n\nattn_scores = transformer.decoder.dec_layers[-1].last_attn_scores\nprint(attn_scores.shape)  # (batch, heads, target_seq, input_seq)\n\n(64, 4, 64, 64)"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#training",
    "href": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#training",
    "title": "Explicando la Mejora de los Transformers sobre las RNNs",
    "section": "Training",
    "text": "Training\nTiempo de entrenar!!\n\nConfigurar el optimizador\nUtilizar el optimizador Adam con un planificador personalizado para la tasa de aprendizaje segÃºn la fÃ³rmula del Transformer original. paper.\n\\[\\Large{lrate = d_{model}^{-0.5} * \\min(step{\\_}num^{-0.5}, step{\\_}num \\cdot warmup{\\_}steps^{-1.5})}\\]\n\n\nExplicaciÃ³n esquema de Learning Rate del Transformer\nParÃ¡metros:\n\nd_model (Dimensionalidad del Modelo):\n\nIntuiciÃ³n: â€œAnchoâ€ del modelo. Mayor d_model = representaciones mÃ¡s ricas, mÃ¡s capacidad.\nEfecto: ActÃºa como un factor de escala base para el learning rate.\n\nSi d_model es grande, este factor serÃ¡ pequeÃ±o, lo que tiende a reducir el learning rate general. La intuiciÃ³n es que modelos mÃ¡s grandes pueden necesitar pasos de aprendizaje mÃ¡s pequeÃ±os para evitar inestabilidad debido a su mayor nÃºmero de parÃ¡metros.\nSi d_model es pequeÃ±o, este factor serÃ¡ mÃ¡s grande, lo que tiende a aumentar el learning rate general.\n\n\nstep_num (NÃºmero de Paso de Entrenamiento):\n\nIntuiciÃ³n: Progreso del entrenamiento (iteraciÃ³n actual).\nEfecto: Determina la fase del learning rate:\n\nFase de Decaimiento (\\({step\\\\_num}^{-0.5}\\)): Este tÃ©rmino hace que el learning rate disminuya a medida que avanza el entrenamiento. La intuiciÃ³n es que al principio queremos dar pasos de aprendizaje mÃ¡s grandes para explorar el espacio de parÃ¡metros rÃ¡pidamente. A medida que nos acercamos a un buen conjunto de pesos, queremos dar pasos mÃ¡s pequeÃ±os para â€œafinarâ€ los valores y evitar oscilar alrededor del mÃ­nimo Ã³ptimo. La disminuciÃ³n es mÃ¡s pronunciada al principio y se ralentiza con el tiempo.\nFase de Calentamiento (\\({step\\\\_num * warmup\\\\_steps}^{-1.5}\\)): Este tÃ©rmino estÃ¡ activo principalmente durante la fase de â€œcalentamientoâ€. Hace que el learning rate aumente linealmente con el nÃºmero de paso hasta que step_num se acerca a warmup_steps.\n\n\nwarmup_steps (Pasos de Calentamiento):\n\nIntuiciÃ³n: DuraciÃ³n de la fase inicial de aumento gradual del learning rate.\nEfecto: Controla cuÃ¡ntos pasos se tarda en alcanzar el learning rate â€œbaseâ€. Ayuda a evitar inestabilidad al inicio.\n\n\nEn resumen:\nEl learning rate:\n\nAumenta gradualmente durante los primeros warmup_steps para estabilizar el entrenamiento inicial.\nDisminuye gradualmente despuÃ©s de warmup_steps para permitir una convergencia fina.\nSu magnitud general estÃ¡ influenciada por la dimensionalidad del modelo (d_model).\n\n\nclass CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n  def __init__(self, d_model, warmup_steps=4000):\n    super().__init__()\n\n    self.d_model = d_model\n    self.d_model = tf.cast(self.d_model, tf.float32)\n\n    self.warmup_steps = warmup_steps\n\n  def __call__(self, step):\n    step = tf.cast(step, dtype=tf.float32)\n    arg1 = tf.math.rsqrt(step)\n    arg2 = step * (self.warmup_steps ** -1.5)\n\n    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n\nInstanciar el optimizador (tf.keras.optimizers.Adam):\n\nlearning_rate = CustomSchedule(d_model)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n                                     epsilon=1e-9)\n\nProbar el scheduler para el learning rate:\n\nplot_lr_planificador(learning_rate)\n\n\n\n\n\n\n\n\n\n\nAjustar funciÃ³n de pÃ©rdida y mÃ©tricas\nDado que las secuencias objetivo estÃ¡n rellenadas (padded), es importante aplicar una mÃ¡scara de padding al calcular la pÃ©rdida. Utiliza la funciÃ³n de pÃ©rdida de entropÃ­a cruzada (tf.keras.losses.SparseCategoricalCrossentropy):\n\ndef masked_loss(label, pred):\n  mask = label != 0 # Indica padding\n  # from_logits=True indica que las predicciones no han pasado por softmax.\n  # reduction='none' hace que se devuelva la pÃ©rdida por cada ejemplo.\n  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n      from_logits=True, reduction='none')\n\n  loss = loss_object(label, pred)\n  mask = tf.cast(mask, dtype=loss.dtype)\n  # Aplica la mÃ¡scara a la pÃ©rdida, multiplicando las pÃ©rdidas de las posiciones\n  # de padding por cero, lo que las elimina del cÃ¡lculo total.\n  loss *= mask\n\n  # Calcula la pÃ©rdida promedio solo sobre las posiciones no enmascaradas.\n  # Suma todas las pÃ©rdidas y divide por el nÃºmero de posiciones no enmascaradas.\n  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n  return loss\n\n\ndef masked_accuracy(label, pred):\n  # Obtiene la clase predicha con la probabilidad mÃ¡s alta (el Ã­ndice mÃ¡ximo)\n  # a lo largo del eje de vocabulario (axis=2).\n  pred = tf.argmax(pred, axis=2)\n  # Convierte las etiquetas al mismo tipo de dato que las predicciones para comparar.\n  label = tf.cast(label, pred.dtype)\n  # Crea un tensor booleano donde True indica que la predicciÃ³n coincide con la etiqueta.\n  match = label == pred\n\n  # Crea una mÃ¡scara donde True indica que la etiqueta no es padding (no es 0).\n  mask = label != 0\n\n  # Combina la mÃ¡scara con las coincidencias. Solo consideramos como \"match\"\n  # las predicciones correctas en las posiciones que no son padding.\n  match = match & mask\n\n  # Convierte los booleanos de 'match' y 'mask' a float para poder calcular la media.\n  match = tf.cast(match, dtype=tf.float32)\n  mask = tf.cast(mask, dtype=tf.float32)\n  # Calcula la precisiÃ³n promedio solo sobre las posiciones no enmascaradas.\n  # Suma las coincidencias (1 para cada predicciÃ³n correcta no enmascarada)\n  # y divide por el nÃºmero total de posiciones no enmascaradas.\n  return tf.reduce_sum(match)/tf.reduce_sum(mask)\n\n\n\nEntrenar el modelo\nCon todo listo, vamos a compilar usando model.compile, y luego entrenar con model.fit:\n\ntransformer.compile(\n    loss=masked_loss,\n    optimizer=optimizer,\n    metrics=[masked_accuracy])\n\n\ntransformer.fit(train_batches,\n                epochs=3,\n                validation_data=val_batches)\n\n\nEpoch 1/3\n\n810/810 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1037s 1s/step - loss: 7.7200 - masked_accuracy: 0.0833 - val_loss: 4.9830 - val_masked_accuracy: 0.2540\n\nEpoch 2/3\n\n810/810 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 531s 633ms/step - loss: 4.6781 - masked_accuracy: 0.2912 - val_loss: 3.9608 - val_masked_accuracy: 0.3668\n\nEpoch 3/3\n\n810/810 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 424s 463ms/step - loss: 3.6996 - masked_accuracy: 0.4002 - val_loss: 3.4028 - val_masked_accuracy: 0.4305\n\n\n\n\n&lt;keras.src.callbacks.history.History at 0x7bffd0607690&gt;"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#ejecutar-inferencia",
    "href": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#ejecutar-inferencia",
    "title": "Explicando la Mejora de los Transformers sobre las RNNs",
    "section": "Ejecutar inferencia",
    "text": "Ejecutar inferencia\nAhora puedes probar el modelo realizando una traducciÃ³n. Los siguientes pasos se utilizan para la inferencia:\n\nCodifica la frase de entrada utilizando el tokenizador portuguÃ©s (tokenizers.pt). Esta es la entrada del codificador.\nLa entrada del decodificador se inicializa con el token [START].\nCalcula las mÃ¡scaras de padding y las mÃ¡scaras causales (para la auto-atenciÃ³n causal).\nEl decodificador luego genera las predicciones observando la salida del codificador y su propia salida (auto-atenciÃ³n).\nConcatena el token predicho a la entrada del decodificador y lo pasa de nuevo al decodificador.\nEn este enfoque, el decodificador predice el siguiente token basÃ¡ndose en los tokens que predijo previamente.\n\nNota: El modelo estÃ¡ optimizado para un entrenamiento eficiente y realiza una predicciÃ³n del siguiente token para cada token en la salida simultÃ¡neamente. Esto es redundante durante la inferencia, y solo se utiliza la Ãºltima predicciÃ³n. Este modelo puede hacerse mÃ¡s eficiente para la inferencia si solo se calcula la Ãºltima predicciÃ³n cuando se ejecuta en modo de inferencia (training=False).\nDefine la clase Translator extendiendo tf.Module:\n\nclass Translator(tf.Module):\n  def __init__(self, tokenizers, transformer):\n    self.tokenizers = tokenizers\n    self.transformer = transformer\n\n  def __call__(self, sentence, max_length=MAX_TOKENS):\n    # La frase de entrada es portuguÃ©s, por lo que se aÃ±aden los tokens `[START]` y `[END]`.\n    assert isinstance(sentence, tf.Tensor)\n    if len(sentence.shape) == 0:\n      sentence = sentence[tf.newaxis]\n\n    sentence = self.tokenizers.pt.tokenize(sentence).to_tensor()\n\n    encoder_input = sentence\n\n    # Como el lenguaje de sÃ¡lida es inglÃ©s\n    # Inicializar con el token `[START]`\n    start_end = self.tokenizers.en.tokenize([''])[0]\n    start = start_end[0][tf.newaxis]\n    end = start_end[1][tf.newaxis]\n\n    # con el TensorArray es posible hacer seguimiento al blucle dinÃ¡mico\n    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n    output_array = output_array.write(0, start)\n\n    for i in tf.range(max_length):\n      # convierte la lista de tokens generados secuencialmente\n      # (almacenada en el TensorArray) en un tensor con la forma (1, secuencia_de_tokens),\n      # donde la secuencia de tokens representa la traducciÃ³n generada hasta ese punto.\n      output = tf.transpose(output_array.stack())\n      predictions = self.transformer([encoder_input, output], training=False)\n\n      # Seleccionar las predicciones para el Ãºltimo token de `seq_len`.\n      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n      # Encontrar la pos del token id con la mayor probabilidad\n      predicted_id = tf.argmax(predictions, axis=-1)\n\n      # Concatenar la predicciÃ³n con los anteriores tokens del decoder.\n      output_array = output_array.write(i+1, predicted_id[0])\n\n      if predicted_id == end:\n        break\n\n    output = tf.transpose(output_array.stack())\n    # La dimensiÃ³n de sÃ¡lida es `(1, tokens)`.\n    text = tokenizers.en.detokenize(output)[0]  # Shape: `()`.\n\n    tokens = tokenizers.en.lookup(output)[0]\n\n    # `@tf.function` optimiza la funciÃ³n, dificultando el acceso\n    # a valores dinÃ¡micos en bucles. Recalculamos la atenciÃ³n\n    # final fuera del bucle para obtener `attention_weights`.\n    self.transformer([encoder_input, output[:,:-1]], training=False)\n    attention_weights = self.transformer.decoder.last_attn_scores\n\n    return text, tokens, attention_weights\n\nNota: Esta funciÃ³n utiliza un bucle unrolled, no un bucle dinÃ¡mico. Genera MAX_TOKENS en cada llamada. Consulta el tutorial de NMT con atenciÃ³n para ver un ejemplo de implementaciÃ³n con un bucle dinÃ¡mico, que puede ser mucho mÃ¡s eficiente.\nProbemos la traducciÃ³n:\n\ntranslator = Translator(tokenizers, transformer)\n\nEjemplo 1:\n\nsentence = 'este Ã© um problema que temos que resolver.'\nground_truth = 'this is a problem we have to solve .'\n\ntranslated_text, translated_tokens, attention_weights = translator(\n    tf.constant(sentence))\nplot_traduccion(sentence, translated_text, ground_truth)\n\nInput:         : este Ã© um problema que temos que resolver.\nPredicciÃ³n     : this is a problem that we have to solve .\nGround truth   : this is a problem we have to solve .\n\n\nEjemplo 2:\n\nsentence = 'os meus vizinhos ouviram sobre esta ideia.'\nground_truth = 'and my neighboring homes heard about this idea .'\n\ntranslated_text, translated_tokens, attention_weights = translator(\n    tf.constant(sentence))\nplot_traduccion(sentence, translated_text, ground_truth)\n\nInput:         : os meus vizinhos ouviram sobre esta ideia.\nPredicciÃ³n     : my friends heard about this idea .\nGround truth   : and my neighboring homes heard about this idea .\n\n\nEjemplo 3:\n\nsentence = 'vou entÃ£o muito rapidamente partilhar convosco algumas histÃ³rias de algumas coisas mÃ¡gicas que aconteceram.'\nground_truth = \"so i'll just share with you some stories very quickly of some magical things that have happened.\"\n\ntranslated_text, translated_tokens, attention_weights = translator(\n    tf.constant(sentence))\nplot_traduccion(sentence, translated_text, ground_truth)\n\nInput:         : vou entÃ£o muito rapidamente partilhar convosco algumas histÃ³rias de algumas coisas mÃ¡gicas que aconteceram.\nPredicciÃ³n     : i ' m going very quickly quickly quickly to share with some stories of things , things that will be going to be going to be going to be going to be going to be going to be able to be able to be able to be very good .\nGround truth   : so i'll just share with you some stories very quickly of some magical things that have happened."
  },
  {
    "objectID": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#crear-los-plots-de-atenciÃ³n",
    "href": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#crear-los-plots-de-atenciÃ³n",
    "title": "Explicando la Mejora de los Transformers sobre las RNNs",
    "section": "Crear los plots de atenciÃ³n",
    "text": "Crear los plots de atenciÃ³n\nUsando la clase traductor Translator que almacena los scores de atenciÃ³n, podemos usarlos para ver su relevancia:\nPor ejemplo:\n\nsentence = 'este Ã© o primeiro livro que eu fiz.'\nground_truth = \"this is the first book i've ever done.\"\n\ntranslated_text, translated_tokens, attention_weights = translator(\n    tf.constant(sentence))\nplot_traduccion(sentence, translated_text, ground_truth)\n\nInput:         : este Ã© o primeiro livro que eu fiz.\nPredicciÃ³n     : this is the first book that i did i did .\nGround truth   : this is the first book i've ever done.\n\n\nCrear una funciÃ³n que grafique la atenciÃ³n cuando se genera un token:\n\nhead = 0\n# Shape: `(batch=1, num_heads, seq_len_q, seq_len_k)`.\nattention_heads = tf.squeeze(attention_weights, 0)\nattention = attention_heads[head]\nattention.shape\n\nTensorShape([12, 11])\n\n\nSon las inputs tokens en portuguÃ©s:\n\nin_tokens = tf.convert_to_tensor([sentence])\nin_tokens = tokenizers.pt.tokenize(in_tokens).to_tensor()\nin_tokens = tokenizers.pt.lookup(in_tokens)[0]\nin_tokens\n\n&lt;tf.Tensor: shape=(11,), dtype=string, numpy=\narray([b'[START]', b'este', b'e', b'o', b'primeiro', b'livro', b'que',\n       b'eu', b'fiz', b'.', b'[END]'], dtype=object)&gt;\n\n\nEstas son las sÃ¡lidas (tokens en inglÃ©s, traducciÃ³n)\n\ntranslated_tokens\n\n&lt;tf.Tensor: shape=(13,), dtype=string, numpy=\narray([b'[START]', b'this', b'is', b'the', b'first', b'book', b'that',\n       b'i', b'did', b'i', b'did', b'.', b'[END]'], dtype=object)&gt;\n\n\n\nplot_attention_weights(sentence,\n                       translated_tokens,\n                       attention_weights[0])"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#exportar-el-modelo",
    "href": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#exportar-el-modelo",
    "title": "Explicando la Mejora de los Transformers sobre las RNNs",
    "section": "Exportar el modelo",
    "text": "Exportar el modelo\nHemos probado el modelo y la inferencia funciona. A continuaciÃ³n, puedes exportarlo como un tf.saved_model. Para aprender cÃ³mo guardar y cargar un modelo en formato SavedModel, consulta esta guÃ­a.\nCrea una clase llamada ExportTranslator extendiendo la subclase tf.Module con un @tf.function en el mÃ©todo __call__:\n\nclass ExportTranslator(tf.Module):\n  def __init__(self, translator):\n    self.translator = translator\n\n  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n  def __call__(self, sentence):\n    (result,\n     tokens,\n     attention_weights) = self.translator(sentence, max_length=MAX_TOKENS)\n\n    return result\n\n\n# Empaqueta el objeto `translator` en la nueva clase `ExportTranslator` creada:\ntranslator = ExportTranslator(translator)\n\nDado que el modelo estÃ¡ decodificando las predicciones utilizando tf.argmax, las predicciones son deterministas. El modelo original y uno recargado desde su SavedModel deberÃ­an dar predicciones idÃ©nticas:\n\ntranslator('este Ã© o primeiro livro que eu fiz.').numpy()\n\nb'this is the first book that i did i did .'\n\n\n\ntf.saved_model.save(translator, export_dir='translator')\n\n\nreloaded = tf.saved_model.load('translator')\n\n\nreloaded('este Ã© o primeiro livro que eu fiz.').numpy()\n\nb'this is the first book that i did i did .'"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#conclusiÃ³n",
    "href": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#conclusiÃ³n",
    "title": "Explicando la Mejora de los Transformers sobre las RNNs",
    "section": "ConclusiÃ³n",
    "text": "ConclusiÃ³n\nEn este tutorial aprendiste sobre:\n\nLos Transformers y su importancia en el aprendizaje automÃ¡tico\nAtenciÃ³n, auto-atenciÃ³n y atenciÃ³n multi-cabeza\nCodificaciÃ³n posicional con embeddings\nLa arquitectura codificador-decodificador del Transformer original\nEnmascaramiento en la auto-atenciÃ³n\nCÃ³mo juntar todo para traducir texto\n\nLas desventajas de esta arquitectura son:\n\nPara una serie temporal, la salida para un paso de tiempo se calcula a partir de la historia completa en lugar de solo las entradas y el estado oculto actual. Esto podrÃ­a ser menos eficiente.\nSi la entrada tiene una relaciÃ³n temporal/espacial, como texto o imÃ¡genes, debe aÃ±adirse alguna codificaciÃ³n posicional o el modelo efectivamente verÃ¡ una bolsa de palabras.\n\nEste notebook se basÃ³ en el notebook de Neural Machine Translation with a Transformer and Keras para el curso de Deep Learning prÃ¡ctico en 3 semanas."
  },
  {
    "objectID": "semana_3/notebooks/Nb_3a_Introduccion_transfer_learning_finetuning.html",
    "href": "semana_3/notebooks/Nb_3a_Introduccion_transfer_learning_finetuning.html",
    "title": "IntroducciÃ³n a pre-trained models, transfer learning and fine tuning",
    "section": "",
    "text": "Ãšltima actualizaciÃ³n 04/08/2025\n#@title Eliminar librerÃ­as (Problema de compatibilidad reciente con numpy )\nfrom IPython.display import clear_output\n\n!pip uninstall  -y numpy\n!pip -q install numpy==1.26.4\n\nclear_output()\n#@title Ejecutar esta celda para reiniciar el entorno de ejecuciÃ³n\nimport os\nos.kill(os.getpid(), 9)\n#@title Instalar librerÃ­as necesarias\n\n!pip -q install keras-nlp\n!pip -q install gensim\n!pip -q install pytorch-tabnet\n\n\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n\nimport keras_hub\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom tensorflow.keras import layers, models, optimizers, callbacks\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.utils import to_categorical\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import backend as K\nfrom gensim.models import KeyedVectors\nfrom IPython.display import clear_output\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import fetch_covtype\nfrom pytorch_tabnet.tab_model import TabNetClassifier\nimport torch\n\nclear_output()\n\nprint(\"Se han importado todas las librerÃ­as correctamente\")\n\nSe han importado todas las librerÃ­as correctamente\n#@title Definir funciones complementarias\ndef plot_graphs(history, metric):\n  plt.plot(history.history[metric])\n  plt.plot(history.history['val_'+metric], '')\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(metric)\n  plt.legend([metric, 'val_'+metric])\n\ndef plot_curvas(history):\n  plt.figure(figsize=(12, 6))\n  plt.subplot(1, 2, 1)\n  plot_graphs(history, 'accuracy')\n  plt.ylim(None, 1)\n  plt.subplot(1, 2, 2)\n  plot_graphs(history, 'loss')\n  plt.ylim(0, None)\n\ndef cosine_similarity(vec_a, vec_b):\n  \"\"\"Compute cosine similarity between vec_a and vec_b\"\"\"\n  return np.dot(vec_a, vec_b) / (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))\n\n# dibujamos ciertos ejemplos de entrenamiento\ndef plot_imagenes(train_images,train_labels):\n  plt.figure(figsize=(10, 4))\n  for i in range(10):\n      plt.subplot(2, 5, i + 1)\n      plt.grid(False)\n      plt.xticks([])\n      plt.yticks([])\n      plt.imshow(train_images[i], cmap=plt.cm.gray)\n      plt.xlabel(train_labels[i])\n  plt.show()\n\ndef plot_predicciones(model,test_images):\n  plt.figure(figsize=(10, 4))\n  for i in range(10):\n      plt.subplot(2, 5, i + 1)\n      plt.grid(False)\n      plt.xticks([])\n      plt.yticks([])\n      plt.imshow(test_images[i], cmap=plt.cm.gray)\n      pred = np.argmax(model.predict(np.expand_dims(test_images[i], axis=0), verbose=False))\n      plt.xlabel(f\"Pred: {pred}\")\n  plt.show()"
  },
  {
    "objectID": "semana_3/notebooks/Nb_3a_Introduccion_transfer_learning_finetuning.html#transfer-learning-y-fine-tuning-en-clasificaciÃ³n-de-imÃ¡genes",
    "href": "semana_3/notebooks/Nb_3a_Introduccion_transfer_learning_finetuning.html#transfer-learning-y-fine-tuning-en-clasificaciÃ³n-de-imÃ¡genes",
    "title": "IntroducciÃ³n a pre-trained models, transfer learning and fine tuning",
    "section": "Transfer Learning y fine tuning en ClasificaciÃ³n de imÃ¡genes",
    "text": "Transfer Learning y fine tuning en ClasificaciÃ³n de imÃ¡genes\n\nDescargar los datos\n\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\nprint(\"Train shape: \", train_images.shape)\nprint(\"Test shape: \", test_images.shape)\n\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n\n11490434/11490434 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 0us/step\n\nTrain shape:  (60000, 28, 28)\n\nTest shape:  (10000, 28, 28)\n\n\n\n\n\nplot_imagenes(train_images,train_labels)\n\n\n\n\n\n\n\n\n\n# Redimensionamos las imagenes a 32x32 (el minimo tamaÃ±o que soporta vgg16)\ntrain_images = tf.image.grayscale_to_rgb(tf.expand_dims(train_images, axis=-1))\ntest_images = tf.image.grayscale_to_rgb(tf.expand_dims(test_images, axis=-1))\n# Agregamos 3 canales de color (RGB) debido a que la red tambien lo necesita\ntrain_images = tf.image.resize(train_images, [32, 32])\ntest_images = tf.image.resize(test_images, [32, 32])\n# Normalizamos las imagenes entre [0, 1] para que el aprendizaje sea mas suave\ntrain_images = train_images / 255.0\ntest_images = test_images / 255.0\nprint(\"Train shape: \", train_images.shape)\nprint(\"Test shape: \", test_images.shape)\n\nTrain shape:  (60000, 32, 32, 3)\nTest shape:  (10000, 32, 32, 3)\n\n\n\n# Convertimos las etiquetas a formato one-hot encoding\ntrain_labels_ohc = to_categorical(train_labels, 10)\ntest_labels_ohc = to_categorical(test_labels, 10)\nprint(train_labels_ohc[:,1])\n\n[0. 0. 0. ... 0. 0. 0.]\n\n\n\n\nTransfer-learning\nEn esta secciÃ³n, utilizaremos el modelo pre-entrenado VGG16, el cual es una red convolucional que es usada para reconocimiento de imÃ¡genes. Este modelo fue entrenado con ImageNet, por lo cual es un modelo bastante robusto.\nObjetivo: - El objetivo serÃ¡ cargar dicho modelo pre-entrenado y utilizarlo sobre el dataset MNIST, el cual contiene 70.000 imagenes de digitos escritos a mano. AsÃ­, poder clasificar dichos nÃºmeros con nuestro modelo entrenado previamente.\n\n# Cargamos el modelo VGG16 preentrenado, excluyendo las capas superiores (top=False)\n# Recuerde que las capas superiores son las que definen el tipo de problema a solucionar\n# Como nuestro problema es de 10 categorias (10 digitos), agregaremos nuestras propias capas superiores\nvgg16_base = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\nvgg16_base.summary()\n\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n\n58889256/58889256 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 0us/step\n\n\n\n\nModel: \"vgg16\"\n\n\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ Layer (type)                    â”ƒ Output Shape           â”ƒ       Param # â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ input_layer (InputLayer)        â”‚ (None, 32, 32, 3)      â”‚             0 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block1_conv1 (Conv2D)           â”‚ (None, 32, 32, 64)     â”‚         1,792 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block1_conv2 (Conv2D)           â”‚ (None, 32, 32, 64)     â”‚        36,928 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block1_pool (MaxPooling2D)      â”‚ (None, 16, 16, 64)     â”‚             0 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block2_conv1 (Conv2D)           â”‚ (None, 16, 16, 128)    â”‚        73,856 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block2_conv2 (Conv2D)           â”‚ (None, 16, 16, 128)    â”‚       147,584 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block2_pool (MaxPooling2D)      â”‚ (None, 8, 8, 128)      â”‚             0 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block3_conv1 (Conv2D)           â”‚ (None, 8, 8, 256)      â”‚       295,168 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block3_conv2 (Conv2D)           â”‚ (None, 8, 8, 256)      â”‚       590,080 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block3_conv3 (Conv2D)           â”‚ (None, 8, 8, 256)      â”‚       590,080 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block3_pool (MaxPooling2D)      â”‚ (None, 4, 4, 256)      â”‚             0 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block4_conv1 (Conv2D)           â”‚ (None, 4, 4, 512)      â”‚     1,180,160 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block4_conv2 (Conv2D)           â”‚ (None, 4, 4, 512)      â”‚     2,359,808 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block4_conv3 (Conv2D)           â”‚ (None, 4, 4, 512)      â”‚     2,359,808 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block4_pool (MaxPooling2D)      â”‚ (None, 2, 2, 512)      â”‚             0 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block5_conv1 (Conv2D)           â”‚ (None, 2, 2, 512)      â”‚     2,359,808 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block5_conv2 (Conv2D)           â”‚ (None, 2, 2, 512)      â”‚     2,359,808 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block5_conv3 (Conv2D)           â”‚ (None, 2, 2, 512)      â”‚     2,359,808 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ block5_pool (MaxPooling2D)      â”‚ (None, 1, 1, 512)      â”‚             0 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n Total params: 14,714,688 (56.13 MB)\n\n\n\n Trainable params: 14,714,688 (56.13 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# Ahora congelamos los pesos del modelo, pues solo queremos agrega una nueva capa\n# con 10 neuronas, donde cada una representarÃ¡ el digito que queremos predecir\nvgg16_base.trainable = False\nmodel = models.Sequential([\n    vgg16_base,\n    layers.Flatten(),\n    layers.Dropout(0.5),\n    layers.Dense(10, activation='softmax')  # 10 clases de salida\n])\n\n\n# Compilamos y entrenamos los pesos de nuestra Ãºltima capa\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model.fit(train_images, train_labels_ohc, epochs=5,\n                    batch_size=64, validation_data=(test_images, test_labels_ohc))\n\n\nEpoch 1/5\n\n938/938 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13s 9ms/step - accuracy: 0.5480 - loss: 1.4039 - val_accuracy: 0.8680 - val_loss: 0.5527\n\nEpoch 2/5\n\n938/938 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 5ms/step - accuracy: 0.8086 - loss: 0.6411 - val_accuracy: 0.8936 - val_loss: 0.4141\n\nEpoch 3/5\n\n938/938 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 6ms/step - accuracy: 0.8306 - loss: 0.5443 - val_accuracy: 0.9095 - val_loss: 0.3555\n\nEpoch 4/5\n\n938/938 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 5ms/step - accuracy: 0.8401 - loss: 0.5059 - val_accuracy: 0.9176 - val_loss: 0.3233\n\nEpoch 5/5\n\n938/938 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 5ms/step - accuracy: 0.8428 - loss: 0.4853 - val_accuracy: 0.9235 - val_loss: 0.3022\n\n\n\n\n\n# Medimos la precisiÃ³n del modelo en el conjunto de prueba\ntest_loss, test_acc = model.evaluate(test_images, test_labels_ohc)\n\nprint(f\"PrecisiÃ³n en el conjunto de prueba: {test_acc}\")\n\n\n313/313 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 4ms/step - accuracy: 0.9145 - loss: 0.3246\n\nPrecisiÃ³n en el conjunto de prueba: 0.9235000014305115\n\n\n\n\n\n# Mostramos el modelo\nmodel.summary()\n\nModel: \"sequential\"\n\n\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ Layer (type)                    â”ƒ Output Shape           â”ƒ       Param # â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ vgg16 (Functional)              â”‚ (None, 1, 1, 512)      â”‚    14,714,688 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ flatten (Flatten)               â”‚ (None, 512)            â”‚             0 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_12 (Dropout)            â”‚ (None, 512)            â”‚             0 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (Dense)                   â”‚ (None, 10)             â”‚         5,130 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n Total params: 14,730,080 (56.19 MB)\n\n\n\n Trainable params: 5,130 (20.04 KB)\n\n\n\n Non-trainable params: 14,714,688 (56.13 MB)\n\n\n\n Optimizer params: 10,262 (40.09 KB)\n\n\n\n\n# Dibujamos ciertas imÃ¡genes con sus predicciones\nplot_predicciones(model,test_images)\n\n\n\n\n\n\n\n\nObservaciones:\n\nNote que al cargar un modelo pre-entrenado, logramos tener unos pesos que ya saben encontrar ciertos tipos de caracterÃ­sticas dentro de las imÃ¡genes. Es por ello que cuando entrenamos nuestra capa superior (10 neuronas), solo hacen falta 5 Ã©pocas para alcanzar un accuracy del 92.14% en el conjunto de prueba.\nCabe resaltar que utilizamos un modelo pre-entrenado y agregamos una capa superior para adaptarlo a nuestro problema. Esto se podrÃ­a considerar transfer learning tambien.\n\n\n\nTuning o Re-entrenamiento\nPara dejar mas claro el concepto de transfer learning lo que haremos es coger el mismo modelo definido anteriormente, solo que esta vez si entrenaremos los pesos del modelo pre-entrenado, para asÃ­ alcanzar un mejor rendimiento.\n\n# Reiniciar el backend para que las ejecuciones anteriores no interfieran\nK.clear_session()\n\n\n# Definimos el modelo, especificando que queremos entrenar el modelo VGG16\nvgg16_base.trainable = True\nmodel_2 = models.Sequential([\n    vgg16_base,\n    layers.Flatten(),\n    layers.Dropout(0.5),\n    layers.Dense(10, activation='softmax')  # 10 clases de salida\n])\n\n\n# Compilamos y entrenamos los pesos de nuestra Ãºltima capa\nmodel_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model_2.fit(train_images, train_labels_ohc, epochs=5,\n                    batch_size=64, validation_data=(test_images, test_labels_ohc))\n\n\nEpoch 1/5\n\n938/938 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 31s 22ms/step - accuracy: 0.1100 - loss: 2.3122 - val_accuracy: 0.1135 - val_loss: 2.3011\n\nEpoch 2/5\n\n938/938 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 15s 16ms/step - accuracy: 0.1129 - loss: 2.3012 - val_accuracy: 0.1135 - val_loss: 2.3010\n\nEpoch 3/5\n\n938/938 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 15s 16ms/step - accuracy: 0.1132 - loss: 2.3009 - val_accuracy: 0.1135 - val_loss: 2.3011\n\nEpoch 4/5\n\n938/938 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 15s 16ms/step - accuracy: 0.1119 - loss: 2.3012 - val_accuracy: 0.1135 - val_loss: 2.3010\n\nEpoch 5/5\n\n938/938 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 15s 16ms/step - accuracy: 0.1119 - loss: 2.3014 - val_accuracy: 0.1135 - val_loss: 2.3011\n\n\n\n\n\n# Medimos la precisiÃ³n del modelo 2 en el conjunto de prueba\ntest_loss, test_acc = model_2.evaluate(test_images, test_labels_ohc)\n\nprint(f\"PrecisiÃ³n en el conjunto de prueba: {test_acc}\")\n\n\n313/313 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 4ms/step - accuracy: 0.1160 - loss: 2.3010\n\nPrecisiÃ³n en el conjunto de prueba: 0.11349999904632568\n\n\n\n\n\n# Imprimamos la estructura del modelo 2\nmodel_2.summary()\n\nModel: \"sequential\"\n\n\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ Layer (type)                    â”ƒ Output Shape           â”ƒ       Param # â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ vgg16 (Functional)              â”‚ (None, 1, 1, 512)      â”‚    14,714,688 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ flatten (Flatten)               â”‚ (None, 512)            â”‚             0 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout (Dropout)               â”‚ (None, 512)            â”‚             0 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (Dense)                   â”‚ (None, 10)             â”‚         5,130 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n Total params: 44,159,456 (168.45 MB)\n\n\n\n Trainable params: 14,719,818 (56.15 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n Optimizer params: 29,439,638 (112.30 MB)\n\n\n\nObservaciones:\n\nAntes de hablar del mal rendimiento del modelo (un 9.7% de accuracy en el conjunto de prueba). Hay que hablar de que ahora demorÃ³ mas entrenandose. Esto se debe a que ahora, se ajustaron todos los parÃ¡meros posibles, no como en el modelo anterior que solo ajustamos los parÃ¡metros de la capa superior.\nUna de las razones por las cuales se obtuvo un accuracy muy bajo, es debido a que empezamos a ajustar el modelo pre-entrenado, pero pasamos de tener 512 neuronas como salida del modelo pre-entrenado, a solo tener 10. Entonces ese error se propagÃ³ y ajusto errÃ³neamente los pesos ya entrenados. Lo cual llevÃ³ a que el modelo no mejorara.\nPara mitigar este error, utilizaremos fine tunning ajustando mas la capa superior. AsÃ­ podremos tener un mejor rendimiento de nuestro modelo de transfer learning\n\n\n\nFine tunning\n\nPara el ajuste fino, lo que haremos es lo siguiente:\n\nCongelaremos las primeras capas del modelo pre-entrenado\nAgregaremos unas capas superiores al modelo.\nEntrenaremos el modelo asÃ­.\nDespuÃ©s, descongelaremos capas superiores del modelo pre-entrenado y hacemos ese ajuste fino (entrenamos) para aumentar el acierto del modelo.\n\n\n\n# Reiniciar el backend para que las ejecuciones anteriores no interfieran\nK.clear_session()\n\n\nvgg16_base = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n\nfor layer in vgg16_base.layers[:15]:  # Congelar las primeras 15 capas\n    layer.trainable = False\n\n# Agregamos mas neuronas despuÃ©s de nuestro modelo pre-entrenado, para hacer un ajuste mas fino\nmodel_3 = models.Sequential([\n    vgg16_base,\n    layers.Flatten(),\n    layers.Dense(512, activation='relu'),  # Incrementamos el nÃºmero de unidades para mayor capacidad de representaciÃ³n\n    layers.Dropout(0.5),                   # Aumentamos el Dropout para evitar el sobreajuste\n    layers.Dense(10, activation='softmax') # Capa final con 10 clases\n])\n\n\n# compilamos el modelo y definimos una parada temprana para mitigar el sobreajuste\nmodel_3.compile(optimizer=optimizers.Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\nearly_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\n\n# Entrenamos nuestro modelo\nhistory = model_3.fit(train_images, train_labels_ohc, batch_size=64,\n                      epochs=20,\n                      validation_data=(test_images, test_labels_ohc),\n                      callbacks=[early_stopping])\n\n\nEpoch 1/20\n\n938/938 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 17s 13ms/step - accuracy: 0.8968 - loss: 0.3208 - val_accuracy: 0.9841 - val_loss: 0.0493\n\nEpoch 2/20\n\n938/938 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7s 8ms/step - accuracy: 0.9850 - loss: 0.0495 - val_accuracy: 0.9894 - val_loss: 0.0372\n\nEpoch 3/20\n\n938/938 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7s 8ms/step - accuracy: 0.9884 - loss: 0.0397 - val_accuracy: 0.9913 - val_loss: 0.0275\n\nEpoch 4/20\n\n938/938 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7s 8ms/step - accuracy: 0.9912 - loss: 0.0292 - val_accuracy: 0.9925 - val_loss: 0.0288\n\nEpoch 5/20\n\n938/938 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7s 8ms/step - accuracy: 0.9929 - loss: 0.0242 - val_accuracy: 0.9854 - val_loss: 0.0503\n\nEpoch 6/20\n\n938/938 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7s 8ms/step - accuracy: 0.9937 - loss: 0.0216 - val_accuracy: 0.9908 - val_loss: 0.0320\n\nEpoch 7/20\n\n938/938 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7s 8ms/step - accuracy: 0.9937 - loss: 0.0199 - val_accuracy: 0.9897 - val_loss: 0.0335\n\nEpoch 8/20\n\n938/938 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7s 8ms/step - accuracy: 0.9947 - loss: 0.0184 - val_accuracy: 0.9914 - val_loss: 0.0270\n\nEpoch 9/20\n\n938/938 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7s 8ms/step - accuracy: 0.9949 - loss: 0.0169 - val_accuracy: 0.9919 - val_loss: 0.0288\n\nEpoch 10/20\n\n938/938 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7s 8ms/step - accuracy: 0.9958 - loss: 0.0136 - val_accuracy: 0.9917 - val_loss: 0.0338\n\nEpoch 11/20\n\n938/938 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7s 8ms/step - accuracy: 0.9959 - loss: 0.0132 - val_accuracy: 0.9929 - val_loss: 0.0277\n\nEpoch 12/20\n\n938/938 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7s 8ms/step - accuracy: 0.9962 - loss: 0.0136 - val_accuracy: 0.9919 - val_loss: 0.0294\n\nEpoch 13/20\n\n938/938 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7s 8ms/step - accuracy: 0.9960 - loss: 0.0127 - val_accuracy: 0.9929 - val_loss: 0.0228\n\nEpoch 14/20\n\n938/938 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7s 8ms/step - accuracy: 0.9974 - loss: 0.0090 - val_accuracy: 0.9888 - val_loss: 0.0443\n\nEpoch 15/20\n\n938/938 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7s 8ms/step - accuracy: 0.9968 - loss: 0.0114 - val_accuracy: 0.9921 - val_loss: 0.0278\n\nEpoch 16/20\n\n938/938 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7s 8ms/step - accuracy: 0.9973 - loss: 0.0094 - val_accuracy: 0.9903 - val_loss: 0.0392\n\nEpoch 17/20\n\n938/938 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7s 8ms/step - accuracy: 0.9968 - loss: 0.0105 - val_accuracy: 0.9924 - val_loss: 0.0302\n\nEpoch 18/20\n\n938/938 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7s 8ms/step - accuracy: 0.9976 - loss: 0.0091 - val_accuracy: 0.9930 - val_loss: 0.0325\n\n\n\n\n\n# Evaluamos el accuracy del modelo en los datos de prueba\ntest_loss, test_acc = model_3.evaluate(test_images, test_labels_ohc)\nprint(f\"PrecisiÃ³n despuÃ©s del fine-tuning avanzado: {test_acc}\")\n\n\n313/313 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 4ms/step - accuracy: 0.9915 - loss: 0.0283\n\nPrecisiÃ³n despuÃ©s del fine-tuning avanzado: 0.9927999973297119"
  },
  {
    "objectID": "semana_3/notebooks/Nb_3a_Introduccion_transfer_learning_finetuning.html#transfer-learning-y-fine-tuning-en-clasificaciÃ³n-de-texto",
    "href": "semana_3/notebooks/Nb_3a_Introduccion_transfer_learning_finetuning.html#transfer-learning-y-fine-tuning-en-clasificaciÃ³n-de-texto",
    "title": "IntroducciÃ³n a pre-trained models, transfer learning and fine tuning",
    "section": "Transfer learning y fine tuning en ClasificaciÃ³n de texto",
    "text": "Transfer learning y fine tuning en ClasificaciÃ³n de texto\n\nDescargar el dataset\n\n# Obtener desde tensorflow datasets\ndataset, info = tfds.load('imdb_reviews', with_info=True,\n                          as_supervised=True)\ntrain_dataset, test_dataset = dataset['train'], dataset['test']\n\nWARNING:absl:Variant folder /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0 has no dataset_info.json\n\n\nDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\n\n\n\nBUFFER_SIZE = 10000\nBATCH_SIZE = 64\n\n# optimizaciÃ³n para train\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\n# optimizaciÃ³n para test\ntest_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\n\n\nMÃ©todo manual - Usando embebidos directamente\nEs necesario descargar los correspondientes vectores pre-entrenados para ser usados posteriormente en nuestros proyectos.\nFastText:\nFastText, desarrollado por el equipo de IA de Facebook (FAIR), es un algoritmo popular para aprender embeddings de palabras. Tiene algunas caracterÃ­sticas distintivas:\n\nBasado en Subpalabras (N-gramas de Caracteres): A diferencia de Word2Vec, que trata cada palabra como una unidad atÃ³mica, FastText representa cada palabra como una bolsa de n-gramas de caracteres. Por ejemplo, la palabra â€œmanzanaâ€ con n-gramas de 3 caracteres (trigramas) se podrÃ­a representar como &lt;ma, man, anz, nza, zan, ana, na&gt; (donde &lt; y &gt; marcan el inicio y fin de la palabra). El vector de la palabra â€œmanzanaâ€ se forma sumando los vectores de sus n-gramas.Esto permite a FastText generar embeddings para palabras que no vio durante el entrenamiento (palabras fuera de vocabulario u OOV), ya que es probable que sus n-gramas sÃ­ hayan sido vistos en otras palabras. TambiÃ©n maneja mejor palabras raras y lenguajes morfolÃ³gicamente ricos (donde las palabras cambian mucho su forma, como el espaÃ±ol o el alemÃ¡n).\nCorpus: Provienen de Common Crawl, un corpus masivo que contiene datos rastreados de la web.\nAlgoritmo: Se entrenan utilizando arquitecturas similares a Word2Vec, como CBOW (Continuous Bag-of-Words) o Skip-gram.\n\nCBOW: Intenta predecir la palabra central a partir de las palabras de su contexto.\nSkip-gram: Intenta predecir las palabras del contexto a partir de la palabra central.\n\n\n\nDescargar embeddings Fasttext\n\n# Descargar vectores embebidos en inglÃ©s y espaÃ±ol\n# Descargar los vectores FastText de inglÃ©s y espaÃ±ol\n!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.vec.gz\n\n# Descomprimir los archivos\n!gunzip cc.en.300.vec.gz # vectores de dim 300 en inglÃ©s\n!gunzip cc.es.300.vec.gz # vectores de dim 300 es espaÃ±ol\n\n--2025-05-13 16:28:14--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\nResolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 108.157.254.124, 108.157.254.15, 108.157.254.102, ...\nConnecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|108.157.254.124|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1325960915 (1.2G) [binary/octet-stream]\nSaving to: â€˜cc.en.300.vec.gzâ€™\n\ncc.en.300.vec.gz    100%[===================&gt;]   1.23G  23.3MB/s    in 61s     \n\n2025-05-13 16:29:15 (20.8 MB/s) - â€˜cc.en.300.vec.gzâ€™ saved [1325960915/1325960915]\n\n--2025-05-13 16:29:16--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.vec.gz\nResolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 108.157.254.15, 108.157.254.124, 108.157.254.121, ...\nConnecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|108.157.254.15|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1285580896 (1.2G) [binary/octet-stream]\nSaving to: â€˜cc.es.300.vec.gzâ€™\n\ncc.es.300.vec.gz    100%[===================&gt;]   1.20G  22.9MB/s    in 53s     \n\n2025-05-13 16:30:09 (23.1 MB/s) - â€˜cc.es.300.vec.gzâ€™ saved [1285580896/1285580896]\n\n\n\n\nprint(f\"NÃºmero de palabras en la matriz de inglÃ©s:\")\n!head -n 1 cc.en.300.vec\n\nprint(f\"NÃºmero de palabras en la matriz de espaÃ±ol:\")\n!head -n 1 cc.es.300.vec\n\nNÃºmero de palabras en la matriz de inglÃ©s:\n2000000 300\nNÃºmero de palabras en la matriz de espaÃ±ol:\n2000000 300\n\n\n\n# Cargar los embeddings preentrenados de FastText (esto puede tardar un poco)\n# limitar a 50.000 palabras mÃ¡s frecuentes\nembedding_en = KeyedVectors.load_word2vec_format('cc.en.300.vec',\n                                                 binary=False,\n                                                 limit=50000)\n\nembedding_es = KeyedVectors.load_word2vec_format('cc.es.300.vec',\n                                                 binary=False,\n                                                 limit=50000)\n\n\n\nRelaciÃ³n semÃ¡ntica de los embeddings\nSemÃ¡ntica en InglÃ©s\n\n# Palabras similares\npalabra_en = \"king\"\nif palabra_en in embedding_en:\n    similares_en = embedding_en.most_similar(palabra_en, topn=5)\n    print(f\"Palabras mÃ¡s similares a '{palabra_en}': {similares_en}\")\n\nPalabras mÃ¡s similares a 'king': [('kings', 0.7550534605979919), ('queen', 0.7069182991981506), ('King', 0.6591336727142334), ('prince', 0.6495301723480225), ('monarch', 0.618451714515686)]\n\n\n\n# AnalogÃ­as: rey es a hombre lo que reina es a ? (mujer)\n# king - man + woman = queen\nif all(word in embedding_en for word in [\"king\", \"man\", \"woman\"]):\n    analogia_en = embedding_en.most_similar(positive=['king', 'woman'],\n                                            negative=['man'], topn=3)\n    print(f\"'king' - 'man' + 'woman' se parece a: {analogia_en}\")\n\n'king' - 'man' + 'woman' se parece a: [('queen', 0.7554903030395508), ('princess', 0.5755002498626709), ('monarch', 0.5741325616836548)]\n\n\nSemÃ¡ntica en espaÃ±ol\n\nif embedding_es:\n  if all(word in embedding_es for word in [\"manzana\", \"pera\"]):\n      distancia = embedding_es.similarity(\"manzana\", \"pera\")\n      # Un valor mÃ¡s cercano a 1 indica mÃ¡s similitud\n      print(f\"Similitud entre 'manzana' y 'pera': {distancia:.4f}\")\n\nSimilitud entre 'manzana' y 'pera': 0.6270\n\n\nLimitaciones\n\nprint(f\"--- Demostrando limitaciones entre lenguajes\")\n\n# Palabras a comparar\npalabra_en = \"water\"\npalabra_es = \"agua\"\n\n# obtener los embeddings\nvector_en = embedding_en.get_vector(palabra_en)\nvector_es = embedding_es.get_vector(palabra_es)\n\n# calcular las similitudes\nsimilitud_cruzada = cosine_similarity(vector_en, vector_es)\nprint(f\"Similitud coseno entre '{palabra_en}' (vector EN) y '{palabra_es}' (vector ES): {similitud_cruzada:.4f}\")\n\n--- Demostrando limitaciones entre lenguajes\nSimilitud coseno entre 'water' (vector EN) y 'agua' (vector ES): -0.0832\n\n\n\n\nCrear Vocabulario y matriz de embeddings\n\nVOCAB_SIZE = 10000\nencoder = tf.keras.layers.TextVectorization(\n    max_tokens=VOCAB_SIZE)\n\n# Crea la capa y pasa el texto del conjunto de datos al mÃ©todo .adapt de la capa\nencoder.adapt(train_dataset.map(lambda text, label: text))\n\n\n# Obtener el vocabulario del encoder\nvocab = encoder.get_vocabulary()\n# Dimensiones de los embeddings preentrenados\nembedding_dim = 300  # DimensiÃ³n de los embeddings de FastText\n\n# 1. Crear la matriz de embeddings. Inicializar con ceros\n# ya que esto maneja el token de padding (Ã­ndice 0) correctamente por defecto.\nembedding_matrix = np.zeros((len(vocab), embedding_dim))\n\n# 2. Llenar la matriz con los vectores de FastText y manejar tokens especiales\n# solo vamos a usar los de inglÃ©s\nfor i, word in enumerate(vocab):\n    if word in embedding_en:\n        # La palabra estÃ¡ en FastText, usa su vector\n        embedding_matrix[i] = embedding_en[word]\n    else:\n        # Otras palabras en nuestro vocabulario pero no en FastText\n        embedding_matrix[i] = np.random.uniform(-0.25, 0.25, embedding_dim)\n\n\nembedding_matrix.shape\n\n(10000, 300)\n\n\n\n\nCrear y entrenar modelo\n\nK.clear_session()\ndef rnn(pretrained_vector_matrix):\n    # Ahora utilizamos la API funcional de Keras\n    inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)  # El input serÃ¡ una cadena de texto\n    x = encoder(inputs)  # Aplicamos el encoder\n\n    x = tf.keras.layers.Embedding(\n        input_dim=len(encoder.get_vocabulary()),  # TamaÃ±o del vocabulario\n        output_dim=pretrained_vector_matrix.shape[1],  # DimensiÃ³n de los embeddings (300)\n        embeddings_initializer=tf.keras.initializers.Constant(pretrained_vector_matrix),\n        trainable=True,  # Ajustar los vectores a nuestro dataset\n        mask_zero=True)(x)  # Capa de Embedding\n\n    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False),\n                                      merge_mode='concat')(x)  # Capa LSTM Bidireccional\n\n    x = tf.keras.layers.Dense(64, activation='relu')(x)  # Capa densa\n    x = tf.keras.layers.Dropout(0.6)(x)  # Capa de Dropout para regularizar\n\n    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x) # Capa de salida\n\n    # Definimos el modelo\n    model = tf.keras.Model(inputs, outputs)\n\n    return model\n\n# crear la red RNN con LSTM\nmodel = rnn(embedding_matrix)\n\n# Compilamos el modelo con una lr baja para evitar sobre entrenar\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n              optimizer=tf.keras.optimizers.Adam(3e-5),\n              metrics=['accuracy'])\n\n# Verificamos la estructura del modelo\nmodel.summary()\n\nModel: \"functional\"\n\n\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ Layer (type)        â”ƒ Output Shape      â”ƒ    Param # â”ƒ Connected to      â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ input_layer         â”‚ (None, 1)         â”‚          0 â”‚ -                 â”‚\nâ”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ text_vectorizationâ€¦ â”‚ (None, None)      â”‚          0 â”‚ input_layer[0][0] â”‚\nâ”‚ (TextVectorization) â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ embedding           â”‚ (None, None, 300) â”‚  3,000,000 â”‚ text_vectorizatiâ€¦ â”‚\nâ”‚ (Embedding)         â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ not_equal           â”‚ (None, None)      â”‚          0 â”‚ text_vectorizatiâ€¦ â”‚\nâ”‚ (NotEqual)          â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ bidirectional       â”‚ (None, 128)       â”‚    186,880 â”‚ embedding[0][0],  â”‚\nâ”‚ (Bidirectional)     â”‚                   â”‚            â”‚ not_equal[0][0]   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (Dense)       â”‚ (None, 64)        â”‚      8,256 â”‚ bidirectional[0]â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout (Dropout)   â”‚ (None, 64)        â”‚          0 â”‚ dense[0][0]       â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_1 (Dense)     â”‚ (None, 1)         â”‚         65 â”‚ dropout[0][0]     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n Total params: 3,195,201 (12.19 MB)\n\n\n\n Trainable params: 3,195,201 (12.19 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# hacer una prueba sin usar padding\n# El texto crudo que quieres predecir\nsample_text = ('The movie was cool. The animation and the graphics were out of this world.')\n\n# No es necesario hacer la vectorizaciÃ³n manual aquÃ­, simplemente pasa el texto crudo al modelo\npredictions = model.predict(tf.constant([sample_text]))\n\n# Imprime la predicciÃ³n\nprint(predictions[0])\n\n\n1/1 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 367ms/step\n\n[0.53357685]\n\n\n\n\n\n# Entrenar con early stopping para evitar sobre entrenamiento\nearly_stopping_callback = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',  # MÃ©trica a monitorear\n    patience=3,          # NÃºmero de Ã©pocas sin mejora despuÃ©s de las cuales se detendrÃ¡ el entrenamiento\n    verbose=1,           # Imprime un mensaje cuando el entrenamiento se detiene\n    restore_best_weights=True # Restaura los pesos del modelo de la Ã©poca con el mejor valor de la mÃ©trica monitoreada\n)\n\n# Entrenar\nhistory = model.fit(train_dataset, epochs=10,\n                    validation_data=test_dataset,\n                    validation_steps=30,\n                    callbacks=[early_stopping_callback])\n\n\nEpoch 1/10\n\n391/391 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 31s 72ms/step - accuracy: 0.5101 - loss: 0.6927 - val_accuracy: 0.6089 - val_loss: 0.6839\n\nEpoch 2/10\n\n391/391 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 28s 71ms/step - accuracy: 0.5805 - loss: 0.6813 - val_accuracy: 0.6719 - val_loss: 0.6526\n\nEpoch 3/10\n\n391/391 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 28s 72ms/step - accuracy: 0.7269 - loss: 0.5806 - val_accuracy: 0.8219 - val_loss: 0.4358\n\nEpoch 4/10\n\n391/391 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 27s 70ms/step - accuracy: 0.8348 - loss: 0.4090 - val_accuracy: 0.8484 - val_loss: 0.3730\n\nEpoch 5/10\n\n391/391 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 28s 71ms/step - accuracy: 0.8681 - loss: 0.3452 - val_accuracy: 0.8620 - val_loss: 0.3445\n\nEpoch 6/10\n\n391/391 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 28s 71ms/step - accuracy: 0.8893 - loss: 0.3033 - val_accuracy: 0.8687 - val_loss: 0.3326\n\nEpoch 7/10\n\n391/391 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 27s 70ms/step - accuracy: 0.8995 - loss: 0.2783 - val_accuracy: 0.8724 - val_loss: 0.3172\n\nEpoch 8/10\n\n391/391 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 28s 71ms/step - accuracy: 0.9145 - loss: 0.2510 - val_accuracy: 0.8797 - val_loss: 0.3110\n\nEpoch 9/10\n\n391/391 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 28s 71ms/step - accuracy: 0.9228 - loss: 0.2317 - val_accuracy: 0.8781 - val_loss: 0.3139\n\nEpoch 10/10\n\n391/391 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 27s 70ms/step - accuracy: 0.9324 - loss: 0.2044 - val_accuracy: 0.8797 - val_loss: 0.3228\n\nRestoring model weights from the end of the best epoch: 8.\n\n\n\n\n\ntest_loss, test_acc = model.evaluate(test_dataset)\n\nprint('Test Loss:', test_loss)\nprint('Test Accuracy:', test_acc)\n\n\n391/391 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13s 32ms/step - accuracy: 0.8787 - loss: 0.3016\n\nTest Loss: 0.30069151520729065\n\nTest Accuracy: 0.8774799704551697\n\n\n\n\n\n# Visualizar acc y loss\nplot_curvas(history)\n\n\n\n\n\n\n\n\n\n\n\nMÃ©todo Modular - Usando arquitecturas\nRecientemente, ha crecido la creaciÃ³n de nuevas herramientas que facilitan el uso de modelos pre-entrenados en diferentes tareas de Deep Learning. A continuaciÃ³n, introduccimos KerasHub una librerÃ­a que ha sido propuesta para hacer uso de modelos pre-entrenado usando Keras. Esta pensanda para ser modular y esta enfocada para tareas de ClasificaciÃ³n de imÃ¡genes, ClasificaciÃ³n de texto y Tareas mÃ¡s especializadas sobre generaciÃ³n de secuencias como Audio o Texto.\nÂ¿QuÃ© es KerasHub?\nKerasHub es una biblioteca que proporciona modelos preentrenados para diversas tareas de aprendizaje automÃ¡tico. Estos modelos estÃ¡n implementados como capas (keras.Layer) y modelos (keras.Model) de Keras, lo que permite una integraciÃ³n sencilla en proyectos existentes. La biblioteca incluye modelos para clasificaciÃ³n de imÃ¡genes, generaciÃ³n de texto, clasificaciÃ³n de texto y mÃ¡s, todos accesibles a travÃ©s de una API.\nKerasHub organiza los modelos en componentes modulares:\nTask: Clase de alto nivel que encapsula el modelo y el preprocesamiento para una tarea especÃ­fica (por ejemplo, ImageClassifier, TextClassifier).\nBackbone: Modelo base que extrae caracterÃ­sticas de los datos de entrada.\nPreprocessor: Capa que realiza el preprocesamiento necesario en los datos de entrada (por ejemplo, redimensionamiento de imÃ¡genes, tokenizaciÃ³n de texto).\nTokenizer: Convierte texto en secuencias de tokens.\nImageConverter: Redimensiona y normaliza imÃ¡genes. keras.io\nCada uno de estos componentes puede cargarse desde un preset utilizando from_preset().\n\n\n\nEstructura KerasHub\n\n\nA continuaciÃ³n vamos a importar la arquitectura completa basada en BERT para clasificar nuestro ejercicio anterior.\nArquitectura del modelo: la estructura del modelo BERT.\nPesos preentrenados: valores aprendidos durante el entrenamiento en grandes conjuntos de datos.\nPreprocesamiento: procesos necesarios para preparar los datos de entrada, como tokenizaciÃ³n y normalizaciÃ³n.\nKerasHub descargarÃ¡ automÃ¡ticamente el modelo pre-entrenado desde Hugging Face Hub.\n\nUsando BERT en inferencia\nBERT (Bidirectional Encoder Representations from Transformers) es un modelo de representaciÃ³n del lenguaje que revolucionÃ³ el procesamiento de lenguaje natural (NLP) debido a tres aportes clave:\n\nRepresentaciones Bidireccionales Profundas: A diferencia de modelos anteriores como GPT (unidireccional) o ELMo, BERT es completamente bidireccional, permitiendo que el modelo entienda mejor el contexto de cada palabra teniendo en cuenta tanto la izquierda como la derecha.\nPre-entrenamiento Universal y Fine-tuning Eficiente: BERT puede preentrenarse en texto no etiquetado y luego afinarse con una capa de salida adicional para tareas especÃ­ficas como preguntas y respuestas (QA), inferencia, y reconocimiento de entidades nombradas (NER), sin necesidad de arquitecturas especializadas para cada tarea.\n\n\n\n\nBERT.png\n\n\nEntrenamiento en 2 fases:\n\nPre-entrenamiento (ver parte izquierda de la imagen):\n\n\nMasked Language Model (MLM): Se ocultan aleatoriamente el 15% de los tokens para que el modelo los prediga, permitiendo el aprendizaje bidireccional.\nNext Sentence Prediction (NSP): El modelo predice si una oraciÃ³n B sigue a una oraciÃ³n A. Esto lo entrena para tareas que implican relaciones entre frases.\n\n\nFine-tuning (ver parte derecha de la imagen):\n\n\nSe adapta el modelo preentrenado a tareas especÃ­ficas aÃ±adiendo una capa de salida.\nTodos los parÃ¡metros se afinan con el conjunto de datos etiquetado de la tarea especÃ­fica (e.g., QA, NER, clasificaciÃ³n).\n\n\nclassifier_BERT = keras_hub.models.BertTextClassifier.from_preset(\n    \"bert_base_en_uncased\",\n    #activation='softmax', # El modelo original no la incluye\n    num_classes=2,\n)\n\nDownloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_base_en_uncased/2/download/config.json...\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 510/510 [00:00&lt;00:00, 1.29MB/s]\n\n\n# Si se aÃ±ade la funciÃ³n de activaciÃ³n softmax, las predicciones tienen la prob\n# Pero no parecen estar muy convincentes, parecen estar muy neutras.\n\nclassifier_BERT.predict(test_dataset)\n\n782/782 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 873s 1s/step\narray([[0.6197734 , 0.3802266 ],\n       [0.57702965, 0.42297035],\n       [0.46357015, 0.5364299 ],\n       ...,\n       [0.602075  , 0.39792505],\n       [0.6158818 , 0.38411826],\n       [0.62885654, 0.37114346]], dtype=float32)\n\n# Evaluar para determinar el rendimiento inicial\nclassifier_BERT.evaluate(test_dataset)\n\n\n782/782 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 865s 1s/step - loss: 0.9045 - sparse_categorical_accuracy: 0.4979\n\n\n\n\n[0.9011728167533875, 0.49983999133110046]\n\n\nEfectivamente el rendimiento no es el adecuado. Quiere decir que es gÃ©nerico el resultado.\n\n\nY si hacemos un poco de Fine-tuning?\nSi queremos hacer fine-tuning sobre mÃ¡s capas:\nfor layer in classifier_BERT.backbone.layers[:-4]:\n    layer.trainable = False\nPero solo lo haremos sobre la final:\n\n# Congelamos todo el backbone y solo entrenamos la capa final\nclassifier_BERT.backbone.trainable = False\n\n\n# Re compilar con una nueva tasa de aprendizaje mÃ¡s pequeÃ±a\nclassifier_BERT.compile(\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=tf.keras.optimizers.Adam(3e-4),\n)\n\n\n# Entrenar con early stopping para evitar sobre entrenamiento\nearly_stopping_callback = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',  # MÃ©trica a monitorear\n    patience=3,          # NÃºmero de Ã©pocas sin mejora despuÃ©s de las cuales se detendrÃ¡ el entrenamiento\n    verbose=1,           # Imprime un mensaje cuando el entrenamiento se detiene\n    restore_best_weights=True # Restaura los pesos del modelo de la Ã©poca con el mejor valor de la mÃ©trica monitoreada\n)\n\n# Ajustamos el clasificador con pocas epocas\nclassifier_BERT.fit(train_dataset, validation_data=test_dataset,\n               epochs=15,\n               callbacks=[early_stopping_callback])\n\n\nEpoch 1/15\n\n391/391 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 591s 1s/step - loss: 0.6668 - sparse_categorical_accuracy: 0.5892 - val_loss: 0.6053 - val_sparse_categorical_accuracy: 0.6562\n\nEpoch 2/15\n\n391/391 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 522s 1s/step - loss: 0.5861 - sparse_categorical_accuracy: 0.7168 - val_loss: 0.5421 - val_sparse_categorical_accuracy: 0.7560\n\nEpoch 3/15\n\n391/391 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 522s 1s/step - loss: 0.5484 - sparse_categorical_accuracy: 0.7403 - val_loss: 0.5594 - val_sparse_categorical_accuracy: 0.6960\n\nEpoch 4/15\n\n391/391 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 522s 1s/step - loss: 0.5295 - sparse_categorical_accuracy: 0.7512 - val_loss: 0.5033 - val_sparse_categorical_accuracy: 0.7643\n\nEpoch 5/15\n\n391/391 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 522s 1s/step - loss: 0.5126 - sparse_categorical_accuracy: 0.7631 - val_loss: 0.4770 - val_sparse_categorical_accuracy: 0.7936\n\nEpoch 6/15\n\n391/391 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 522s 1s/step - loss: 0.5102 - sparse_categorical_accuracy: 0.7595 - val_loss: 0.4724 - val_sparse_categorical_accuracy: 0.7898\n\nEpoch 7/15\n\n391/391 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 522s 1s/step - loss: 0.4965 - sparse_categorical_accuracy: 0.7730 - val_loss: 0.4728 - val_sparse_categorical_accuracy: 0.7831\n\nEpoch 8/15\n\n391/391 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 522s 1s/step - loss: 0.4888 - sparse_categorical_accuracy: 0.7708 - val_loss: 0.4520 - val_sparse_categorical_accuracy: 0.8011\n\nEpoch 9/15\n\n391/391 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 522s 1s/step - loss: 0.4899 - sparse_categorical_accuracy: 0.7735 - val_loss: 0.4458 - val_sparse_categorical_accuracy: 0.8042\n\nEpoch 10/15\n\n391/391 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 522s 1s/step - loss: 0.4772 - sparse_categorical_accuracy: 0.7774 - val_loss: 0.4375 - val_sparse_categorical_accuracy: 0.8110\n\nEpoch 11/15\n\n391/391 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 522s 1s/step - loss: 0.4774 - sparse_categorical_accuracy: 0.7795 - val_loss: 0.4443 - val_sparse_categorical_accuracy: 0.8003\n\nEpoch 12/15\n\n391/391 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 522s 1s/step - loss: 0.4755 - sparse_categorical_accuracy: 0.7779 - val_loss: 0.4296 - val_sparse_categorical_accuracy: 0.8130\n\nEpoch 13/15\n\n391/391 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 522s 1s/step - loss: 0.4725 - sparse_categorical_accuracy: 0.7803 - val_loss: 0.4310 - val_sparse_categorical_accuracy: 0.8094\n\nEpoch 14/15\n\n391/391 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 522s 1s/step - loss: 0.4725 - sparse_categorical_accuracy: 0.7784 - val_loss: 0.4270 - val_sparse_categorical_accuracy: 0.8111\n\nEpoch 15/15\n\n391/391 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 522s 1s/step - loss: 0.4698 - sparse_categorical_accuracy: 0.7800 - val_loss: 0.4229 - val_sparse_categorical_accuracy: 0.8147\n\nRestoring model weights from the end of the best epoch: 15.\n\n\n\n\n&lt;keras.src.callbacks.history.History at 0x78998cceaa90&gt;\n\n\n\n# Finalmente evaluamos el rendimiento despuÃ©s de hacer fine tuning\nclassifier_BERT.evaluate(test_dataset)\n\n\n391/391 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 235s 600ms/step - loss: 0.4238 - sparse_categorical_accuracy: 0.8131\n\n\n\n\n[0.4228881299495697, 0.8147199749946594]\n\n\n\n\nUsando tÃº propio BERT\nTendrÃ¡s que hacer los siguientes pasos:\n\nConfigurar el tokenizador con el vocabulario que hayas definido (como lo hicimos en el ejemplo base.\nCrear un preprocesador\nDefinir el backbone, con las dimensiones deseadas (ya sabes de que hablamos si conoces Transformers),\nConstruir el clasificador y entrenar en tus datos)\n\ntokenizer = keras_hub.models.BertTokenizer(\n    vocabulary=vocab,\n\npreprocessor = keras_hub.models.BertTextClassifierPreprocessor(\n    tokenizer=tokenizer,\n    sequence_length=128,\n)\nbackbone = keras_hub.models.BertBackbone(\n    vocabulary_size=30552,\n    num_layers=4,\n    num_heads=4,\n    hidden_dim=256,\n    intermediate_dim=512,\n    max_sequence_length=128,\n)\nclassifier = keras_hub.models.BertTextClassifier(\n    backbone=backbone,\n    preprocessor=preprocessor,\n    num_classes=4,\n)\nclassifier.fit(x=features, y=labels, batch_size=2)\nPara mÃ¡s informaciÃ³n consultar: Getting Started con Keras Hub\nAdemÃ¡s puede consultar todos los modelos pre-entrenados en Keras presets y las arquitecturas disponibles Keras architectures\nPara mÃ¡s sobre BERT."
  },
  {
    "objectID": "semana_3/notebooks/Nb_3a_Introduccion_transfer_learning_finetuning.html#pre-training-en-datos-tabulares-usando-tabnet",
    "href": "semana_3/notebooks/Nb_3a_Introduccion_transfer_learning_finetuning.html#pre-training-en-datos-tabulares-usando-tabnet",
    "title": "IntroducciÃ³n a pre-trained models, transfer learning and fine tuning",
    "section": "Pre-training en Datos tabulares usando TabNet",
    "text": "Pre-training en Datos tabulares usando TabNet\n\ndata = fetch_covtype(as_frame=True)\nX = data.data.astype(np.float32)\ny = data.target.astype(int) - 1  # etiquetas 0â€‘6 para las 7 clases\n\nX\n\n\n    \n\n\n\n\n\n\nElevation\nAspect\nSlope\nHorizontal_Distance_To_Hydrology\nVertical_Distance_To_Hydrology\nHorizontal_Distance_To_Roadways\nHillshade_9am\nHillshade_Noon\nHillshade_3pm\nHorizontal_Distance_To_Fire_Points\n...\nSoil_Type_30\nSoil_Type_31\nSoil_Type_32\nSoil_Type_33\nSoil_Type_34\nSoil_Type_35\nSoil_Type_36\nSoil_Type_37\nSoil_Type_38\nSoil_Type_39\n\n\n\n\n0\n2596.0\n51.0\n3.0\n258.0\n0.0\n510.0\n221.0\n232.0\n148.0\n6279.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n2590.0\n56.0\n2.0\n212.0\n-6.0\n390.0\n220.0\n235.0\n151.0\n6225.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n2804.0\n139.0\n9.0\n268.0\n65.0\n3180.0\n234.0\n238.0\n135.0\n6121.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n2785.0\n155.0\n18.0\n242.0\n118.0\n3090.0\n238.0\n238.0\n122.0\n6211.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n2595.0\n45.0\n2.0\n153.0\n-1.0\n391.0\n220.0\n234.0\n150.0\n6172.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n581007\n2396.0\n153.0\n20.0\n85.0\n17.0\n108.0\n240.0\n237.0\n118.0\n837.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n581008\n2391.0\n152.0\n19.0\n67.0\n12.0\n95.0\n240.0\n237.0\n119.0\n845.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n581009\n2386.0\n159.0\n17.0\n60.0\n7.0\n90.0\n236.0\n241.0\n130.0\n854.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n581010\n2384.0\n170.0\n15.0\n60.0\n5.0\n90.0\n230.0\n245.0\n143.0\n864.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n581011\n2383.0\n165.0\n13.0\n60.0\n4.0\n67.0\n231.0\n244.0\n141.0\n875.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n581012 rows Ã— 54 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\n\nclf = TabNetClassifier(\n    optimizer_fn=torch.optim.Adam,\n    optimizer_params=dict(lr=2e-2),\n    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n    scheduler_params={\"step_size\":10, \"gamma\":0.9},\n    mask_type='entmax', #sparsemax\n    device_name='cuda'\n)\n\n/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n  warnings.warn(f\"Device used : {self.device}\")\n\n\n\nclf.fit(\n    X_train=X_train.values, y_train=y_train.values,\n    eval_set=[(X_valid.values, y_valid.values)],\n    eval_name=['valid'],\n    eval_metric=['accuracy'],\n    max_epochs=50,\n    num_workers=2\n)\n\nepoch 0  | loss: 0.72523 | valid_accuracy: 0.7489  |  0:00:20s\nepoch 1  | loss: 0.55862 | valid_accuracy: 0.78073 |  0:00:40s\nepoch 2  | loss: 0.51353 | valid_accuracy: 0.79955 |  0:01:01s\nepoch 3  | loss: 0.48882 | valid_accuracy: 0.81313 |  0:01:21s\nepoch 4  | loss: 0.46969 | valid_accuracy: 0.82237 |  0:01:41s\nepoch 5  | loss: 0.45493 | valid_accuracy: 0.82435 |  0:02:01s\nepoch 6  | loss: 0.44548 | valid_accuracy: 0.82876 |  0:02:22s\nepoch 7  | loss: 0.43394 | valid_accuracy: 0.83884 |  0:02:42s\nepoch 8  | loss: 0.42682 | valid_accuracy: 0.84025 |  0:03:02s\nepoch 9  | loss: 0.42305 | valid_accuracy: 0.84224 |  0:03:23s\nepoch 10 | loss: 0.41117 | valid_accuracy: 0.84793 |  0:03:43s\nepoch 11 | loss: 0.40697 | valid_accuracy: 0.8532  |  0:04:04s\nepoch 12 | loss: 0.40315 | valid_accuracy: 0.85364 |  0:04:24s\nepoch 13 | loss: 0.40081 | valid_accuracy: 0.8563  |  0:04:44s\nepoch 14 | loss: 0.39851 | valid_accuracy: 0.84837 |  0:05:04s\nepoch 15 | loss: 0.39728 | valid_accuracy: 0.85664 |  0:05:25s\nepoch 16 | loss: 0.39151 | valid_accuracy: 0.85597 |  0:05:45s\nepoch 17 | loss: 0.38807 | valid_accuracy: 0.86308 |  0:06:06s\nepoch 18 | loss: 0.38705 | valid_accuracy: 0.85793 |  0:06:26s\nepoch 19 | loss: 0.38338 | valid_accuracy: 0.85365 |  0:06:47s\nepoch 20 | loss: 0.38018 | valid_accuracy: 0.86545 |  0:07:07s\nepoch 21 | loss: 0.37847 | valid_accuracy: 0.86482 |  0:07:28s\nepoch 22 | loss: 0.37902 | valid_accuracy: 0.86319 |  0:07:48s\nepoch 23 | loss: 0.3751  | valid_accuracy: 0.87118 |  0:08:08s\nepoch 24 | loss: 0.37328 | valid_accuracy: 0.86573 |  0:08:29s\nepoch 25 | loss: 0.3702  | valid_accuracy: 0.86972 |  0:08:49s\nepoch 26 | loss: 0.37631 | valid_accuracy: 0.86124 |  0:09:10s\nepoch 27 | loss: 0.37374 | valid_accuracy: 0.87088 |  0:09:31s\nepoch 28 | loss: 0.37042 | valid_accuracy: 0.86521 |  0:09:51s\nepoch 29 | loss: 0.37133 | valid_accuracy: 0.86584 |  0:10:12s\nepoch 30 | loss: 0.36739 | valid_accuracy: 0.87149 |  0:10:33s\nepoch 31 | loss: 0.36381 | valid_accuracy: 0.86824 |  0:10:54s\nepoch 32 | loss: 0.37192 | valid_accuracy: 0.871   |  0:11:14s\nepoch 33 | loss: 0.36089 | valid_accuracy: 0.87442 |  0:11:35s\nepoch 34 | loss: 0.36402 | valid_accuracy: 0.87417 |  0:11:56s\nepoch 35 | loss: 0.36055 | valid_accuracy: 0.87339 |  0:12:16s\n\n\n\nParÃ¡metros de TabNet (Instanciar)\n\n\n\n\n\n\n\nParÃ¡metro\nDescripciÃ³n\n\n\n\n\nn_d / n_a\nDimensiÃ³n de la capa de decisiÃ³n y atenciÃ³n. Usualmente entre 8 y 64. Recomendado: n_d = n_a.\n\n\nn_steps\nNÃºmero de pasos (bloques de atenciÃ³n). Sugerido: 3â€“10.\n\n\ngamma\nCoeficiente de reuso de caracterÃ­sticas en las mÃ¡scaras. Rango: 1.0â€“2.0 (default 1.3).\n\n\ncat_idxs\nÃndices de las variables categÃ³ricas (requerido para embeddings).\n\n\ncat_dims\nNÃºmero de categorÃ­as Ãºnicas por variable categÃ³rica.\n\n\ncat_emb_dim\nDimensiones de embedding por variable categÃ³rica. Default: 1.\n\n\nn_shared / n_independent\nCapas GLU compartidas e independientes por paso. Usual: 1â€“5.\n\n\nepsilon\nEstabilidad numÃ©rica. No modificar (default 1e-15).\n\n\nseed\nSemilla para reproducibilidad.\n\n\nmomentum\nMomentum para batch norm. Rango tÃ­pico: 0.01â€“0.4. Default: 0.02.\n\n\nclip_value\nValor para gradient clipping. Default: None.\n\n\nlambda_sparse\nCoeficiente para pÃ©rdida de dispersiÃ³n. Mayor â†’ mÃ¡s enmascaramiento. Default: 1e-3.\n\n\noptimizer_fn\nOptimizador de PyTorch. Default: torch.optim.Adam.\n\n\noptimizer_params\nParÃ¡metros del optimizador. Default: {'lr': 2e-2}.\n\n\nscheduler_fn / scheduler_params\nScheduler de tasa de aprendizaje y sus parÃ¡metros. Ej: {\"gamma\": 0.95, \"step_size\": 10}.\n\n\nmodel_name\nNombre del modelo para guardar. Default: \"DreamQuarkTabNet\".\n\n\nverbose\nNivel de verbosidad. 0 (silencio) o 1 (mostrar Ã©pocas).\n\n\ndevice_name\n\"cpu\", \"cuda\" o \"auto\".\n\n\nmask_type\nTipo de mÃ¡scara: \"sparsemax\" o \"entmax\".\n\n\ngrouped_features\nAgrupaciÃ³n de variables correlacionadas (ej. PCA o TF-IDF).\n\n\nn_shared_decoder / n_indep_decoder\nSolo para preentrenamiento. Bloques GLU compartidos/independientes en decoder.\n\n\n\n\n\nParÃ¡metros de entrenamiento (fit)\n\n\n\n\n\n\n\nParÃ¡metro\nDescripciÃ³n\n\n\n\n\nX_train, y_train\nDatos y etiquetas de entrenamiento.\n\n\neval_set\nLista de tuplas (X, y) para evaluaciÃ³n. La Ãºltima se usa para early stopping.\n\n\neval_name\nNombres para los conjuntos de evaluaciÃ³n.\n\n\neval_metric\nLista de mÃ©tricas (ej. \"accuracy\", \"auc\", \"rmse\"). La Ãºltima se usa para early stopping.\n\n\nmax_epochs\nNÃºmero mÃ¡ximo de Ã©pocas. Default: 200.\n\n\npatience\nÃ‰pocas sin mejora antes de detener. Default: 10.\n\n\nweights\nSolo para clasificaciÃ³n. 0: sin ponderaciÃ³n, 1: ponderaciÃ³n automÃ¡tica, dict: pesos por clase.\n\n\nloss_fn\nFunciÃ³n de pÃ©rdida. Default: mse (regresiÃ³n) o cross_entropy (clasificaciÃ³n).\n\n\nbatch_size\nTamaÃ±o del batch. Sugerido: grande (ej. 1024).\n\n\nvirtual_batch_size\nPara Ghost Batch Norm. Debe dividir batch_size. Default: 128.\n\n\nnum_workers\nNÃºmero de workers para DataLoader.\n\n\ndrop_last\nSi se debe descartar el Ãºltimo batch incompleto.\n\n\ncallbacks\nLista de callbacks personalizados.\n\n\npretraining_ratio\nSolo TabNetPretrainer: proporciÃ³n de caracterÃ­sticas a enmascarar (entre 0 y 1).\n\n\nwarm_start\nPermite continuar entrenamiento previo.\n\n\ncompute_importance\nSi se deben calcular importancias de variables. Default: True."
  },
  {
    "objectID": "semana_3/notebooks/Nb_3a_Introduccion_transfer_learning_finetuning.html#conclusiones",
    "href": "semana_3/notebooks/Nb_3a_Introduccion_transfer_learning_finetuning.html#conclusiones",
    "title": "IntroducciÃ³n a pre-trained models, transfer learning and fine tuning",
    "section": "Conclusiones",
    "text": "Conclusiones\n\nNote que al agregarle al modelo mas capas superiores, logramos mitigar el problema que presentamos en nuestro modelo 2, logrando un accuracy del 99.30% en nuestro datos de prueba.\nComo conlusiÃ³n, cuando hagamos uso de modelo pre-entrenados. Tenemos que hacer uso de todas las herramientas que disponemos, como lo son el transfer learning y el fine tunning, una caracteristica muy importante que siempre hay que aplicar.\nEn el modelo de clasificaciÃ³n de texto usando el mÃ©todo manual, logrÃ³ una mejora significativa usando los embeddings de FastText y ajustandolo a los datos. Sin embargo, tiende a sobre entrenar (algo con lo que se debe luchar aplicando tÃ©cnicas de regularizaciÃ³n). Por otro lado, vemos que con el mÃ©todo a nivel de arquitectura (que es como funcionan los modelos hoy en dÃ­a) ya es mucho mÃ¡s directo pero el nivel de computaciÃ³n requerido es mayor y funcionan mejor con una mayor cantidad de datos."
  }
]