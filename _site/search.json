[
  {
    "objectID": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html",
    "href": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html",
    "title": "Introducción a la explicabilidad e interpretabilidad en modelos",
    "section": "",
    "text": "Open In Colab\nEn este notebook encontrarás material introductorio para entender los conceptos de expicabilidad e interpretabilidad en modelos de inteligencia artificial.\nAbordaremos el siguiente paso a paso:\n#@title Importar librerías\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Input\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets\nfrom tensorflow.keras.preprocessing import image\n#@title Importar functions\ndef show_images(image_titles, images, cmap='viridis'):\n    f, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n    for i, title in enumerate(image_titles):\n        ax[i].set_title(title, fontsize=16)\n        ax[i].imshow(images[i], cmap=cmap)\n        ax[i].axis('off')\n    plt.tight_layout()\n    plt.show()\n\ndef show_images_with_heatmap(image_titles, images, cam, cmap='viridis'):\n    f, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n    for i, title in enumerate(image_titles):\n        heatmap = np.uint8(cm.jet(cam[i])[..., :3] * 255)\n        ax[i].set_title(title, fontsize=16)\n        ax[i].imshow(images[i], cmap=cmap)\n        ax[i].imshow(heatmap, cmap='jet', alpha=0.5)  # overlay\n        ax[i].axis('off')\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html#explicabilidad-en-imágenes",
    "href": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html#explicabilidad-en-imágenes",
    "title": "Introducción a la explicabilidad e interpretabilidad en modelos",
    "section": "Explicabilidad en Imágenes",
    "text": "Explicabilidad en Imágenes\n\n!pip install tf-keras-vis tensorflow\n\n\nCollecting tf-keras-vis\n\n  Downloading tf_keras_vis-0.8.7-py3-none-any.whl.metadata (10 kB)\n\nRequirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from tf-keras-vis) (1.16.0)\n\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from tf-keras-vis) (11.3.0)\n\nCollecting deprecated (from tf-keras-vis)\n\n  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n\nRequirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (from tf-keras-vis) (2.37.0)\n\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tf-keras-vis) (25.0)\n\nRequirement already satisfied: absl-py&gt;=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n\nRequirement already satisfied: astunparse&gt;=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n\nRequirement already satisfied: flatbuffers&gt;=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,&gt;=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n\nRequirement already satisfied: google-pasta&gt;=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n\nRequirement already satisfied: libclang&gt;=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n\nRequirement already satisfied: opt-einsum&gt;=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,&lt;6.0.0dev,&gt;=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n\nRequirement already satisfied: requests&lt;3,&gt;=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n\nRequirement already satisfied: six&gt;=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n\nRequirement already satisfied: termcolor&gt;=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n\nRequirement already satisfied: typing-extensions&gt;=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n\nRequirement already satisfied: wrapt&gt;=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n\nRequirement already satisfied: grpcio&lt;2.0,&gt;=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.74.0)\n\nRequirement already satisfied: tensorboard&lt;2.19,&gt;=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n\nRequirement already satisfied: keras&gt;=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n\nRequirement already satisfied: numpy&lt;2.1.0,&gt;=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n\nRequirement already satisfied: h5py&gt;=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n\nRequirement already satisfied: ml-dtypes&lt;0.5.0,&gt;=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n\nRequirement already satisfied: tensorflow-io-gcs-filesystem&gt;=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n\nRequirement already satisfied: wheel&lt;1.0,&gt;=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse&gt;=1.6.0-&gt;tensorflow) (0.45.1)\n\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras&gt;=3.5.0-&gt;tensorflow) (13.9.4)\n\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras&gt;=3.5.0-&gt;tensorflow) (0.1.0)\n\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras&gt;=3.5.0-&gt;tensorflow) (0.17.0)\n\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.11/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow) (3.4.2)\n\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.11/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow) (3.10)\n\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow) (2.5.0)\n\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow) (2025.7.14)\n\nRequirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard&lt;2.19,&gt;=2.18-&gt;tensorflow) (3.8.2)\n\nRequirement already satisfied: tensorboard-data-server&lt;0.8.0,&gt;=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard&lt;2.19,&gt;=2.18-&gt;tensorflow) (0.7.2)\n\nRequirement already satisfied: werkzeug&gt;=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard&lt;2.19,&gt;=2.18-&gt;tensorflow) (3.1.3)\n\nRequirement already satisfied: MarkupSafe&gt;=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug&gt;=1.0.1-&gt;tensorboard&lt;2.19,&gt;=2.18-&gt;tensorflow) (3.0.2)\n\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich-&gt;keras&gt;=3.5.0-&gt;tensorflow) (3.0.0)\n\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich-&gt;keras&gt;=3.5.0-&gt;tensorflow) (2.19.2)\n\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras&gt;=3.5.0-&gt;tensorflow) (0.1.2)\n\nDownloading tf_keras_vis-0.8.7-py3-none-any.whl (52 kB)\n\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.5/52.5 kB 3.0 MB/s eta 0:00:00\n\nDownloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n\nInstalling collected packages: deprecated, tf-keras-vis\n\nSuccessfully installed deprecated-1.2.18 tf-keras-vis-0.8.7\n\n\n\n\nEl primer paso es tener listo nuestra red convolucional, pre-cargada o entrenada desde cero.\n\n# Primero cargamos el modelo\nmodel =tf.keras.applications.vgg16.VGG16(weights='imagenet', include_top=True)\nmodel.summary()\n\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n\n553467096/553467096 ━━━━━━━━━━━━━━━━━━━━ 2s 0us/step\n\n\n\n\nModel: \"vgg16\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (InputLayer)        │ (None, 224, 224, 3)    │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_conv1 (Conv2D)           │ (None, 224, 224, 64)   │         1,792 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_conv2 (Conv2D)           │ (None, 224, 224, 64)   │        36,928 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_pool (MaxPooling2D)      │ (None, 112, 112, 64)   │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_conv1 (Conv2D)           │ (None, 112, 112, 128)  │        73,856 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_conv2 (Conv2D)           │ (None, 112, 112, 128)  │       147,584 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_pool (MaxPooling2D)      │ (None, 56, 56, 128)    │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv1 (Conv2D)           │ (None, 56, 56, 256)    │       295,168 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv2 (Conv2D)           │ (None, 56, 56, 256)    │       590,080 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv3 (Conv2D)           │ (None, 56, 56, 256)    │       590,080 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_pool (MaxPooling2D)      │ (None, 28, 28, 256)    │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block4_conv1 (Conv2D)           │ (None, 28, 28, 512)    │     1,180,160 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block4_conv2 (Conv2D)           │ (None, 28, 28, 512)    │     2,359,808 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block4_conv3 (Conv2D)           │ (None, 28, 28, 512)    │     2,359,808 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block4_pool (MaxPooling2D)      │ (None, 14, 14, 512)    │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block5_conv1 (Conv2D)           │ (None, 14, 14, 512)    │     2,359,808 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block5_conv2 (Conv2D)           │ (None, 14, 14, 512)    │     2,359,808 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block5_conv3 (Conv2D)           │ (None, 14, 14, 512)    │     2,359,808 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block5_pool (MaxPooling2D)      │ (None, 7, 7, 512)      │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (Flatten)               │ (None, 25088)          │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1 (Dense)                     │ (None, 4096)           │   102,764,544 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2 (Dense)                     │ (None, 4096)           │    16,781,312 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ predictions (Dense)             │ (None, 1000)           │     4,097,000 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 138,357,544 (527.79 MB)\n\n\n\n Trainable params: 138,357,544 (527.79 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# Cargar imágenes\n# Diccionario de nombres y URLs públicas\nimage_info = {\n    'goldfish.jpg': 'https://images.unsplash.com/photo-1668862347626-70a980820f06?q=80&w=1170&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D',\n    'bear.jpg': 'https://plus.unsplash.com/premium_photo-1664298010187-091137b9e296?q=80&w=687&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D',\n    'soldier.jpg': 'https://plus.unsplash.com/premium_photo-1683133493443-0eee93bfe24f?q=80&w=1170&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D'\n}\n\nimage_titles = ['goldfish', 'bear', 'Assault rifle']\nloaded_images = []\n\nfor name, url in image_info.items():\n    path = tf.keras.utils.get_file(name, origin=url)\n    img = image.load_img(path, target_size=(224, 224))\n    loaded_images.append(np.array(img))\n\n# convertir a numpy y organizarlas en un batch\nimages = np.asarray(loaded_images)\n\n# Preparar la imágenes como entrada para VGG16\nX = tf.keras.applications.vgg16.preprocess_input(images)\n\n# Mostrar imágenes\nshow_images(image_titles, images, cmap='viridis')\n\n\n\n\n\n\n\n\n📌 Nota sobre la función de activación en la última capa.\nCuando se aplica la función de activación softmax en la última capa del modelo, esto puede interferir con la generación de mapas de atención (attention maps). Por esta razón, es recomendable reemplazarla por una función de activación lineal (es decir, sin activación).\nEn este ejemplo usamos una clase llamada ReplaceToLinear para hacer esa sustitución automáticamente. Sin embargo, también es posible definir nuestra propia función modificadora del modelo si se desea más control.\n\nfrom tf_keras_vis.utils.model_modifiers import ReplaceToLinear\n\nreplace2linear = ReplaceToLinear()\n\n# También es posible definir una función totalmente desde cero\ndef model_modifier_function(cloned_model):\n    cloned_model.layers[-1].activation = tf.keras.activations.linear\n\n🎯 Definir la función de puntuación (score)\nLuego, es necesario crear una instancia de Score o definir una función personalizada de puntuación que devuelva los valores objetivo (scores) del modelo.\nEn este caso, la función devuelve las puntuaciones correspondientes a las clases de interés: Goldfish, Bear y Assault Rifle.\nEsto es fundamental para indicar al método de visualización (como Grad-CAM) qué clase específica queremos analizar en la imagen.\n\nfrom tf_keras_vis.utils.scores import CategoricalScore\n\n# clase 1 es el index de imagenet correspondiente a Goldfish\n# Clase 294 a Bear\n# Clase 413 a Assault Rifle\n\nscore = CategoricalScore([1, 294, 413])\n\n# Para definir un score también es posible hacerlo desde cero\ndef score_function(output):\n    # La `output` refiere a la salida del modelo,\n    # En este caso, `output` es `(3, 1000)`\n    return (output[0][1], output[1][294], output[2][413])\n\n🧠 ¿Qué es un Saliency Map?\nSaliency genera un saliency map (mapa de saliencia) que resalta las regiones de la imagen de entrada que más contribuyen al valor de salida del modelo.\nEn otras palabras, muestra qué partes de la imagen fueron más importantes para que el modelo tomara su decisión (por ejemplo, clasificar como “oso” o “pez dorado”).\nEste tipo de visualización es útil para interpretar y explicar el comportamiento de redes neuronales profundas.\n\nfrom tf_keras_vis.saliency import Saliency\n\n# Crear un objeto de saliencia.\nsaliency = Saliency(model, model_modifier=replace2linear, clone=True)\n\n# Generar el mapa de saliencia\nsaliency_map = saliency(score, X)\n\n# Mostrar imágenes\nshow_images(image_titles, saliency_map, cmap='jet')\n\n/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\nExpected: keras_tensor\nReceived: inputs=['Tensor(shape=(3, 224, 224, 3))']\n  warnings.warn(msg)\n\n\n\n\n\n\n\n\n\n🧹 Mejora del Saliency Map con SmoothGrad\nComo puedes ver en el ejemplo anterior, el mapa de saliencia (Vanilla Saliency) suele ser muy ruidoso. Para mejorar su claridad, usaremos SmoothGrad, una técnica que reduce el ruido agregando pequeñas perturbaciones (ruido) a la imagen de entrada y promediando los mapas resultantes.\nEsto permite obtener un mapa de saliencia más suave y más interpretable.\n📌 Nota: SmoothGrad necesita calcular el gradiente muchas veces, por lo que puede tardar 1 a 3 minutos si se usa CPU. Se recomienda ejecutar en GPU si es posible.\n\n# Generate saliency map with smoothing that reduce noise by adding noise\nsaliency_map = saliency(\n    score,\n    X,\n    smooth_samples=20,  # El número de iteraciones para calcular los gradientes.\n    smooth_noise=0.20)  # nivel de ruido aplicado.\n\n# Mostrar imágenes\nshow_images(image_titles, saliency_map, cmap='jet')\n\n\n\n\n\n\n\n\n👀 Visualización de la atención: Grad-CAM\nSaliency es una técnica útil para visualizar la atención del modelo, ya que muestra qué regiones de la imagen de entrada contribuyen más al valor de salida (por ejemplo, a la clasificación final).\nOtra forma popular de visualizar la atención es Grad-CAM (Gradient-weighted Class Activation Mapping).\nA diferencia de Saliency, que utiliza directamente los gradientes del valor de salida, Grad-CAM se basa en la salida de la capa convolucional previa a las capas densas (también conocida como penultimate layer). Esto permite generar mapas de atención más localizados y visualmente interpretables, sobre todo en modelos CNN.\n\nfrom matplotlib import cm\nfrom tf_keras_vis.gradcam import Gradcam\n\n# Crear el objeto GRAD-CAM\ngradcam = Gradcam(model, model_modifier=replace2linear, clone=True)\n\n# Generar un mapa de calor con GradCAM\ncam = gradcam(score, X, penultimate_layer=-1)\n\n# Mostrar imágenes\nshow_images_with_heatmap(image_titles, images, cam, cmap='jet')\n\n\n\n\n\n\n\n\n🔍 Grad-CAM++: Atención mejorada\nGrad-CAM++ es una extensión de Grad-CAM que puede proporcionar explicaciones visuales más precisas y detalladas sobre las predicciones de un modelo CNN.\nEsta técnica mejora la localización de las regiones importantes, especialmente en situaciones donde hay múltiples objetos o detalles finos en la imagen. Es útil cuando se desea una interpretación más fina que la que ofrece Grad-CAM estándar.\n\nfrom tf_keras_vis.gradcam_plus_plus import GradcamPlusPlus\n\n# Crear el objeto GradCAM++\ngradcam = GradcamPlusPlus(model, model_modifier=replace2linear, clone=True)\n\n# Generar el mapa con GradCAM++\ncam = gradcam(score, X, penultimate_layer=-1)\n\n# Mostrar imágenes\nshow_images_with_heatmap(image_titles, images, cam, cmap='jet')\n\n\n\n\n\n\n\n\n⚡️ Faster-ScoreCAM\nScoreCAM es un método muy eficaz para visualizar la atención de los modelos, pero suele ser más lento que otras variantes de CAM (como Grad-CAM o Grad-CAM++).\nLa buena noticia es que existe una versión optimizada llamada Faster-ScoreCAM, que mejora el rendimiento de ScoreCAM sin perder precisión.\n🧠 La idea detrás de Faster-ScoreCAM es que solo algunos canales del mapa de activaciones son realmente relevantes para generar el mapa final. Por eso, Faster-ScoreCAM usa solo los canales con mayor varianza como máscaras, lo que reduce significativamente el tiempo de procesamiento.\n🔧 Nota técnica: Establecer max_N = -1 utiliza el comportamiento original de Score-CAM (es decir, sin filtrado por varianza).\n\nfrom tf_keras_vis.scorecam import Scorecam\n\n# Create ScoreCAM object\nscorecam = Scorecam(model, model_modifier=replace2linear)\n\n# Crear el mapa de calor con Faster-ScoreCAM\ncam = scorecam(score, X, penultimate_layer=-1, max_N=10)\n\n# Mostrar imágenes\nshow_images_with_heatmap(image_titles, images, cam, cmap='jet')\n\n/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\nExpected: keras_tensor\nReceived: inputs=('Tensor(shape=(32, 224, 224, 3))',)\n  warnings.warn(msg)\n\n\n\n2/3 ━━━━━━━━━━━━━━━━━━━━ 0s 159ms/step\n\n\n\n/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\nExpected: keras_tensor\nReceived: inputs=('Tensor(shape=(None, 224, 224, 3))',)\n  warnings.warn(msg)\nWARNING:tensorflow:5 out of the last 54 calls to &lt;function TensorFlowTrainer.make_predict_function.&lt;locals&gt;.one_step_on_data_distributed at 0x795ca4526e80&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n\n\n\n3/3 ━━━━━━━━━━━━━━━━━━━━ 1s 373ms/step"
  },
  {
    "objectID": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html#explicabilidad-en-texto-y-datos-tabulares",
    "href": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html#explicabilidad-en-texto-y-datos-tabulares",
    "title": "Introducción a la explicabilidad e interpretabilidad en modelos",
    "section": "Explicabilidad en texto y datos tabulares",
    "text": "Explicabilidad en texto y datos tabulares\n\n!pip install -q lime\n\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/275.7 kB ? eta -:--:--\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━ 266.2/275.7 kB 11.0 MB/s eta 0:00:01\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 275.7/275.7 kB 5.6 MB/s eta 0:00:00\n\n  Preparing metadata (setup.py) ... done\n\n  Building wheel for lime (setup.py) ... done\n\n\n\n\n\n#importamos las librerias necesarias a utilizar para explicabilidad\nimport shap\nfrom lime.lime_tabular import LimeTabularExplainer\n\n%matplotlib inline"
  },
  {
    "objectID": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html#dataset",
    "href": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html#dataset",
    "title": "Introducción a la explicabilidad e interpretabilidad en modelos",
    "section": "Dataset",
    "text": "Dataset\nEl dataset cuenta con 178 registros, cada uno con 13 caracteristicas:\n\nAlcohol\nMalic Acid\nAsh\nAlcalinity of Ash\nMagnesium\nTotal Phenols\nFlavanoids\nNonflavanoid Phenols\nProanthocyanins\nColour Intensity\nHue\nOD280/OD315 of diluted wines\nProline\n\nEl dataset contiene 3 clases diferentes: Class_1, Class_2,Class_3\n\n#Cargamos el conjunto de datos y procesamos\nwine = load_wine()\nX, y = wine.data, wine.target\n\n# dividimos los datos\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size=0.3,\n                                                    random_state=42,\n                                                    stratify=y)\n\n# convertimos y a one hot encoded vector\none_hot_encoder = OneHotEncoder(sparse_output=False)\ny_train = one_hot_encoder.fit_transform(y_train.reshape(-1, 1))\ny_test = one_hot_encoder.transform(y_test.reshape(-1, 1))\n\n# escalamos los datos\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nprint(\"Dimension datos de entrenamiento: \", X_train.shape)\nprint(\"Dimension datos de prueba: \", X_test.shape)\n\nDimension datos de entrenamiento:  (124, 13)\nDimension datos de prueba:  (54, 13)"
  },
  {
    "objectID": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html#modelo",
    "href": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html#modelo",
    "title": "Introducción a la explicabilidad e interpretabilidad en modelos",
    "section": "Modelo",
    "text": "Modelo\n\n#Definimos la red neuronal a entrenar y compilamos el modelo\nmodel = Sequential()\n\nmodel.add(Input(shape=(X_train.shape[1],)))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(y_train.shape[1], activation='softmax'))\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n\n#Entrenamos la red\nhistory = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=1)\n\n\nEpoch 1/20\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 3s 363ms/step - accuracy: 0.4157 - loss: 1.0442 - val_accuracy: 0.7200 - val_loss: 0.8392\n\nEpoch 2/20\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 1s 20ms/step - accuracy: 0.5802 - loss: 0.9172 - val_accuracy: 0.8800 - val_loss: 0.7460\n\nEpoch 3/20\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - accuracy: 0.5664 - loss: 0.9099 - val_accuracy: 0.9600 - val_loss: 0.6597\n\nEpoch 4/20\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - accuracy: 0.7142 - loss: 0.7492 - val_accuracy: 0.9600 - val_loss: 0.5856\n\nEpoch 5/20\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step - accuracy: 0.8364 - loss: 0.6740 - val_accuracy: 0.9600 - val_loss: 0.5215\n\nEpoch 6/20\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - accuracy: 0.7955 - loss: 0.6507 - val_accuracy: 0.9600 - val_loss: 0.4634\n\nEpoch 7/20\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.8352 - loss: 0.6070 - val_accuracy: 0.9600 - val_loss: 0.4121\n\nEpoch 8/20\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 39ms/step - accuracy: 0.9290 - loss: 0.5245 - val_accuracy: 0.9600 - val_loss: 0.3665\n\nEpoch 9/20\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - accuracy: 0.8888 - loss: 0.5035 - val_accuracy: 0.9600 - val_loss: 0.3268\n\nEpoch 10/20\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 39ms/step - accuracy: 0.8951 - loss: 0.4608 - val_accuracy: 0.9600 - val_loss: 0.2910\n\nEpoch 11/20\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 38ms/step - accuracy: 0.9351 - loss: 0.4075 - val_accuracy: 0.9600 - val_loss: 0.2605\n\nEpoch 12/20\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step - accuracy: 0.9570 - loss: 0.3141 - val_accuracy: 0.9600 - val_loss: 0.2323\n\nEpoch 13/20\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.9350 - loss: 0.3289 - val_accuracy: 0.9600 - val_loss: 0.2072\n\nEpoch 14/20\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step - accuracy: 0.9445 - loss: 0.2903 - val_accuracy: 0.9600 - val_loss: 0.1856\n\nEpoch 15/20\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - accuracy: 0.9784 - loss: 0.2655 - val_accuracy: 0.9600 - val_loss: 0.1673\n\nEpoch 16/20\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step - accuracy: 0.9753 - loss: 0.2490 - val_accuracy: 0.9600 - val_loss: 0.1537\n\nEpoch 17/20\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step - accuracy: 0.9691 - loss: 0.2239 - val_accuracy: 0.9600 - val_loss: 0.1403\n\nEpoch 18/20\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step - accuracy: 0.9784 - loss: 0.1995 - val_accuracy: 0.9600 - val_loss: 0.1281\n\nEpoch 19/20\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step - accuracy: 0.9878 - loss: 0.1343 - val_accuracy: 0.9600 - val_loss: 0.1183\n\nEpoch 20/20\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step - accuracy: 0.9753 - loss: 0.1496 - val_accuracy: 0.9600 - val_loss: 0.1111\n\n\n\n\n\ny_pred = model.predict(X_test)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_test_classes = np.argmax(y_test, axis=1)\n\naccuracy = accuracy_score(y_test_classes, y_pred_classes)\nprint(f\"Accuracy en el conjunto de prueba: {accuracy:.4f}\")\n\n\n2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 123ms/step\n\nAccuracy en el conjunto de prueba: 1.0000\n\n\n\n\n\nplt.figure(figsize=(6, 3))\nplt.plot(history.history['accuracy'], label='accuray en entrenamiento')\nplt.plot(history.history['val_accuracy'], label='accuracy de validación')\nplt.xlabel('épocas')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('Rendimiento del modelo durante el entrenamiento')\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nYa tenemos un MLP o red densa que nos predice la clase del vino bastante bien. Sin embargo, lo que no está muy claro es cómo cada una de estas características contribuye a la probabilidad de clase de vino predicha. Podemos pensar en estas explicaciones en términos globales (es decir, ¿cómo impacta cada característica en los resultados en promedio para todo los datos?) o en términos locales (es decir, ¿cómo impacta cada característica en las predicciones para una muestra en específico?).\nAlgunos modelos tienen propiedades incorporadas que proporcionan este tipo de explicaciones. Estos se conocen típicamente como modelos de caja blanca (white-box) y los ejemplos incluyen la regresión lineal (coeficientes del modelo), la regresión logística (coeficientes del modelo) y los árboles de decisión (importancia de las características). Debido a su complejidad, otros modelos, como las Máquinas de Vectores de Soporte (SVM) y las Redes Neuronales (incluyendo nuestro Perceptrón Multicapa) etc., no tienen métodos directos para explicar sus predicciones. Para estos modelos (también conocidos como modelos de caja negra (black-box)), se pueden aplicar enfoques como LIME y SHAP."
  },
  {
    "objectID": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html#lime-para-datos-tabulares",
    "href": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html#lime-para-datos-tabulares",
    "title": "Introducción a la explicabilidad e interpretabilidad en modelos",
    "section": "LIME para datos tabulares",
    "text": "LIME para datos tabulares\nLocal Interpretable Model-agnostic Explanation (LIME) proporciona un método rápido y relativamente simple para explicar localmente modelos de caja negra. El algoritmo LIME se puede simplificar en unos pocos pasos:\n\nPara un punto de datos dado, perturba aleatoriamente sus características repetidamente. Para datos tabulares, esto implica agregar una pequeña cantidad de ruido a cada característica.\nObtén predicciones para cada instancia de datos perturbada. Esto nos ayuda a construir una imagen local de la superficie de decisión en ese punto.\nUsa las predicciones para calcular un “modelo de explicación” lineal aproximado utilizando las predicciones. Los coeficientes del modelo lineal se utilizan como explicaciones.\n\nLa librería de Python LIME proporciona interfaces para explicar modelos construidos sobre datos tabulares (TabularExplainer), imágenes (LimeImageExplainer) y texto (LimeTextExplainer).\nEn la siguiente sección, intentaremos explicar las predicciones de una única instancia de datos de prueba utilizando LimeTabularExplainer\n\ndata_df = pd.DataFrame(X_train,\n                      columns=wine.feature_names)\n\nlime_explainer = LimeTabularExplainer(training_data=data_df.values,\n                                      feature_names=list(data_df.columns),\n                                      class_names = ['Class_1', 'Class_2', 'Class_3'],\n                                      # básicamente no tenemos datos\n                                      # categóricos\n                                      categorical_features=[],\n                                      mode=\"classification\")\n\n\nindex = 9\nexplanation = lime_explainer.explain_instance(X_test[index],\n                                              model.predict,\n                                              num_features=len(wine.feature_names),\n                                              top_labels=2)\n\nprint(f\"Explicación para la muestra con índice: {index} con clase verdadera: class_{np.argmax(y_test[index])+1}\")\nexplanation.show_in_notebook(show_table=True, show_all=True,)\n\n\n157/157 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step\n\nExplicación para la muestra con índice: 9 con clase verdadera: class_3\n\n\n\n\n\n        \n        \n        \n        \n        \n        \n\n\nNOTA: Esta nota es para recordar que el rendimiento de este algoritmo no es plenamente confiable ya que se basa en que tanto el modelo lineal que LIME crea pueda explicar el modelo NO LINEAL original. Así que, las capacidades de explicabilidad y confianza son limitadas. En la vida real, habrá que explorar crear mejores aproximaciones. Sin embargo seguirá siendo una limitante grande.\nPara tratar con este problema, es posible hacer uso de algunos atributos de LIME:\nexplanation.intercept\nexplanation.local_pred\nexplanation.local_exp\nexplanation.score\nexplanation.top_labels\n\nhelp(LimeTabularExplainer)\n\nHelp on class LimeTabularExplainer in module lime.lime_tabular:\n\nclass LimeTabularExplainer(builtins.object)\n |  LimeTabularExplainer(training_data, mode='classification', training_labels=None, feature_names=None, categorical_features=None, categorical_names=None, kernel_width=None, kernel=None, verbose=False, class_names=None, feature_selection='auto', discretize_continuous=True, discretizer='quartile', sample_around_instance=False, random_state=None, training_data_stats=None)\n |  \n |  Explains predictions on tabular (i.e. matrix) data.\n |  For numerical features, perturb them by sampling from a Normal(0,1) and\n |  doing the inverse operation of mean-centering and scaling, according to the\n |  means and stds in the training data. For categorical features, perturb by\n |  sampling according to the training distribution, and making a binary\n |  feature that is 1 when the value is the same as the instance being\n |  explained.\n |  \n |  Methods defined here:\n |  \n |  __init__(self, training_data, mode='classification', training_labels=None, feature_names=None, categorical_features=None, categorical_names=None, kernel_width=None, kernel=None, verbose=False, class_names=None, feature_selection='auto', discretize_continuous=True, discretizer='quartile', sample_around_instance=False, random_state=None, training_data_stats=None)\n |      Init function.\n |      \n |      Args:\n |          training_data: numpy 2d array\n |          mode: \"classification\" or \"regression\"\n |          training_labels: labels for training data. Not required, but may be\n |              used by discretizer.\n |          feature_names: list of names (strings) corresponding to the columns\n |              in the training data.\n |          categorical_features: list of indices (ints) corresponding to the\n |              categorical columns. Everything else will be considered\n |              continuous. Values in these columns MUST be integers.\n |          categorical_names: map from int to list of names, where\n |              categorical_names[x][y] represents the name of the yth value of\n |              column x.\n |          kernel_width: kernel width for the exponential kernel.\n |              If None, defaults to sqrt (number of columns) * 0.75\n |          kernel: similarity kernel that takes euclidean distances and kernel\n |              width as input and outputs weights in (0,1). If None, defaults to\n |              an exponential kernel.\n |          verbose: if true, print local prediction values from linear model\n |          class_names: list of class names, ordered according to whatever the\n |              classifier is using. If not present, class names will be '0',\n |              '1', ...\n |          feature_selection: feature selection method. can be\n |              'forward_selection', 'lasso_path', 'none' or 'auto'.\n |              See function 'explain_instance_with_data' in lime_base.py for\n |              details on what each of the options does.\n |          discretize_continuous: if True, all non-categorical features will\n |              be discretized into quartiles.\n |          discretizer: only matters if discretize_continuous is True\n |              and data is not sparse. Options are 'quartile', 'decile',\n |              'entropy' or a BaseDiscretizer instance.\n |          sample_around_instance: if True, will sample continuous features\n |              in perturbed samples from a normal centered at the instance\n |              being explained. Otherwise, the normal is centered on the mean\n |              of the feature data.\n |          random_state: an integer or numpy.RandomState that will be used to\n |              generate random numbers. If None, the random state will be\n |              initialized using the internal numpy seed.\n |          training_data_stats: a dict object having the details of training data\n |              statistics. If None, training data information will be used, only matters\n |              if discretize_continuous is True. Must have the following keys:\n |              means\", \"mins\", \"maxs\", \"stds\", \"feature_values\",\n |              \"feature_frequencies\"\n |  \n |  explain_instance(self, data_row, predict_fn, labels=(1,), top_labels=None, num_features=10, num_samples=5000, distance_metric='euclidean', model_regressor=None)\n |      Generates explanations for a prediction.\n |      \n |      First, we generate neighborhood data by randomly perturbing features\n |      from the instance (see __data_inverse). We then learn locally weighted\n |      linear models on this neighborhood data to explain each of the classes\n |      in an interpretable way (see lime_base.py).\n |      \n |      Args:\n |          data_row: 1d numpy array or scipy.sparse matrix, corresponding to a row\n |          predict_fn: prediction function. For classifiers, this should be a\n |              function that takes a numpy array and outputs prediction\n |              probabilities. For regressors, this takes a numpy array and\n |              returns the predictions. For ScikitClassifiers, this is\n |              `classifier.predict_proba()`. For ScikitRegressors, this\n |              is `regressor.predict()`. The prediction function needs to work\n |              on multiple feature vectors (the vectors randomly perturbed\n |              from the data_row).\n |          labels: iterable with labels to be explained.\n |          top_labels: if not None, ignore labels and produce explanations for\n |              the K labels with highest prediction probabilities, where K is\n |              this parameter.\n |          num_features: maximum number of features present in explanation\n |          num_samples: size of the neighborhood to learn the linear model\n |          distance_metric: the distance metric to use for weights.\n |          model_regressor: sklearn regressor to use in explanation. Defaults\n |              to Ridge regression in LimeBase. Must have model_regressor.coef_\n |              and 'sample_weight' as a parameter to model_regressor.fit()\n |      \n |      Returns:\n |          An Explanation object (see explanation.py) with the corresponding\n |          explanations.\n |  \n |  ----------------------------------------------------------------------\n |  Static methods defined here:\n |  \n |  convert_and_round(values)\n |  \n |  validate_training_data_stats(training_data_stats)\n |      Method to validate the structure of training data stats\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables\n |  \n |  __weakref__\n |      list of weak references to the object\n\n\n\nEsta idea de aplicar perturbaciones y generar explicaciones locales, ha sido extendida a texto e imágenes:\nLIME para texto: LIME para Imágenes"
  },
  {
    "objectID": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html#shap-para-datos-tabulares",
    "href": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html#shap-para-datos-tabulares",
    "title": "Introducción a la explicabilidad e interpretabilidad en modelos",
    "section": "SHAP para datos tabulares",
    "text": "SHAP para datos tabulares\nSitio oficial\nImagina esta situación: Tenemos a tres brillantes científicos de datos (llamémoslos Ana, Luis y Sofía) trabajando juntos en un proyecto para la empresa. Al final del año, su increíble modelo predictivo ha generado un aumento de ganancias de 5 millones de euros. Ahora, la pregunta es: ¿cómo repartimos estos 5 millones de manera justa entre Ana, Luis y Sofía, de acuerdo con la contribución real de cada uno al éxito del modelo?\nSHAP (SHapley Additive exPlanations) nos da una manera de hacer precisamente esto, pero en lugar de científicos de datos y ganancias, piensa en las características de tus datos y la predicción de tu modelo.\nLa Idea Central: El Valor de Shapley\nLa base de SHAP son los llamados “valores de Shapley”. Estos valores vienen de un área de las matemáticas que estudia cómo repartir justamente las ganancias en juegos de colaboración entre varios jugadores. SHAP toma esta idea y la aplica a las características de tu modelo.\n¿Cómo se calcula este “valor justo” para cada característica?\nImagina que vamos añadiendo las características a nuestro modelo una por una, en todos los órdenes posibles, y observamos cuánto cambia la predicción en cada paso.\nPor ejemplo, imágina en un servicio de subscripción por cable y pensemos en predecir si un cliente va a abandonar el servicio de cable, basándonos en tres características: su antigüedad, sus cargos mensuales y si tiene fibra óptica.\nPrimero, podríamos usar solo la antigüedad para hacer una predicción (aunque probablemente no sería muy buena). Anotamos esta predicción. Luego, añadimos los cargos mensuales a la antigüedad y vemos cómo cambia la predicción. La diferencia entre la nueva predicción y la anterior sería la “contribución marginal” de los cargos mensuales en este orden específico. Finalmente, añadimos si tiene fibra óptica a las dos anteriores y vemos el cambio en la predicción. Esa sería la contribución marginal de la fibra óptica en este orden. Pero, ¡el orden importa! Podríamos haber empezado añadiendo la fibra óptica primero, luego la antigüedad y después los cargos mensuales, y la “contribución” de cada característica en cada paso podría ser diferente.\nEl Truco de SHAP: Promediar Todas las Posibilidades\nPara obtener el valor de Shapley de una característica, SHAP hace precisamente esto: calcula la contribución marginal promedio de esa característica en ¡todos los posibles órdenes en los que podríamos haber añadido las características al modelo!\nAplicando esto al Aprendizaje Automático:\nEn el contexto de tu modelo de Deep Learning (como tu Perceptrón Multicapa), SHAP trata cada característica de tus datos como si fuera uno de nuestros científicos de datos, y la predicción del modelo como si fueran las ganancias.\nPara entender cómo contribuye cada característica a una predicción específica, SHAP calcula el valor de Shapley para cada característica de esa instancia. Un valor de Shapley positivo para una característica significa que ese valor de la característica empujó la predicción del modelo hacia un resultado particular (por ejemplo, una mayor probabilidad de abandono). Un valor de Shapley negativo significa que empujó la predicción en la dirección opuesta. Algo parecido a LIME.\n¿Por qué SHAP es especial? Dos Garantías Importantes:\nSHAP tiene dos propiedades muy importantes que lo hacen destacar frente a otras formas de explicar modelos (como LIME o la importancia de las características basada en la permutación):\nPrecisión Local: La suma de los valores de Shapley de todas las características para una predicción específica, más un valor base (la predicción promedio del modelo), debe ser igual a la predicción real del modelo para esa instancia. Esto significa que la explicación local es consistente con la salida del modelo original.\nConsistencia: Si cambias tu modelo de tal manera que una característica tenga un mayor impacto en la predicción en todos los posibles órdenes en los que se podría añadir, entonces su valor de Shapley (su “crédito”) nunca debería disminuir. Esto asegura que las explicaciones sean intuitivas y coherentes con los cambios en el modelo.\nEn la Práctica: Simulando la Eliminación de Características\nUna dificultad práctica es cómo simular la “eliminación” de una característica al calcular su contribución marginal en un modelo ya entrenado. No podemos simplemente quitar una columna de datos y esperar que el modelo siga funcionando.\nLa librería SHAP utiliza una técnica inteligente para abordar esto. Simula la ausencia de una característica reemplazándola con los valores que esa característica toma en un “conjunto de datos de fondo” (background dataset). Este conjunto de datos de fondo representa la distribución “típica” de las características.\nEn resumen, SHAP te da una manera justa y consistente de entender la contribución de cada característica a la predicción de tu modelo para una instancia específica, basándose en la idea de cómo se repartirían las ganancias en un juego colaborativo. Te dice cuánto “responsable” es cada característica del resultado final de la predicción.\n\n#Computemos los valores SHAP de nuestro modelo\nexplainer = shap.Explainer(model, X_train,feature_names=wine.feature_names)\nshap_values = explainer(X_test)\n\n\nPor ejemplo. Para nuestro modelo, la caracteristica que mas aporta para que se prediga de la clase 1, es proline, y la que menos aporta es malic_acid.\n\nPero atención! Esto ya es a nivel de Clase no de explicación de una muestra.\n\nshap.plots.bar(shap_values[:,:,0], max_display=X_test.shape[1])\n\n\n\n\n\n\n\n\n\nPara la clase 2, la caracteristica que mas aporta es alcohol/proline y la que menos aporta a dicha predicción es proanthocyanins.\n\n\nshap.plots.bar(shap_values[:,:,1], max_display=X_test.shape[1])\n\n\n\n\n\n\n\n\n\nPara la tercera clase, la caracteristica mas importante es diluted wines y la menos importante es magnesium\n\n\nshap.plots.bar(shap_values[:,:,2], max_display=X_test.shape[1])\n\n\n\n\n\n\n\n\nPara ver estas contribuciones podemos verlo en un gráfico conjunto para un subconjunto de datos.\n\nshap.initjs()\nshap.summary_plot(shap_values, X_test, feature_names=wine.feature_names,\n                  max_display=10);\n\n\n\n\nFutureWarning: The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.\n  shap.summary_plot(shap_values, X_test, feature_names=wine.feature_names,\n\n\n\n\n\n\n\n\n\n\nTambien podriamos ver para cada clase como aporta cada caracteristica a la predicción del modelo.\n\n\nprint(\"clase 1\")\nshap.plots.beeswarm(shap_values[:,:,0], max_display=X_test.shape[1])\n\nclase 1\n\n\n\n\n\n\n\n\n\n\nprint(\"clase 2\")\nshap.plots.beeswarm(shap_values[:,:,1], max_display=X_test.shape[1])\n\nclase 2\n\n\n\n\n\n\n\n\n\n\nprint(\"clase 3\")\nshap.plots.beeswarm(shap_values[:,:,2], max_display=X_test.shape[1])\n\nclase 3\n\n\n\n\n\n\n\n\n\nAhora si explicaciones más locales por muestra:\n\nAhora tomemos un dato de test y veamos como las caracteristicas influyeron para que la red se inclinara por la categoría cierta.\n\n\nclase = y_test[9]\nprint(\"clase: \", clase)\nshap.plots.bar(shap_values[9,:,np.argmax(clase)], max_display=X_test.shape[1])\n\nclase:  [0. 0. 1.]\n\n\n\n\n\n\n\n\n\nSHAP ha sido adaptado para otros dominios y problemas:\nImágenes\nTexto"
  },
  {
    "objectID": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html#conclusión-lime-vs.-shap-cuándo-usar-cuál",
    "href": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html#conclusión-lime-vs.-shap-cuándo-usar-cuál",
    "title": "Introducción a la explicabilidad e interpretabilidad en modelos",
    "section": "Conclusión: LIME vs. SHAP: ¿Cuándo usar cuál?",
    "text": "Conclusión: LIME vs. SHAP: ¿Cuándo usar cuál?\nTanto LIME como SHAP son buenos métodos para explicar los modelos de aprendizaje automático.\nEn teoría, SHAP es el mejor enfoque porque ofrece garantías matemáticas sobre la precisión y consistencia de sus explicaciones. Esto significa que podemos confiar más en que las explicaciones de SHAP reflejan fielmente cómo funciona el modelo.\nA continuación, mencionamos algunas limitaciones adicionales de ambos métodos:\nLimitaciones de LIME:\n\nNo está diseñado para datos con “one-hot encoding”: El “one-hot encoding” es cuando transformamos variables categóricas (como “color” con valores “rojo”, “azul”, “verde”) en varias columnas binarias (una para “color_rojo”, otra para “color_azul”, etc.). LIME funciona creando pequeñas variaciones (perturbaciones) de tus datos para ver cómo cambia la predicción. Si perturbas una variable “one-hot encoded”, podrías terminar con combinaciones sin sentido (por ejemplo, que una observación sea “rojo” y “azul” al mismo tiempo, o que no sea ningún color), lo que llevaría a explicaciones poco fiables. (Puedes ver una discusión sobre esto aquí).\nDepende de cómo “perturbas” los datos: LIME necesita alterar las muestras de datos de forma que tenga sentido para tu caso específico. Para datos tabulares, esto suele implicar añadir un poco de “ruido” aleatorio a cada característica. Para imágenes, podría significar reemplazar pequeñas regiones de la imagen (superpíxeles) con un color promedio o con ceros. Para texto, podría ser quitar palabras del texto. Es importante pensar si estas formas de perturbar tus datos podrían tener efectos secundarios no deseados que afecten la confianza en las explicaciones.\nEl modelo local de LIME podría no ser un buen reflejo del modelo original: LIME crea un modelo local más simple para explicar una predicción específica. A veces, este modelo local puede no capturar bien el comportamiento del modelo original complejo. Es una buena práctica verificar si hay inconsistencias antes de confiar plenamente en las explicaciones de LIME.\nFunciona mejor con modelos que dan probabilidades: LIME está pensado para modelos de clasificación que predicen la probabilidad de cada clase (por ejemplo, “70% de probabilidad de ser clase A”). Algunos modelos, como las Máquinas de Vectores de Soporte (SVMs), no están diseñados naturalmente para dar probabilidades (aunque se les puede forzar, a veces con problemas). Usar LIME con las “pseudo-probabilidades” de estos modelos podría introducir algún sesgo en las explicaciones.\n\nLimitaciones de SHAP:\n\nDepende de un “conjunto de datos de fondo” (background dataset): SHAP necesita un conjunto de datos de referencia para calcular un valor base o esperado de la predicción. Si tu conjunto de datos es muy grande, usarlo todo para este cálculo puede ser muy costoso computacionalmente (llevaría mucho tiempo). Por eso, a menudo se usan aproximaciones, como tomar una muestra más pequeña del conjunto de datos. Esto podría afectar un poco la precisión de la explicación.\nExplica la desviación respecto a un valor base estimado del entrenamiento: SHAP te dice cómo la predicción de una instancia se desvía del valor promedio que el modelo aprendió con todo el conjunto de datos de entrenamiento. Sin embargo, dependiendo de tu objetivo, podría ser más útil comparar la predicción con un grupo más específico. Ejemplo: Si estás prediciendo la “fuga de clientes” (churn), quizás te interese más explicar por qué un cliente se va a ir en comparación con los clientes que no se fueron, en lugar de compararlo con el promedio de todos los clientes (incluyendo los que se fueron y los que no). En este caso, querrías usar el conjunto de datos de los clientes que no se fueron como tu “conjunto de datos de fondo”."
  },
  {
    "objectID": "semana_3/index.html",
    "href": "semana_3/index.html",
    "title": "Semana 3: Técnicas Avanzadas y Robustez",
    "section": "",
    "text": "La última semana se centra en refinar y aplicar técnicas avanzadas para construir modelos robustos e interpretables.\nTemas Clave:\n\nPrincipios de Generalización.\nTransferencia de Aprendizaje y Fine-tuning.\nExplicabilidad e Interpretabilidad de modelos.\nOptimización de hiperparámetros.\nEntrenamiento Adversarial y Robustez.\n\nEnfoque Práctico: Desarrollarás habilidades para mejorar la eficacia de tus modelos en entornos reales. Implementarás técnicas de transferencia de aprendizaje, fine-tuning, generalización y usarás herramientas de explicabilidad para comprender y comunicar los resultados de modelos complejos.\nMateriales de la Semana:\n\nSlides de la Semana 3 (Próximamente)\nNotebook: Introducción a Transfer Learning y Finetuning\nNotebook: Introducción a Explicabilidad e Interpretabilidad"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2b_Resolviendo_desafios_con_RNNs_usando_keras.html",
    "href": "semana_2/notebooks/Nb_2b_Resolviendo_desafios_con_RNNs_usando_keras.html",
    "title": "Resolviendo diversidad de tareas con RNNs",
    "section": "",
    "text": "Última actualización: 09/05/2025\n#@title Importar librerías\n#importar librerías necesarias\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\n#@title Funciones complementarias\n\ndef plot_graphs(history, metric):\n  plt.plot(history.history[metric])\n  plt.plot(history.history['val_'+metric], '')\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(metric)\n  plt.legend([metric, 'val_'+metric])\n\ndef plot_variables(df, date_time):\n    plot_cols = ['T (degC)', 'p (mbar)', 'rho (g/m**3)']\n    plot_features = df[plot_cols]\n    plot_features.index = date_time\n    _ = plot_features.plot(subplots=True)\n\n    plot_features = df[plot_cols][:480]\n    plot_features.index = date_time[:480]\n    _ = plot_features.plot(subplots=True)\n\ndef read_dataset_clima():\n\n    zip_path = tf.keras.utils.get_file(\n        origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n        fname='jena_climate_2009_2016.csv.zip',\n        extract=True)\n    csv_path, _ = os.path.splitext(zip_path)\n\n    df = pd.read_csv(csv_path+'/'+'jena_climate_2009_2016.csv')\n    # Slice [start:stop:step], starting from index 5 take every 6th record.\n    df = df[5::6]\n\n    date_time = pd.to_datetime(df.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')\n\n    return df, date_time\n\ndef split_data(df):\n    column_indices = {name: i for i, name in enumerate(df.columns)}\n\n    n = len(df)\n    train_df = df[0:int(n*0.7)]\n    val_df = df[int(n*0.7):int(n*0.9)]\n    test_df = df[int(n*0.9):]\n\n    num_features = df.shape[1]\n\n    return train_df, val_df, test_df, num_features, column_indices\n\ndef normalizacion_datos(train_df, val_df, test_df):\n    train_mean = train_df.mean()\n    train_std = train_df.std()\n\n    train_df = (train_df - train_mean) / train_std\n    val_df = (val_df - train_mean) / train_std\n    test_df = (test_df - train_mean) / train_std\n\n    return train_df, val_df, test_df\n\n\nclass WindowGenerator():\n  def __init__(self, input_width, label_width, shift,\n               train_df, val_df, test_df,\n               label_columns=None):\n    # Store the raw data.\n    self.train_df = train_df\n    self.val_df = val_df\n    self.test_df = test_df\n\n    # Work out the label column indices.\n    self.label_columns = label_columns\n    if label_columns is not None:\n      self.label_columns_indices = {name: i for i, name in\n                                    enumerate(label_columns)}\n    self.column_indices = {name: i for i, name in\n                           enumerate(train_df.columns)}\n\n    # Work out the window parameters.\n    self.input_width = input_width\n    self.label_width = label_width\n    self.shift = shift\n\n    self.total_window_size = input_width + shift\n\n    self.input_slice = slice(0, input_width)\n    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n\n    self.label_start = self.total_window_size - self.label_width\n    self.labels_slice = slice(self.label_start, None)\n    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n\n  def __repr__(self):\n    return '\\n'.join([\n        f'Total window size: {self.total_window_size}',\n        f'Input indices: {self.input_indices}',\n        f'Label indices: {self.label_indices}',\n        f'Label column name(s): {self.label_columns}'])\n\n\ndef split_window(self, features):\n  inputs = features[:, self.input_slice, :]\n  labels = features[:, self.labels_slice, :]\n  if self.label_columns is not None:\n    labels = tf.stack(\n        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n        axis=-1)\n\n  # Slicing doesn't preserve static shape information, so set the shapes\n  # manually. This way the `tf.data.Datasets` are easier to inspect.\n  inputs.set_shape([None, self.input_width, None])\n  labels.set_shape([None, self.label_width, None])\n\n  return inputs, labels\n\ndef plot(self, model=None, plot_col='T (degC)', max_subplots=3):\n  inputs, labels = self.example\n  plt.figure(figsize=(12, 8))\n  plot_col_index = self.column_indices[plot_col]\n  max_n = min(max_subplots, len(inputs))\n  for n in range(max_n):\n    plt.subplot(max_n, 1, n+1)\n    plt.ylabel(f'{plot_col} [normed]')\n    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n             label='Inputs', marker='.', zorder=-10)\n\n    if self.label_columns:\n      label_col_index = self.label_columns_indices.get(plot_col, None)\n    else:\n      label_col_index = plot_col_index\n\n    if label_col_index is None:\n      continue\n\n    plt.scatter(self.label_indices, labels[n, :, label_col_index],\n                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n    if model is not None:\n      predictions = model(inputs)\n      plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n                  marker='X', edgecolors='k', label='Predictions',\n                  c='#ff7f0e', s=64)\n\n    if n == 0:\n      plt.legend()\n\n  plt.xlabel('Time [h]')\n\ndef make_dataset(self, data):\n  data = np.array(data, dtype=np.float32)\n  ds = tf.keras.utils.timeseries_dataset_from_array(\n      data=data,\n      targets=None,\n      sequence_length=self.total_window_size,\n      sequence_stride=1,\n      shuffle=True,\n      batch_size=32,)\n\n  ds = ds.map(self.split_window)\n\n  return ds\n\n\n@property\ndef train(self):\n  return self.make_dataset(self.train_df)\n\n@property\ndef val(self):\n  return self.make_dataset(self.val_df)\n\n@property\ndef test(self):\n  return self.make_dataset(self.test_df)\n\n@property\ndef example(self):\n  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n  result = getattr(self, '_example', None)\n  if result is None:\n    # No example batch was found, so get one from the `.train` dataset\n    result = next(iter(self.train))\n    # And cache it for next time\n    self._example = result\n  return result\n\n\n\nWindowGenerator.plot = plot\nWindowGenerator.split_window = split_window\nWindowGenerator.make_dataset = make_dataset\nWindowGenerator.train = train\nWindowGenerator.val = val\nWindowGenerator.test = test\nWindowGenerator.example = example"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2b_Resolviendo_desafios_con_RNNs_usando_keras.html#clasificación-de-texto-usando-rnns-relación-muchas-entradas-a-una-salida",
    "href": "semana_2/notebooks/Nb_2b_Resolviendo_desafios_con_RNNs_usando_keras.html#clasificación-de-texto-usando-rnns-relación-muchas-entradas-a-una-salida",
    "title": "Resolviendo diversidad de tareas con RNNs",
    "section": "Clasificación de texto usando RNNs (Relación muchas entradas a una salida)",
    "text": "Clasificación de texto usando RNNs (Relación muchas entradas a una salida)\nEn este escenario se reciben multiples entradas pero solo se genera una salida. Aquí podemos abordar el ejemplo más común que es la clasificación de texto.\n\nDescargar el dataset de reviews de peliculas usando Tensorflow.\nEl gran conjunto de datos de críticas de películas de IMDB es un conjunto de datos de clasificación binaria: todas las críticas tienen un sentimiento positivo o negativo.\n\ndataset, info = tfds.load('imdb_reviews', with_info=True,\n                          as_supervised=True)\ntrain_dataset, test_dataset = dataset['train'], dataset['test']\n\nWARNING:absl:Variant folder /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0 has no dataset_info.json\n\n\nDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\n\n\n\n# contar cuantas muestras tiene train_dataset y test_dataset\nprint(info.splits)\n\n{Split('train'): &lt;SplitInfo num_examples=25000, num_shards=1&gt;, Split('test'): &lt;SplitInfo num_examples=25000, num_shards=1&gt;, Split('unsupervised'): &lt;SplitInfo num_examples=50000, num_shards=1&gt;}\n\n\n\n# extraer una muestra del conjunto de entrenamiento\ntrain_example, train_label = next(iter(train_dataset.batch(1)))\n\nprint('Para la etiqueta: {} se tiene el texto: {}'.format(train_label, train_example.numpy()))\n\nPara la etiqueta: [0] se tiene el texto: [b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"]\n\n\nLas siguientes lineas son para optimizar los conjuntos y que el entrenamiento sea más acelerado.\n\nBUFFER_SIZE = 10000\nBATCH_SIZE = 32\n\n# optimización para train\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\n# optimización para test\ntest_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\n\n# muestras para el conjunto de test\nfor ejemplo, label in test_dataset.take(1):\n  print('textos: ', ejemplo.numpy()[:3])\n  print()\n  print('labels: ', label.numpy()[:3])\n\ntextos:  [b\"There are films that make careers. For George Romero, it was NIGHT OF THE LIVING DEAD; for Kevin Smith, CLERKS; for Robert Rodriguez, EL MARIACHI. Add to that list Onur Tukel's absolutely amazing DING-A-LING-LESS. Flawless film-making, and as assured and as professional as any of the aforementioned movies. I haven't laughed this hard since I saw THE FULL MONTY. (And, even then, I don't think I laughed quite this hard... So to speak.) Tukel's talent is considerable: DING-A-LING-LESS is so chock full of double entendres that one would have to sit down with a copy of this script and do a line-by-line examination of it to fully appreciate the, uh, breadth and width of it. Every shot is beautifully composed (a clear sign of a sure-handed director), and the performances all around are solid (there's none of the over-the-top scenery chewing one might've expected from a film like this). DING-A-LING-LESS is a film whose time has come.\"\n b\"A blackly comic tale of a down-trodden priest, Nazarin showcases the economy that Luis Bunuel was able to achieve in being able to tell a deeply humanist fable with a minimum of fuss. As an output from his Mexican era of film making, it was an invaluable talent to possess, with little money and extremely tight schedules. Nazarin, however, surpasses many of Bunuel's previous Mexican films in terms of the acting (Francisco Rabal is excellent), narrative and theme.&lt;br /&gt;&lt;br /&gt;The theme, interestingly, is something that was explored again in Viridiana, made three years later in Spain. It concerns the individual's struggle for humanity and altruism amongst a society that rejects any notion of virtue. Father Nazarin, however, is portrayed more sympathetically than Sister Viridiana. Whereas the latter seems to choose charity because she wishes to atone for her (perceived) sins, Nazarin's whole existence and reason for being seems to be to help others, whether they (or we) like it or not. The film's last scenes, in which he casts doubt on his behaviour and, in a split second, has to choose between the life he has been leading or the conventional life that is expected of a priest, are so emotional because they concern his moral integrity and we are never quite sure whether it remains intact or not.&lt;br /&gt;&lt;br /&gt;This is a remarkable film and I would urge anyone interested in classic cinema to seek it out. It is one of Bunuel's most moving films, and encapsulates many of his obsessions: frustrated desire, mad love, religious hypocrisy etc. In my view 'Nazarin' is second only to 'The Exterminating Angel', in terms of his Mexican movies, and is certainly near the top of the list of Bunuel's total filmic output.\"\n b'Scary Movie 1-4, Epic Movie, Date Movie, Meet the Spartans, Not another Teen Movie and Another Gay Movie. Making \"Superhero Movie\" the eleventh in a series that single handily ruined the parody genre. Now I\\'ll admit it I have a soft spot for classics such as Airplane and The Naked Gun but you know you\\'ve milked a franchise so bad when you can see the gags a mile off. In fact the only thing that might really temp you into going to see this disaster is the incredibly funny but massive sell-out Leslie Neilson.&lt;br /&gt;&lt;br /&gt;You can tell he needs the money, wither that or he intends to go down with the ship like a good Capitan would. In no way is he bringing down this genre but hell he\\'s not helping it. But if I feel sorry for anybody in this film its decent actor Drake Bell who is put through an immense amount of embarrassment. The people who are put through the largest amount of torture by far however is the audience forced to sit through 90 minutes of laughless bile no funnier than herpes.&lt;br /&gt;&lt;br /&gt;After spoofing disaster films in Airplane!, police shows in The Naked Gun, and Hollywood horrors in Scary Movie 3 and 4, producer David Zucker sets his satirical sights on the superhero genre with this anarchic comedy lampooning everything from Spider-Man to X-Men and Superman Returns.&lt;br /&gt;&lt;br /&gt;Shortly after being bitten by a genetically altered dragonfly, high-school outcast Rick Riker (Drake Bell) begins to experience a startling transformation. Now Rick\\'s skin is as strong as steel, and he possesses the strength of ten men. Determined to use his newfound powers to fight crime, Rick creates a special costume and assumes the identity of The Dragonfly -- a fearless crime fighter dedicated to keeping the streets safe for law-abiding citizens.&lt;br /&gt;&lt;br /&gt;But every superhero needs a nemesis, and after Lou Landers (Christopher McDonald) is caught in the middle of an experiment gone horribly awry, he develops the power to leech the life force out of anyone he meets and becomes the villainous Hourglass. Intent on achieving immortality, the Hourglass attempts to gather as much life force as possible as the noble Dragonfly sets out to take down his archenemy and realize his destiny as a true hero. Craig Mazin writes and directs this low-flying spoof.&lt;br /&gt;&lt;br /&gt;featuring Tracy Morgan, Pamela Anderson, Leslie Nielsen, Marion Ross, Jeffrey Tambor, and Regina Hall.&lt;br /&gt;&lt;br /&gt;Hell Superhero Movie may earn some merit in the fact that it\\'s a hell of a lot better than Meet the Spartans and Epic Movie. But with great responsibility comes one of the worst outings of 2008 to date. Laughless but a little less irritating than Meet the Spartans. And in the same sense much more forgettable than meet the Spartans. But maybe that\\'s a good reason. There are still some of us trying to scrape away the stain that was Meet the Spartans from our memory.&lt;br /&gt;&lt;br /&gt;My final verdict? Avoid, unless you\\'re one of thoses people who enjoy such car crash cinema. As bad as Date Movie and Scary Movie 2 but not quite as bad as Meet the Spartans or Epic Movie. Super Villain.']\n\nlabels:  [1 1 0]\n\n\n\n\nPre-procesamiento y modelado.\n\n\n\nbidirectional.png\n\n\nExplicación del modelo:\nEste modelo se puede construir como un tf.keras.Sequential.\n\nLa primera capa (amarilla) es el codificador, que convierte el texto en una secuencia de índices de tokens.\nDespués del codificador hay una capa de embedding (verde). Una capa de embedding almacena un vector por palabra. Cuando se llama, convierte las secuencias de índices de palabras en secuencias de vectores. Estos vectores son entrenables. Después del entrenamiento (con suficientes datos), las palabras con significados similares a menudo tienen vectores similares.\nUna red neuronal recurrente (RNN) procesa la entrada secuencial iterando a través de los elementos. Las RNN pasan las salidas de un instante de tiempo a su entrada en el siguiente instante de tiempo.\nLa capa tf.keras.layers.Bidirectional también se puede usar con una capa RNN. Esto propaga la entrada hacia adelante y hacia atrás a través de la capa RNN y luego concatena la salida final.\n\nLa principal ventaja de una RNN bidireccional es que la señal desde el comienzo de la entrada no necesita ser procesada a lo largo de cada instante de tiempo para afectar la salida.\nLa principal desventaja de una RNN bidireccional es que no se pueden transmitir predicciones de manera eficiente a medida que se agregan palabras al final.\n\nDespués de que la RNN ha convertido la secuencia en un solo vector, las dos capas Dense realizan un procesamiento final y convierten esta representación vectorial en un solo valor como salida de clasificación.\n\n\nCapa Codificador (Text Vectorization)\nEl texto sin procesar cargado por tfds necesita ser procesado antes de que pueda ser utilizado en un modelo. La forma más sencilla de procesar texto para el entrenamiento es utilizando la capa TextVectorization.\nExplicación:\nLa capa TextVectorization es una herramienta en TensorFlow que transforma texto sin procesar en una representación numérica que los modelos de aprendizaje automático pueden entender. Convierte las palabras en vectores, lo que facilita el entrenamiento del modelo. Esto incluye tareas como:\n\nTokenización: Dividir el texto en palabras o subpalabras.\nNormalización: Convertir todo el texto a minúsculas, eliminar caracteres especiales, etc.\nVectorización: Asignar un índice o valor numérico a cada palabra o token, lo que permite representarlo como una matriz numérica.\n\n\nVOCAB_SIZE = 1000\nencoder = tf.keras.layers.TextVectorization(\n    max_tokens=VOCAB_SIZE)\n\n# Crea la capa y pasa el texto del conjunto de datos al método .adapt de la capa\nencoder.adapt(train_dataset.map(lambda text, label: text))\n\nEl método .adapt se utiliza para “entrenar” la capa en el conjunto de datos de texto. Esto significa que la capa analizará el texto y aprenderá el vocabulario, así como otras características (como la frecuencia de las palabras) que serán útiles para la vectorización\n\n# revisar los primeros 20 elementos del vocabulario creado\nvocab = np.array(encoder.get_vocabulary())\nvocab[:20]\n\narray(['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i',\n       'this', 'that', 'br', 'was', 'as', 'for', 'with', 'movie', 'but'],\n      dtype='&lt;U14')\n\n\nUna vez que el vocabulario está establecido, la capa puede codificar el texto en índices. Los tensores de índices se rellenan con ceros hasta la secuencia más larga en el batch (a menos que establezcas una longitud de salida fija).\n\nencoded_ejemplo = encoder(ejemplo)[:3].numpy()\nencoded_ejemplo\n\narray([[ 48,  24,  95, ...,   0,   0,   0],\n       [  4,   1, 723, ...,   0,   0,   0],\n       [633,  18,   1, ...,  18,   1,   1]])\n\n\n\n# Visualizamos algunas oraciones\nfor n in range(3):\n  print(\"Original: \", ejemplo[n].numpy())\n  # intentar decodificar solo usando el vocab\n  print(\"Round-trip: \", \" \".join(vocab[encoded_ejemplo[n]]))\n  print()\n\nOriginal:  b\"There are films that make careers. For George Romero, it was NIGHT OF THE LIVING DEAD; for Kevin Smith, CLERKS; for Robert Rodriguez, EL MARIACHI. Add to that list Onur Tukel's absolutely amazing DING-A-LING-LESS. Flawless film-making, and as assured and as professional as any of the aforementioned movies. I haven't laughed this hard since I saw THE FULL MONTY. (And, even then, I don't think I laughed quite this hard... So to speak.) Tukel's talent is considerable: DING-A-LING-LESS is so chock full of double entendres that one would have to sit down with a copy of this script and do a line-by-line examination of it to fully appreciate the, uh, breadth and width of it. Every shot is beautifully composed (a clear sign of a sure-handed director), and the performances all around are solid (there's none of the over-the-top scenery chewing one might've expected from a film like this). DING-A-LING-LESS is a film whose time has come.\"\nRound-trip:  there are films that make [UNK] for george [UNK] it was night of the living dead for [UNK] [UNK] [UNK] for robert [UNK] [UNK] [UNK] add to that [UNK] [UNK] [UNK] absolutely amazing [UNK] [UNK] [UNK] and as [UNK] and as [UNK] as any of the [UNK] movies i havent [UNK] this hard since i saw the full [UNK] and even then i dont think i [UNK] quite this hard so to [UNK] [UNK] talent is [UNK] [UNK] is so [UNK] full of [UNK] [UNK] that one would have to sit down with a [UNK] of this script and do a [UNK] [UNK] of it to [UNK] [UNK] the [UNK] [UNK] and [UNK] of it every shot is [UNK] [UNK] a clear [UNK] of a [UNK] director and the performances all around are [UNK] theres none of the [UNK] [UNK] [UNK] one [UNK] expected from a film like this [UNK] is a film whose time has come                                                                                                                                                                                                                                                                                                                                                                                        \n\nOriginal:  b\"A blackly comic tale of a down-trodden priest, Nazarin showcases the economy that Luis Bunuel was able to achieve in being able to tell a deeply humanist fable with a minimum of fuss. As an output from his Mexican era of film making, it was an invaluable talent to possess, with little money and extremely tight schedules. Nazarin, however, surpasses many of Bunuel's previous Mexican films in terms of the acting (Francisco Rabal is excellent), narrative and theme.&lt;br /&gt;&lt;br /&gt;The theme, interestingly, is something that was explored again in Viridiana, made three years later in Spain. It concerns the individual's struggle for humanity and altruism amongst a society that rejects any notion of virtue. Father Nazarin, however, is portrayed more sympathetically than Sister Viridiana. Whereas the latter seems to choose charity because she wishes to atone for her (perceived) sins, Nazarin's whole existence and reason for being seems to be to help others, whether they (or we) like it or not. The film's last scenes, in which he casts doubt on his behaviour and, in a split second, has to choose between the life he has been leading or the conventional life that is expected of a priest, are so emotional because they concern his moral integrity and we are never quite sure whether it remains intact or not.&lt;br /&gt;&lt;br /&gt;This is a remarkable film and I would urge anyone interested in classic cinema to seek it out. It is one of Bunuel's most moving films, and encapsulates many of his obsessions: frustrated desire, mad love, religious hypocrisy etc. In my view 'Nazarin' is second only to 'The Exterminating Angel', in terms of his Mexican movies, and is certainly near the top of the list of Bunuel's total filmic output.\"\nRound-trip:  a [UNK] comic tale of a [UNK] [UNK] [UNK] [UNK] the [UNK] that [UNK] [UNK] was able to [UNK] in being able to tell a [UNK] [UNK] [UNK] with a [UNK] of [UNK] as an [UNK] from his [UNK] [UNK] of film making it was an [UNK] talent to [UNK] with little money and extremely [UNK] [UNK] [UNK] however [UNK] many of [UNK] previous [UNK] films in [UNK] of the acting [UNK] [UNK] is excellent [UNK] and [UNK] br the theme [UNK] is something that was [UNK] again in [UNK] made three years later in [UNK] it [UNK] the [UNK] [UNK] for [UNK] and [UNK] [UNK] a society that [UNK] any [UNK] of [UNK] father [UNK] however is portrayed more [UNK] than sister [UNK] [UNK] the [UNK] seems to [UNK] [UNK] because she [UNK] to [UNK] for her [UNK] [UNK] [UNK] whole [UNK] and reason for being seems to be to help others whether they or we like it or not the films last scenes in which he [UNK] doubt on his [UNK] and in a [UNK] second has to [UNK] between the life he has been leading or the [UNK] life that is expected of a [UNK] are so emotional because they [UNK] his [UNK] [UNK] and we are never quite sure whether it [UNK] [UNK] or [UNK] br this is a [UNK] film and i would [UNK] anyone interested in classic cinema to [UNK] it out it is one of [UNK] most moving films and [UNK] many of his [UNK] [UNK] [UNK] [UNK] love [UNK] [UNK] etc in my view [UNK] is second only to the [UNK] [UNK] in [UNK] of his [UNK] movies and is certainly near the top of the [UNK] of [UNK] total [UNK] [UNK]                                                                                                                                                                                                                                                   \n\nOriginal:  b'Scary Movie 1-4, Epic Movie, Date Movie, Meet the Spartans, Not another Teen Movie and Another Gay Movie. Making \"Superhero Movie\" the eleventh in a series that single handily ruined the parody genre. Now I\\'ll admit it I have a soft spot for classics such as Airplane and The Naked Gun but you know you\\'ve milked a franchise so bad when you can see the gags a mile off. In fact the only thing that might really temp you into going to see this disaster is the incredibly funny but massive sell-out Leslie Neilson.&lt;br /&gt;&lt;br /&gt;You can tell he needs the money, wither that or he intends to go down with the ship like a good Capitan would. In no way is he bringing down this genre but hell he\\'s not helping it. But if I feel sorry for anybody in this film its decent actor Drake Bell who is put through an immense amount of embarrassment. The people who are put through the largest amount of torture by far however is the audience forced to sit through 90 minutes of laughless bile no funnier than herpes.&lt;br /&gt;&lt;br /&gt;After spoofing disaster films in Airplane!, police shows in The Naked Gun, and Hollywood horrors in Scary Movie 3 and 4, producer David Zucker sets his satirical sights on the superhero genre with this anarchic comedy lampooning everything from Spider-Man to X-Men and Superman Returns.&lt;br /&gt;&lt;br /&gt;Shortly after being bitten by a genetically altered dragonfly, high-school outcast Rick Riker (Drake Bell) begins to experience a startling transformation. Now Rick\\'s skin is as strong as steel, and he possesses the strength of ten men. Determined to use his newfound powers to fight crime, Rick creates a special costume and assumes the identity of The Dragonfly -- a fearless crime fighter dedicated to keeping the streets safe for law-abiding citizens.&lt;br /&gt;&lt;br /&gt;But every superhero needs a nemesis, and after Lou Landers (Christopher McDonald) is caught in the middle of an experiment gone horribly awry, he develops the power to leech the life force out of anyone he meets and becomes the villainous Hourglass. Intent on achieving immortality, the Hourglass attempts to gather as much life force as possible as the noble Dragonfly sets out to take down his archenemy and realize his destiny as a true hero. Craig Mazin writes and directs this low-flying spoof.&lt;br /&gt;&lt;br /&gt;featuring Tracy Morgan, Pamela Anderson, Leslie Nielsen, Marion Ross, Jeffrey Tambor, and Regina Hall.&lt;br /&gt;&lt;br /&gt;Hell Superhero Movie may earn some merit in the fact that it\\'s a hell of a lot better than Meet the Spartans and Epic Movie. But with great responsibility comes one of the worst outings of 2008 to date. Laughless but a little less irritating than Meet the Spartans. And in the same sense much more forgettable than meet the Spartans. But maybe that\\'s a good reason. There are still some of us trying to scrape away the stain that was Meet the Spartans from our memory.&lt;br /&gt;&lt;br /&gt;My final verdict? Avoid, unless you\\'re one of thoses people who enjoy such car crash cinema. As bad as Date Movie and Scary Movie 2 but not quite as bad as Meet the Spartans or Epic Movie. Super Villain.'\nRound-trip:  scary movie [UNK] [UNK] movie [UNK] movie meet the [UNK] not another [UNK] movie and another [UNK] movie making [UNK] movie the [UNK] in a series that single [UNK] [UNK] the [UNK] genre now ill admit it i have a [UNK] [UNK] for [UNK] such as [UNK] and the [UNK] [UNK] but you know youve [UNK] a [UNK] so bad when you can see the [UNK] a [UNK] off in fact the only thing that might really [UNK] you into going to see this [UNK] is the incredibly funny but [UNK] [UNK] [UNK] [UNK] br you can tell he needs the money [UNK] that or he [UNK] to go down with the [UNK] like a good [UNK] would in no way is he [UNK] down this genre but hell hes not [UNK] it but if i feel sorry for [UNK] in this film its decent actor [UNK] [UNK] who is put through an [UNK] [UNK] of [UNK] the people who are put through the [UNK] [UNK] of [UNK] by far however is the audience forced to sit through [UNK] minutes of [UNK] [UNK] no [UNK] than [UNK] br after [UNK] [UNK] films in [UNK] police shows in the [UNK] [UNK] and hollywood [UNK] in scary movie 3 and 4 [UNK] david [UNK] sets his [UNK] [UNK] on the [UNK] genre with this [UNK] comedy [UNK] everything from [UNK] to [UNK] and [UNK] [UNK] br [UNK] after being [UNK] by a [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] begins to experience a [UNK] [UNK] now [UNK] [UNK] is as strong as [UNK] and he [UNK] the [UNK] of ten men [UNK] to use his [UNK] [UNK] to fight crime [UNK] [UNK] a special [UNK] and [UNK] the [UNK] of the [UNK] a [UNK] crime [UNK] [UNK] to [UNK] the [UNK] [UNK] for [UNK] [UNK] br but every [UNK] needs a [UNK] and after [UNK] [UNK] [UNK] [UNK] is [UNK] in the middle of an [UNK] gone [UNK] [UNK] he [UNK] the power to [UNK] the life [UNK] out of anyone he meets and becomes the [UNK] [UNK] [UNK] on [UNK] [UNK] the [UNK] attempts to [UNK] as much life [UNK] as possible as the [UNK] [UNK] sets out to take down his [UNK] and realize his [UNK] as a true hero [UNK] [UNK] [UNK] and [UNK] this [UNK] [UNK] br [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] and [UNK] [UNK] br hell [UNK] movie may [UNK] some [UNK] in the fact that its a hell of a lot better than meet the [UNK] and [UNK] movie but with great [UNK] comes one of the worst [UNK] of [UNK] to [UNK] [UNK] but a little less [UNK] than meet the [UNK] and in the same sense much more [UNK] than meet the [UNK] but maybe thats a good reason there are still some of us trying to [UNK] away the [UNK] that was meet the [UNK] from our [UNK] br my final [UNK] avoid unless youre one of [UNK] people who enjoy such car [UNK] cinema as bad as [UNK] movie and scary movie 2 but not quite as bad as meet the [UNK] or [UNK] movie [UNK] [UNK]\n\n\n\n\n\nCreación del modelo usando keras\n\n'''model = tf.keras.Sequential([\n    encoder,\n    tf.keras.layers.Embedding(\n        input_dim=len(encoder.get_vocabulary()),\n        output_dim=64,\n        # Se ignora los valores en 0\n        mask_zero=True),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64), merge_mode='concat'),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1)\n])'''\n\ndef rnn():\n    # Ahora utilizamos la API funcional de Keras\n    inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)  # El input será una cadena de texto\n    x = encoder(inputs)  # Aplicamos el encoder\n\n    x = tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary()), output_dim=64, mask_zero=True)(x)  # Capa de Embedding\n\n    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False), merge_mode='concat')(x)  # Capa LSTM Bidireccional\n\n    x = tf.keras.layers.Dense(64, activation='relu')(x)  # Capa densa\n    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)  # Capa de salida\n\n    # Definimos el modelo\n    model = tf.keras.Model(inputs, outputs)\n\n    return model\n\n# crear la red RNN\nmodel = rnn()\n\n# Compilamos el modelo\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n              optimizer=tf.keras.optimizers.Adam(1e-4),\n              metrics=['accuracy'])\n\n# Verificamos la estructura del modelo\nmodel.summary()\n\nModel: \"functional_1\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_1       │ (None, 1)         │          0 │ -                 │\n│ (InputLayer)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ text_vectorization  │ (None, None)      │          0 │ input_layer_1[0]… │\n│ (TextVectorization) │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding_1         │ (None, None, 64)  │     64,000 │ text_vectorizati… │\n│ (Embedding)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ not_equal_1         │ (None, None)      │          0 │ text_vectorizati… │\n│ (NotEqual)          │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bidirectional_1     │ (None, 128)       │     66,048 │ embedding_1[0][0… │\n│ (Bidirectional)     │                   │            │ not_equal_1[0][0] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_2 (Dense)     │ (None, 64)        │      8,256 │ bidirectional_1[… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_3 (Dense)     │ (None, 1)         │         65 │ dense_2[0][0]     │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n\n\n\n Total params: 138,369 (540.50 KB)\n\n\n\n Trainable params: 138,369 (540.50 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\ntf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)\n\n\n\n\n\n\n\n\n\n# El texto crudo que quieres predecir\nsample_text = ('The movie was cool. The animation and the graphics were out of this world.')\n\n# No es necesario hacer la vectorización manual aquí, simplemente pasa el texto crudo al modelo\npredictions = model.predict(tf.constant([sample_text]))\n\n# Imprime la predicción .. que debe ser casi 0.5 (neutralidad)\nprint(predictions[0])\n\n\n1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 1s/step\n\n[0.4999323]\n\n\n\n\n\n\nEntrenamiento y evaluación del modelo\n\nhistory = model.fit(train_dataset, epochs=10,\n                    validation_data=test_dataset,\n                    validation_steps=30)\n\n\nEpoch 1/10\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 39s 44ms/step - accuracy: 0.5798 - loss: 0.6706 - val_accuracy: 0.7823 - val_loss: 0.5019\n\nEpoch 2/10\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 40s 45ms/step - accuracy: 0.7957 - loss: 0.4493 - val_accuracy: 0.8271 - val_loss: 0.4129\n\nEpoch 3/10\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 41s 52ms/step - accuracy: 0.8513 - loss: 0.3502 - val_accuracy: 0.8427 - val_loss: 0.3751\n\nEpoch 4/10\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 82s 52ms/step - accuracy: 0.8605 - loss: 0.3311 - val_accuracy: 0.8521 - val_loss: 0.3591\n\nEpoch 5/10\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 74s 43ms/step - accuracy: 0.8665 - loss: 0.3233 - val_accuracy: 0.8698 - val_loss: 0.3445\n\nEpoch 6/10\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 36s 46ms/step - accuracy: 0.8688 - loss: 0.3157 - val_accuracy: 0.8687 - val_loss: 0.3407\n\nEpoch 7/10\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 82s 99ms/step - accuracy: 0.8722 - loss: 0.3120 - val_accuracy: 0.8531 - val_loss: 0.3814\n\nEpoch 8/10\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 41s 46ms/step - accuracy: 0.8702 - loss: 0.3176 - val_accuracy: 0.8490 - val_loss: 0.3559\n\nEpoch 9/10\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 41s 52ms/step - accuracy: 0.8701 - loss: 0.3108 - val_accuracy: 0.8646 - val_loss: 0.3362\n\nEpoch 10/10\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 75s 44ms/step - accuracy: 0.8703 - loss: 0.3075 - val_accuracy: 0.8302 - val_loss: 0.3808\n\n\n\n\n\ntest_loss, test_acc = model.evaluate(test_dataset)\n\nprint('Test Loss:', test_loss)\nprint('Test Accuracy:', test_acc)\n\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 17s 22ms/step - accuracy: 0.8340 - loss: 0.3708\n\nTest Loss: 0.37266772985458374\n\nTest Accuracy: 0.8334000110626221\n\n\n\n\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplot_graphs(history, 'accuracy')\nplt.ylim(None, 1)\nplt.subplot(1, 2, 2)\nplot_graphs(history, 'loss')\nplt.ylim(0, None)\n\n\n\n\n\n\n\n\n\n# realizar una predicción des pues de entrenar\nsample_text = ('The movie was cool. The animation and the graphics were out of this world.')\npredictions = model.predict(tf.constant([sample_text]))\nprint(predictions)\n\n\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 30ms/step\n\n[[0.6709632]]\n\n\n\n\n\n# realizar una predicción des pues de entrenar\nsample_text = ('The movie was very cool. The animation and the graphics were out of this world.')\npredictions = model.predict(tf.constant([sample_text]))\nprint(predictions)\n\n\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step\n\n[[0.71158457]]\n\n\n\n\nCómo podríamos agregar más capas LSTM para aumentar la complejidad del modelo?\n\n\n\nlayered_bidirectional.png\n\n\nLas capas recurrentes de Keras tienen dos modos disponibles que son controlados por el argumento del constructor return_sequences:\n\nSi es False devuelve sólo la última salida para cada secuencia de entrada (un tensor 2D de forma (batch_size, output_features)). Este es el valor por defecto, utilizado en el modelo anterior.\nSi es True se devuelven las secuencias completas de salidas sucesivas para cada paso de tiempo (un tensor 3D de forma (batch_size, timesteps, output_features)).\n\n# model = tf.keras.Sequential([\n    encoder,\n    tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1)\n])"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2b_Resolviendo_desafios_con_RNNs_usando_keras.html#leer-datos",
    "href": "semana_2/notebooks/Nb_2b_Resolviendo_desafios_con_RNNs_usando_keras.html#leer-datos",
    "title": "Resolviendo diversidad de tareas con RNNs",
    "section": "Leer datos",
    "text": "Leer datos\n\ndf, date_time = read_dataset_clima()\n\n\ndf\n\n\n    \n\n\n\n\n\n\np (mbar)\nT (degC)\nTpot (K)\nTdew (degC)\nrh (%)\nVPmax (mbar)\nVPact (mbar)\nVPdef (mbar)\nsh (g/kg)\nH2OC (mmol/mol)\nrho (g/m**3)\nwv (m/s)\nmax. wv (m/s)\nwd (deg)\n\n\n\n\n5\n996.50\n-8.05\n265.38\n-8.78\n94.40\n3.33\n3.14\n0.19\n1.96\n3.15\n1307.86\n0.21\n0.63\n192.7\n\n\n11\n996.62\n-8.88\n264.54\n-9.77\n93.20\n3.12\n2.90\n0.21\n1.81\n2.91\n1312.25\n0.25\n0.63\n190.3\n\n\n17\n996.84\n-8.81\n264.59\n-9.66\n93.50\n3.13\n2.93\n0.20\n1.83\n2.94\n1312.18\n0.18\n0.63\n167.2\n\n\n23\n996.99\n-9.05\n264.34\n-10.02\n92.60\n3.07\n2.85\n0.23\n1.78\n2.85\n1313.61\n0.10\n0.38\n240.0\n\n\n29\n997.46\n-9.63\n263.72\n-10.65\n92.20\n2.94\n2.71\n0.23\n1.69\n2.71\n1317.19\n0.40\n0.88\n157.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n420521\n1002.18\n-0.98\n272.01\n-5.36\n72.00\n5.69\n4.09\n1.59\n2.54\n4.08\n1280.70\n0.87\n1.36\n190.6\n\n\n420527\n1001.40\n-1.40\n271.66\n-6.84\n66.29\n5.51\n3.65\n1.86\n2.27\n3.65\n1281.87\n1.02\n1.92\n225.4\n\n\n420533\n1001.19\n-2.75\n270.32\n-6.90\n72.90\n4.99\n3.64\n1.35\n2.26\n3.63\n1288.02\n0.71\n1.56\n158.7\n\n\n420539\n1000.65\n-2.89\n270.22\n-7.15\n72.30\n4.93\n3.57\n1.37\n2.22\n3.57\n1288.03\n0.35\n0.68\n216.7\n\n\n420545\n1000.11\n-3.93\n269.23\n-8.09\n72.60\n4.56\n3.31\n1.25\n2.06\n3.31\n1292.41\n0.56\n1.00\n202.6\n\n\n\n\n70091 rows × 14 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\ndate_time\n\n\n\n\n\n\n\n\nDate Time\n\n\n\n\n5\n2009-01-01 01:00:00\n\n\n11\n2009-01-01 02:00:00\n\n\n17\n2009-01-01 03:00:00\n\n\n23\n2009-01-01 04:00:00\n\n\n29\n2009-01-01 05:00:00\n\n\n...\n...\n\n\n420521\n2016-12-31 19:10:00\n\n\n420527\n2016-12-31 20:10:00\n\n\n420533\n2016-12-31 21:10:00\n\n\n420539\n2016-12-31 22:10:00\n\n\n420545\n2016-12-31 23:10:00\n\n\n\n\n70091 rows × 1 columns\ndtype: datetime64[ns]\n\n\n\nplot_variables(df, date_time)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_df, val_df, test_df, num_features, column_indices = split_data(df)\ntrain_df.shape, val_df.shape, test_df.shape\n\n((49063, 14), (14018, 14), (7010, 14))\n\n\n\ntrain_df.head()\n\n\n    \n\n\n\n\n\n\np (mbar)\nT (degC)\nTpot (K)\nTdew (degC)\nrh (%)\nVPmax (mbar)\nVPact (mbar)\nVPdef (mbar)\nsh (g/kg)\nH2OC (mmol/mol)\nrho (g/m**3)\nwv (m/s)\nmax. wv (m/s)\nwd (deg)\n\n\n\n\n5\n996.50\n-8.05\n265.38\n-8.78\n94.4\n3.33\n3.14\n0.19\n1.96\n3.15\n1307.86\n0.21\n0.63\n192.7\n\n\n11\n996.62\n-8.88\n264.54\n-9.77\n93.2\n3.12\n2.90\n0.21\n1.81\n2.91\n1312.25\n0.25\n0.63\n190.3\n\n\n17\n996.84\n-8.81\n264.59\n-9.66\n93.5\n3.13\n2.93\n0.20\n1.83\n2.94\n1312.18\n0.18\n0.63\n167.2\n\n\n23\n996.99\n-9.05\n264.34\n-10.02\n92.6\n3.07\n2.85\n0.23\n1.78\n2.85\n1313.61\n0.10\n0.38\n240.0\n\n\n29\n997.46\n-9.63\n263.72\n-10.65\n92.2\n2.94\n2.71\n0.23\n1.69\n2.71\n1317.19\n0.40\n0.88\n157.0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\n\ntrain_df, val_df, test_df = normalizacion_datos(train_df, val_df, test_df)\ntrain_df.describe()\n\n\n    \n\n\n\n\n\n\np (mbar)\nT (degC)\nTpot (K)\nTdew (degC)\nrh (%)\nVPmax (mbar)\nVPact (mbar)\nVPdef (mbar)\nsh (g/kg)\nH2OC (mmol/mol)\nrho (g/m**3)\nwv (m/s)\nmax. wv (m/s)\nwd (deg)\n\n\n\n\ncount\n4.906300e+04\n4.906300e+04\n4.906300e+04\n4.906300e+04\n4.906300e+04\n49063.000000\n4.906300e+04\n49063.000000\n4.906300e+04\n4.906300e+04\n4.906300e+04\n4.906300e+04\n4.906300e+04\n4.906300e+04\n\n\nmean\n2.027515e-16\n-1.946415e-16\n9.685730e-16\n1.853728e-17\n-6.719765e-16\n0.000000\n-2.270817e-16\n0.000000\n-1.506154e-16\n1.807385e-16\n-2.321795e-15\n1.540912e-16\n-1.616219e-16\n-2.178131e-16\n\n\nstd\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000\n1.000000e+00\n1.000000\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n\n\nmin\n-9.045695e+00\n-3.682079e+00\n-3.707266e+00\n-4.216645e+00\n-3.746587e+00\n-1.609554\n-2.030996e+00\n-0.829861\n-2.022853e+00\n-2.031986e+00\n-3.846513e+00\n-1.403684e+00\n-1.535423e+00\n-1.977937e+00\n\n\n25%\n-6.093840e-01\n-7.069026e-01\n-6.939982e-01\n-6.697392e-01\n-6.581569e-01\n-0.750526\n-7.786971e-01\n-0.657581\n-7.762466e-01\n-7.761335e-01\n-7.116941e-01\n-7.390786e-01\n-7.595612e-01\n-6.326198e-01\n\n\n50%\n5.467421e-02\n9.450477e-03\n1.318575e-02\n5.168967e-02\n1.989686e-01\n-0.222892\n-1.561120e-01\n-0.383594\n-1.548152e-01\n-1.540757e-01\n-7.847992e-02\n-2.308512e-01\n-2.250786e-01\n2.674928e-01\n\n\n75%\n6.548575e-01\n7.200265e-01\n7.123465e-01\n7.530390e-01\n8.150841e-01\n0.533469\n6.684569e-01\n0.268164\n6.650251e-01\n6.651626e-01\n6.442168e-01\n4.793641e-01\n5.206108e-01\n6.932912e-01\n\n\nmax\n2.913378e+00\n3.066661e+00\n3.041354e+00\n2.647686e+00\n1.455361e+00\n5.846190\n4.489514e+00\n7.842254\n4.550843e+00\n4.524268e+00\n4.310438e+00\n7.724863e+00\n8.593884e+00\n2.131645e+00"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2b_Resolviendo_desafios_con_RNNs_usando_keras.html#creación-de-ventanas",
    "href": "semana_2/notebooks/Nb_2b_Resolviendo_desafios_con_RNNs_usando_keras.html#creación-de-ventanas",
    "title": "Resolviendo diversidad de tareas con RNNs",
    "section": "Creación de ventanas",
    "text": "Creación de ventanas\nEjemplo para un problema que dado las últimas 6 mediciones de temperatura va a predecir la siguiente hora de temperatura.\n\n\n\nsplit_window.png\n\n\n\nEjemplo creación dataset ventanas\n\n# Crear un array de 10 minutos y 3 características (features=2), con valores consecutivos para facilitar el entendimiento\ndatae = np.arange(10 * 3).reshape(10, 3)\n\n# Crear un DataFrame para mostrar los datos de manera más clara\ndfe = pd.DataFrame(datae, columns=['Feature_1', 'Feature_2', 'Target'])\ndfe.index.name = 'Minuto'\n\n# Mostrar el DataFrame\ndfe\n\n\n    \n\n\n\n\n\n\nFeature_1\nFeature_2\nTarget\n\n\nMinuto\n\n\n\n\n\n\n\n0\n0\n1\n2\n\n\n1\n3\n4\n5\n\n\n2\n6\n7\n8\n\n\n3\n9\n10\n11\n\n\n4\n12\n13\n14\n\n\n5\n15\n16\n17\n\n\n6\n18\n19\n20\n\n\n7\n21\n22\n23\n\n\n8\n24\n25\n26\n\n\n9\n27\n28\n29\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n# La variable a predecir es la última columna (columna final)\ntargets = datae[2:, -1]  # Los targets son el valor de la columna final (variable a predecir) desplazados por 2 pasos\ntargets\n\narray([ 8, 11, 14, 17, 20, 23, 26, 29])\n\n\n\n# Crear el dataset de series temporales\nds = tf.keras.utils.timeseries_dataset_from_array(\n    data=datae[:-1],  # Todas las filas menos la última, porque no hay target para la última fila\n    targets=targets,  # Los valores a predecir son la columna final de la siguiente fila\n    sequence_length=2,  # Usamos secuencias de 2 minutos\n    sequence_stride=1,  # Stride de 1 para obtener todas las posibles ventanas\n    shuffle=False,  # Barajamos las secuencias\n    batch_size=2  # Agrupamos en lotes de 2 secuencias\n)\n\n\n# Mostrar los primeros 5 lotes de inputs y labels, con un formato mejorado\nfor batch_num, batch in enumerate(ds.take(5), 1):\n    inputs, labels = batch\n    print(f\"Batch {batch_num}:\")\n    print(f\"Inputs shape: {inputs.shape}\")\n    print(\"Inputs:\")\n\n    # Imprimir cada secuencia de inputs con su respectiva etiqueta al final\n    for i, (input_sequence, label) in enumerate(zip(inputs.numpy(), labels.numpy()), 1):\n        print(f\"  Sequence {i}:\")\n        print(f\" {input_sequence} -&gt; Target: {label}\")\n\n    print(\"\\n\" + \"-\"*50 + \"\\n\")\n\nBatch 1:\nInputs shape: (2, 2, 3)\nInputs:\n  Sequence 1:\n [[0 1 2]\n [3 4 5]] -&gt; Target: 8\n  Sequence 2:\n [[3 4 5]\n [6 7 8]] -&gt; Target: 11\n\n--------------------------------------------------\n\nBatch 2:\nInputs shape: (2, 2, 3)\nInputs:\n  Sequence 1:\n [[ 6  7  8]\n [ 9 10 11]] -&gt; Target: 14\n  Sequence 2:\n [[ 9 10 11]\n [12 13 14]] -&gt; Target: 17\n\n--------------------------------------------------\n\nBatch 3:\nInputs shape: (2, 2, 3)\nInputs:\n  Sequence 1:\n [[12 13 14]\n [15 16 17]] -&gt; Target: 20\n  Sequence 2:\n [[15 16 17]\n [18 19 20]] -&gt; Target: 23\n\n--------------------------------------------------\n\nBatch 4:\nInputs shape: (2, 2, 3)\nInputs:\n  Sequence 1:\n [[18 19 20]\n [21 22 23]] -&gt; Target: 26\n  Sequence 2:\n [[21 22 23]\n [24 25 26]] -&gt; Target: 29\n\n--------------------------------------------------\n\n\n\n\n\nRetomando\n\nMAX_EPOCHS = 20\n\ndef compile_and_fit(model, window, patience=2):\n  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                                    patience=patience,\n                                                    mode='min')\n\n  model.compile(loss=tf.keras.losses.MeanSquaredError(),\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[tf.keras.metrics.MeanAbsoluteError()])\n\n  history = model.fit(window.train, epochs=MAX_EPOCHS,\n                      validation_data=window.val,\n                      callbacks=[early_stopping])\n  return history\n\n\nWIDTH_TEMP = 6\nwindow = WindowGenerator(\n    input_width=WIDTH_TEMP,\n    label_width=1,\n    shift=1,\n    train_df = train_df,\n    val_df = val_df,\n    test_df=test_df,\n    label_columns=['T (degC)'])\n\nwindow\n\nTotal window size: 7\nInput indices: [0 1 2 3 4 5]\nLabel indices: [6]\nLabel column name(s): ['T (degC)']\n\n\n\n# Stack three slices, the length of the total window.\nexample_window = tf.stack([np.array(train_df[:window.total_window_size]),\n                           np.array(train_df[100:100+window.total_window_size]),\n                           np.array(train_df[200:200+window.total_window_size])])\n\nexample_inputs, example_labels = window.split_window(example_window)\n\nprint('All shapes are: (batch, time, features)')\nprint(f'Window shape: {example_window.shape}')\nprint(f'Inputs shape: {example_inputs.shape}')\nprint(f'Labels shape: {example_labels.shape}')\n\nAll shapes are: (batch, time, features)\nWindow shape: (3, 7, 14)\nInputs shape: (3, 6, 14)\nLabels shape: (3, 1, 1)\n\n\n\n# graficar los datos en las ventanas\nwindow.plot()\n\n\n\n\n\n\n\n\n\nlstm_model = tf.keras.models.Sequential([\n    # Shape [batch, time, features] =&gt; [batch, time, lstm_units]\n    tf.keras.layers.LSTM(32, return_sequences=True),\n    # Shape =&gt; [batch, time, features]\n    tf.keras.layers.Dense(units=1)\n])\n\n\nprint('Input shape:', window.example[0].shape)\nprint('Output shape:', lstm_model(window.example[0]).shape)\n\nInput shape: (32, 6, 14)\nOutput shape: (32, 6, 1)\n\n\n\nhistory = compile_and_fit(lstm_model, window)\n\nval_performance = {}\nperformance = {}\nval_performance['LSTM'] = lstm_model.evaluate(window.val, return_dict=True)\nperformance['LSTM'] = lstm_model.evaluate(window.test, verbose=0, return_dict=True)\n\n\nEpoch 1/20\n\n1534/1534 ━━━━━━━━━━━━━━━━━━━━ 14s 8ms/step - loss: 0.1676 - mean_absolute_error: 0.2821 - val_loss: 0.0809 - val_mean_absolute_error: 0.1999\n\nEpoch 2/20\n\n1534/1534 ━━━━━━━━━━━━━━━━━━━━ 12s 8ms/step - loss: 0.0787 - mean_absolute_error: 0.1963 - val_loss: 0.0758 - val_mean_absolute_error: 0.1910\n\nEpoch 3/20\n\n1534/1534 ━━━━━━━━━━━━━━━━━━━━ 12s 8ms/step - loss: 0.0735 - mean_absolute_error: 0.1877 - val_loss: 0.0735 - val_mean_absolute_error: 0.1872\n\nEpoch 4/20\n\n1534/1534 ━━━━━━━━━━━━━━━━━━━━ 18s 6ms/step - loss: 0.0716 - mean_absolute_error: 0.1845 - val_loss: 0.0726 - val_mean_absolute_error: 0.1848\n\nEpoch 5/20\n\n1534/1534 ━━━━━━━━━━━━━━━━━━━━ 11s 7ms/step - loss: 0.0704 - mean_absolute_error: 0.1826 - val_loss: 0.0723 - val_mean_absolute_error: 0.1830\n\nEpoch 6/20\n\n1534/1534 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - loss: 0.0696 - mean_absolute_error: 0.1809 - val_loss: 0.0715 - val_mean_absolute_error: 0.1817\n\nEpoch 7/20\n\n1534/1534 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - loss: 0.0692 - mean_absolute_error: 0.1806 - val_loss: 0.0704 - val_mean_absolute_error: 0.1806\n\nEpoch 8/20\n\n1534/1534 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - loss: 0.0685 - mean_absolute_error: 0.1791 - val_loss: 0.0708 - val_mean_absolute_error: 0.1817\n\nEpoch 9/20\n\n1534/1534 ━━━━━━━━━━━━━━━━━━━━ 23s 8ms/step - loss: 0.0681 - mean_absolute_error: 0.1784 - val_loss: 0.0709 - val_mean_absolute_error: 0.1836\n\n438/438 ━━━━━━━━━━━━━━━━━━━━ 2s 4ms/step - loss: 0.0694 - mean_absolute_error: 0.1819\n\n\n\n\n\nperformance\n\n{'LSTM': {'loss': 0.06843273341655731,\n  'mean_absolute_error': 0.1830073744058609}}\n\n\n\nval_performance\n\n{'LSTM': {'loss': 0.07088509947061539,\n  'mean_absolute_error': 0.18363438546657562}}"
  },
  {
    "objectID": "semana_2/index.html",
    "href": "semana_2/index.html",
    "title": "Semana 2: Arquitecturas de Redes Neuronales",
    "section": "",
    "text": "Expandimos nuestro conocimiento explorando diversas arquitecturas de redes neuronales y sus aplicaciones.\nTemas Clave:\n\nDiversas arquitecturas de redes neuronales (DNN, CNN, RNN).\nAplicaciones y ventajas de cada arquitectura.\nIntroducción a arquitecturas avanzadas (Transformers, GANs, Redes de Grafos).\n\nEnfoque Práctico: Aprenderás a identificar y seleccionar la arquitectura más adecuada para diferentes tipos de problemas. Implementarás modelos DNN, CNN y RNN en proyectos prácticos para experimentar con su desempeño.\nMateriales de la Semana:\n\nSlides de la Semana 2 (Próximamente)\nNotebook: Implementando una CNN usando Keras\nNotebook: Resolviendo desafíos con RNNs usando Keras\nNotebook: Traducción usando Transformers y Keras"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html",
    "href": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html",
    "title": "Implementado un Perceptrón multi-capa usando frameworks y MLFlow",
    "section": "",
    "text": "Open In Colab\n#@title Importar librerías\n#importar librerías necesarias\nimport json\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Input\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.datasets import mnist\n\n!pip install mlflow --quiet\n\nimport mlflow\nimport mlflow.sklearn\nimport mlflow.tensorflow\nfrom mlflow.tracking import MlflowClient\n\n\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.7/26.7 MB 19.7 MB/s eta 0:00:00\n\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.7/5.7 MB 32.5 MB/s eta 0:00:00\n\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 233.5/233.5 kB 8.5 MB/s eta 0:00:00\n\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.8/147.8 kB 5.6 MB/s eta 0:00:00\n\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.7/114.7 kB 3.8 MB/s eta 0:00:00\n\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.0/85.0 kB 2.3 MB/s eta 0:00:00\n\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 569.1/569.1 kB 10.8 MB/s eta 0:00:00\n\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 203.2/203.2 kB 3.6 MB/s eta 0:00:00\n\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.6/78.6 kB 4.4 MB/s eta 0:00:00\n#@title Configurando MLFlow\n# configurar que el servidor de trackin sea localhost con una BD sqlite como el almacenamiento para almacenar el proceso de tracking\n\nlocal_registry = \"sqlite:///mlruns.db\"\nprint(f\"Ejecutando registro en modo local={local_registry}\")\nmlflow.set_tracking_uri(local_registry)\n\nEjecutando registro en modo local=sqlite:///mlruns.db\n#@title Funciones complementarias\ndef plot_samples_dataset(X, y):\n    # Convertir las etiquetas a enteros\n    y = y.astype(int)\n\n    # Crear la grilla de 4x4\n    fig, axes = plt.subplots(4, 4, figsize=(6, 6))\n    fig.suptitle('Grilla de imágenes del dataset MNIST')\n\n    # Iterar para mostrar las primeras 16 imágenes con sus etiquetas\n    for i, ax in enumerate(axes.flat):\n        img = X.iloc[i].values.reshape(28, 28)\n        label = y[i]\n        ax.imshow(img, cmap='gray')\n        ax.set_title(f'Label: {label}')\n        ax.axis('off')\n\n    plt.show()\n\ndef plot_curva_aprendizaje(mlp):\n    plt.figure(figsize=(8, 5))\n    plt.plot(mlp.loss_curve_, marker='o')\n    plt.title('Pérdida durante el entrenamiento del MLP por iteración')\n    plt.xlabel('Iteración')\n    plt.ylabel('Pérdida (Loss)')\n    plt.grid()\n    plt.show()\n\ndef plot_matriz_confusion(cm, nombre):\n    # Visualizar la matriz de confusión usando Seaborn\n    plt.figure(figsize=(10, 7))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n    plt.xlabel('Etiqueta predicha')\n    plt.ylabel('Etiqueta real')\n    plt.title('Matriz de Confusión para el MLP en el dataset MNIST')\n    plt.show()\n    plt.savefig(f\"{nombre}.png\")\n\ndef encontrar_dim_imagen(n_neurons):\n    \"\"\"\n    Encuentra la mejor forma cuadrada (filas, columnas) para una cantidad dada de neuronas.\n    \"\"\"\n    side_length = int(np.sqrt(n_neurons))  # Calcular la raíz cuadrada del número de neuronas\n    if side_length * side_length == n_neurons:\n        return (side_length, side_length)  # Si es un cuadrado perfecto\n    else:\n        # Si no es un cuadrado perfecto, buscamos la mejor aproximación (filas, columnas)\n        for i in range(side_length, 0, -1):\n            if n_neurons % i == 0:\n                return (i, n_neurons // i)  # Devolver filas y columnas\n    return (n_neurons, 1)  # Si no encuentra, retornar en forma de vector (n_neurons, 1)\n\ndef visualizacion_pesos_mlp(mlp):\n    # Definir la figura con 3 filas y 5 columnas\n    fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n\n    # Asignar las dimensiones para visualizar cada capa\n    layer_shapes = [encontrar_dim_imagen(layer.shape[0]) for layer in mlp.coefs_]\n\n    # Recorrer cada capa de coeficientes del MLP\n    for layer_index, (layer_coefs, ax_row) in enumerate(zip(mlp.coefs_, axes)):\n        # Seleccionar aleatoriamente 5 neuronas de la capa actual\n        num_neurons = layer_coefs.shape[1]\n        random_neurons = random.sample(range(num_neurons), 5)\n\n        # Obtener la forma de visualización para esta capa\n        layer_shape = layer_shapes[layer_index]\n\n        vmin, vmax = layer_coefs.min(), layer_coefs.max()\n\n        # Visualizar las neuronas seleccionadas\n        for neuron_index, ax in zip(random_neurons, ax_row):\n            # Seleccionar los pesos de la neurona específica y reestructurarlos en una matriz 2D\n            neuron_weights = layer_coefs[:, neuron_index].reshape(layer_shape)\n            # Dibujar la imagen de los pesos de la neurona\n            ax.matshow(neuron_weights, cmap=plt.cm.gray, vmin=0.5 * vmin, vmax=0.5 * vmax)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.set_title(f'Capa {layer_index+1}, Neurona {neuron_index}')\n\n    plt.suptitle('Visualización de Pesos de las Neuronas en las Capas Ocultas')\n    plt.tight_layout()\n    plt.show()\n\ndef plot_loss_historia_keras(history):\n    # Graficar el histórico de pérdida durante el entrenamiento\n    plt.plot(history.history['loss'], label='Pérdida de Entrenamiento')\n    plt.plot(history.history['val_loss'], label='Pérdida de Validación')\n    plt.title('Pérdida durante el Entrenamiento')\n    plt.xlabel('Época')\n    plt.ylabel('Pérdida')\n    plt.legend()\n    plt.show()\n\ndef plot_acc_historia_keras(history):\n    # Graficar la precisión durante el entrenamiento\n    plt.plot(history.history['accuracy'], label='Precisión de Entrenamiento')\n    plt.plot(history.history['val_accuracy'], label='Precisión de Validación')\n    plt.title('Precisión durante el Entrenamiento')\n    plt.xlabel('Época')\n    plt.ylabel('Precisión')\n    plt.legend()\n    plt.show()\n\ndef visualizacion_pesos_mlp_keras(model):\n    # Obtener los pesos del modelo (par de listas [pesos, biases] para cada capa)\n    weights = model.get_weights()\n\n    # Extraer solo los pesos de cada capa oculta, ignorando los bias\n    layer_weights = [weights[i] for i in range(0, len(weights), 2)]  # Solo los pesos, no los sesgos\n\n    # Definir la figura con 3 filas (una por cada capa) y 5 columnas (5 neuronas al azar)\n    fig, axes = plt.subplots(len(layer_weights), 5, figsize=(15, 9))\n\n    # Calcular las formas de cada capa de manera dinámica\n    layer_shapes = [encontrar_dim_imagen(layer.shape[0]) for layer in layer_weights]\n\n    # Recorrer cada capa y sus pesos\n    for layer_index, (layer_coefs, ax_row) in enumerate(zip(layer_weights, axes)):\n        # Seleccionar aleatoriamente 5 neuronas de la capa actual\n        num_neurons = layer_coefs.shape[1]\n        random_neurons = random.sample(range(num_neurons), 5)\n\n        # Obtener la forma de visualización para esta capa\n        layer_shape = layer_shapes[layer_index]\n        vmin, vmax = layer_coefs.min(), layer_coefs.max()\n\n\n        # Visualizar las neuronas seleccionadas\n        for neuron_index, ax in zip(random_neurons, ax_row):\n            # Seleccionar los pesos de la neurona específica y reestructurarlos en una matriz 2D\n            neuron_weights = layer_coefs[:, neuron_index].reshape(layer_shape)\n            # Dibujar la imagen de los pesos de la neurona\n            ax.matshow(neuron_weights, cmap=plt.cm.gray, vmin=0.5 * vmin, vmax=0.5 * vmax)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.set_title(f'Capa {layer_index+1}, Neurona {neuron_index}')\n\n    plt.suptitle('Visualización de Pesos de las Neuronas en las Capas Ocultas de Keras')\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html#evaluación-completa",
    "href": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html#evaluación-completa",
    "title": "Implementado un Perceptrón multi-capa usando frameworks y MLFlow",
    "section": "Evaluación completa",
    "text": "Evaluación completa\nRealizaremos una evaluación completa revisando el rendimiento en ambos conjuntos, seguidamente generaremos el reporte de clasificación y la matriz de confusión.\n\nprint(f\"Training set score: {mlp.score(X_train, y_train):.3f}\")\nprint(f\"Test set score: {mlp.score(X_test, y_test):.3f}\")\n\nTraining set score: 1.000\nTest set score: 0.982\n\n\n\n# re abrir un run anterior para registrar más datos\nmlflow.start_run(run_id=run_id)\n\n# Realizar predicciones\ny_pred = mlp.predict(X_test)\n\n# Imprimir el reporte de métricas\nprint(\"Reporte de Clasificación del MLP en MNIST:\\n\")\nreport = classification_report(y_test, y_pred)\nprint(report)\n\n# registrar artefacto\nwith open(\"classification_report.json\", \"w\") as f:\n    json.dump(report, f)\nmlflow.log_artifact(\"classification_report.json\")\n\n# Generar la matriz de confusión\ncm = confusion_matrix(y_test, y_pred)\n\n# visualizar la matriz de confusión\nplot_matriz_confusion(cm, nombre='confusion_matrix')\n# Guardar la visualización como imagen y registrarla en MLflow\nmlflow.log_artifact(\"confusion_matrix.png\")\n\n# Finalizar el `run` si ya no se va a registrar nada más\nmlflow.end_run()\n\nReporte de Clasificación del MLP en MNIST:\n\n              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99      2058\n           1       0.99      0.99      0.99      2364\n           2       0.98      0.98      0.98      2133\n           3       0.98      0.98      0.98      2176\n           4       0.98      0.98      0.98      1936\n           5       0.98      0.98      0.98      1915\n           6       0.98      0.99      0.99      2088\n           7       0.98      0.98      0.98      2248\n           8       0.98      0.97      0.97      1992\n           9       0.98      0.97      0.98      2090\n\n    accuracy                           0.98     21000\n   macro avg       0.98      0.98      0.98     21000\nweighted avg       0.98      0.98      0.98     21000\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\nEjecutar MLFlow UI\n\n# Ejecutar tracking UI en background\nget_ipython().system_raw(\"mlflow ui --backend-store-uri sqlite:///mlruns.db --port 5000 &\")\n\n\n# Crear un tunel remoto usando ngrok.com\n!pip install -U pyngrok --quiet\nfrom pyngrok import ngrok\n\n# Terminate open tunnels if exist\nngrok.kill()\n\n# Colocar el token\n# Coloque su authtoken obtenido de https://dashboard.ngrok.com/auth\nNGROK_AUTH_TOKEN = \"2oR7ctXTRQ7kJl8csoQXXoxsb6V_72fsChgD1kFQN5MkVr7U3\"\nngrok.set_auth_token(NGROK_AUTH_TOKEN)\n\n# Abrir el tunel http en el puerto 5000 para http://localhost:5000\npublic_url = ngrok.connect(\"http://localhost:5000\", proto='http')\nprint(\"MLflow Tracking UI:\", public_url)\n\nMLflow Tracking UI: NgrokTunnel: \"https://3ec3-34-125-33-68.ngrok-free.app\" -&gt; \"http://localhost:5000\""
  },
  {
    "objectID": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html#visualización-de-pesos",
    "href": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html#visualización-de-pesos",
    "title": "Implementado un Perceptrón multi-capa usando frameworks y MLFlow",
    "section": "Visualización de pesos",
    "text": "Visualización de pesos\n\nvisualizacion_pesos_mlp(mlp)\n\n\n\n\n\n\n\n\n\nTutoriales relacionados\n\nAnálisis de la variación del parametro de regularización alpha\nComparación de las diferentes estrategias de aprendizaje"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html#evaluación-completa-1",
    "href": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html#evaluación-completa-1",
    "title": "Implementado un Perceptrón multi-capa usando frameworks y MLFlow",
    "section": "Evaluación completa",
    "text": "Evaluación completa\n\ny_test_categorical = to_categorical(y_test)\n\nscore = mlp_keras.evaluate(X_test.values.astype(float), y_test_categorical, batch_size=128)\n\nscore\n\n\n165/165 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - accuracy: 0.9481 - loss: 0.1761\n\n\n\n\n[0.17650671303272247, 0.9479047656059265]\n\n\n\n# Realizar predicciones en el conjunto de prueba\ny_pred = mlp_keras.predict(X_test.values.astype(float))\n\n# Convertir las predicciones en etiquetas (la clase con mayor probabilidad)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true = y_test.values.astype(int)  # Las etiquetas reales del conjunto de prueba\n\n\n657/657 ━━━━━━━━━━━━━━━━━━━━ 4s 6ms/step\n\n\n\n\n\n# Generar el reporte de clasificación\nprint(\"Reporte de Clasificación para el MLP en MNIST:\\n\")\nprint(classification_report(y_true, y_pred_classes))\n\n# Crear la matriz de confusión\ncm = confusion_matrix(y_true, y_pred_classes)\n\n# Visualizar la matriz de confusión usando Seaborn\nplot_matriz_confusion(cm)\n\nReporte de Clasificación para el MLP en MNIST:\n\n              precision    recall  f1-score   support\n\n           0       0.96      0.97      0.97      2058\n           1       0.96      0.98      0.97      2364\n           2       0.96      0.93      0.95      2133\n           3       0.93      0.93      0.93      2176\n           4       0.94      0.96      0.95      1936\n           5       0.95      0.93      0.94      1915\n           6       0.96      0.97      0.96      2088\n           7       0.95      0.95      0.95      2248\n           8       0.95      0.92      0.93      1992\n           9       0.93      0.93      0.93      2090\n\n    accuracy                           0.95     21000\n   macro avg       0.95      0.95      0.95     21000\nweighted avg       0.95      0.95      0.95     21000"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html#visualización-de-pesos-1",
    "href": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html#visualización-de-pesos-1",
    "title": "Implementado un Perceptrón multi-capa usando frameworks y MLFlow",
    "section": "Visualización de pesos",
    "text": "Visualización de pesos\n\nvisualizacion_pesos_mlp_keras(mlp_keras)"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html#actividad-extra-clase",
    "href": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html#actividad-extra-clase",
    "title": "Implementado un Perceptrón multi-capa usando frameworks y MLFlow",
    "section": "Actividad extra clase",
    "text": "Actividad extra clase\nUsando la siguiente documentación: https://mlflow.org/docs/latest/deep-learning/tensorflow/guide/index.html realizar un seguimiento y registro del MLP construido en keras/tensorflow."
  },
  {
    "objectID": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html",
    "href": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html",
    "title": "Implementando un Perceptrón Multi-capa (MLP) desde CERO usando python",
    "section": "",
    "text": "Open In Colab\n#@title Importar librerías\n#importar librerías necesarias\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_moons\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\n#@title Funciones complementarias\ndef plot_dataset(X_train, y_train, X_test, y_test):\n    # Tamaño de paso en la grilla de valores\n    # (para la visualización del espacio de características)\n    h = 0.02\n\n    # Definir los límites del gráfico en el eje x e y basados\n    # en los datos de entrenamiento\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n\n    # Crear una malla de puntos para cubrir el espacio de características\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n    # Creación del lienzo para visualizar los datos\n    fig, ax = plt.subplots(1,1, figsize=(8, 5))\n\n    # Agregar titulo a la grafica\n    ax.set_title(\"Dataset linealmente no separable\")\n\n    # Agregar nombres a cada eje de caracteristica\n    ax.set_xlabel(\"Característica x_1\")\n    ax.set_ylabel(\"Característica x_2\")\n\n    # Puntos de entrenamiento\n    ax.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1],\n               c=\"#FF0000\", edgecolors=\"k\", label='Clase de entrenamiento 1')\n    ax.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1],\n               c=\"#0000FF\", edgecolors=\"k\", label='Clase de entrenamiento 2')\n\n    # Puntos de prueba\n    ax.scatter(X_test[y_test == 0, 0], X_test[y_test == 0, 1],\n               c=\"#FF0000\", edgecolors=\"k\", alpha=0.6, label='Clase de prueba 1')\n    ax.scatter(X_test[y_test == 1, 0], X_test[y_test == 1, 1],\n               c=\"#0000FF\", edgecolors=\"k\", alpha=0.6, label='Clase de prueba 2')\n\n    # Establecer los límites del gráfico para asegurar que todos los puntos sean visibles\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n\n    # Eliminar las marcas en los ejes x e y para un gráfico más limpio\n    ax.set_xticks(())\n    ax.set_yticks(())\n\n    # Añadir una leyenda para identificar las clases de los\n    # puntos de entrenamiento y prueba\n    ax.legend()\n\n    # mostrar el grafico\n    plt.show()\n\ndef plot_decision_boundary(mlp, X, y, h=0.02):\n    # Crear una malla de puntos para el espacio de características\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # por cada punto de la grilla, hacer una predicción del MLP\n    Z = np.array([mlp.prediccion([np.array([xx.ravel()[i], yy.ravel()[i]])])\n                  for i in range(len(xx.ravel()))])\n\n    # redimensionar para que tenga el mismo shape de la grilla\n    Z = Z.reshape(xx.shape)\n\n    # crear una figura de dos subplots\n    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n\n    # Graficar los puntos originales\n    ax[0].set_title('Puntos originales')\n    ax[0].scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu,\n                  edgecolors='k', alpha=0.6)\n\n    # Graficar los puntos de entrenamiento\n    ax[1].set_title('Frontera de decisión generada por el MLP')\n    ax[1].scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.RdBu)\n    # Graficar la frontera de decisión con un contorno\n    ax[1].contourf(xx, yy, Z, alpha=0.6, cmap=plt.cm.RdBu)\n\n    # mejorar la visualización\n    for i in range(2):\n        ax[i].set_xlim(xx.min(), xx.max())\n        ax[i].set_ylim(yy.min(), yy.max())\n        ax[i].set_xticks(())\n        ax[i].set_yticks(())\n        ax[i].set_xlabel(\"Característica x_1\")\n        ax[i].set_ylabel(\"Característica x_2\")\n\n    plt.show()\n\ndef plot_cost_history(costo_historia):\n    # Grafica el cambio del costo en el entrenamiento\n    plt.figure(figsize=(8, 4))\n    plt.plot(costo_historia, label='Costo')\n    plt.title('Historia del Costo')\n    plt.xlabel('Épocas')\n    plt.ylabel('Costo')\n    plt.legend()\n    plt.grid(True)\n    plt.show()"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#inicialización",
    "href": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#inicialización",
    "title": "Implementando un Perceptrón Multi-capa (MLP) desde CERO usando python",
    "section": "1. Inicialización",
    "text": "1. Inicialización\nAsignación de atributos e inicialización de pesos y biases\n\nclass PerceptronMulticapa():\n    def __init__(self, params=None):\n        # Asignación de hiperparámetros\n        self.capa_entrada = params['capa_entrada']\n        self.capa_oculta = params['capa_oculta']\n        self.capa_salida = params['capa_salida']\n        self.epochs = params['epochs']\n        self.lr = params['lr']\n        self.relu = (lambda x: x*(x &gt; 0))\n        self.derivada_relu = (lambda x: 1 * (x&gt;0))\n        self.sigmoide = (lambda x: 1/(1 + np.exp(-x)))\n        self.derivada_sigmoide = (lambda x: x*(1-x))\n\n        # inicialización de pesos y bias\n        self.inicializacion()\n\n    def inicializacion(self):\n        # inicialización de pesos y bias aleatoria\n        np.random.seed(42) # fijar una semilla para reproducir resultados\n\n        # Capa Oculta\n        self.pesos_capa_oculta = np.random.rand(self.capa_oculta, self.capa_entrada)\n        self.bias_capa_oculta = np.ones((self.capa_oculta, 1))\n\n        # Capa de salida\n        self.pesos_capa_salida = np.random.rand(self.capa_salida, self.capa_oculta)\n        self.bias_capa_salida = np.ones((self.capa_salida, 1))\n\n\n# Instanciamos nuestro perceptrón multicapa\nmlp =  PerceptronMulticapa(params)\n\n\nprint('Dimensión pesos capa oculta: {}'.format(mlp.pesos_capa_oculta.shape))\nprint('Dimensión biases capa oculta: {}'.format(mlp.bias_capa_oculta.shape))\nprint('Dimensión pesos capa salida: {}'.format(mlp.pesos_capa_salida.shape))\nprint('Dimensión bias capa salida: {}'.format(mlp.bias_capa_salida.shape))\n\nDimensión pesos capa oculta: (50, 2)\nDimensión biases capa oculta: (50, 1)\nDimensión pesos capa salida: (1, 50)\nDimensión bias capa salida: (1, 1)\n\n\n\n# ejemplo pesos capa salida\nmlp.pesos_capa_salida\n\narray([[0.03142919, 0.63641041, 0.31435598, 0.50857069, 0.90756647,\n        0.24929223, 0.41038292, 0.75555114, 0.22879817, 0.07697991,\n        0.28975145, 0.16122129, 0.92969765, 0.80812038, 0.63340376,\n        0.87146059, 0.80367208, 0.18657006, 0.892559  , 0.53934224,\n        0.80744016, 0.8960913 , 0.31800347, 0.11005192, 0.22793516,\n        0.42710779, 0.81801477, 0.86073058, 0.00695213, 0.5107473 ,\n        0.417411  , 0.22210781, 0.11986537, 0.33761517, 0.9429097 ,\n        0.32320293, 0.51879062, 0.70301896, 0.3636296 , 0.97178208,\n        0.96244729, 0.2517823 , 0.49724851, 0.30087831, 0.28484049,\n        0.03688695, 0.60956433, 0.50267902, 0.05147875, 0.27864646]])"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#propagación-hacia-adelante-forward",
    "href": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#propagación-hacia-adelante-forward",
    "title": "Implementando un Perceptrón Multi-capa (MLP) desde CERO usando python",
    "section": "2. Propagación hacia adelante (forward)",
    "text": "2. Propagación hacia adelante (forward)\nMétodo de propagación hacia adelante\n\ndef forward_pass(self, x):\n    # Realizar la operacion Wx + b de la capa oculta, x = x_0\n    z = np.matmul(self.pesos_capa_oculta, x) + self.bias_capa_oculta\n    # Aplicar función de activación\n    h = self.relu(z) # z = x_1, h = x_2\n\n    # Aplicar la operación Wh + b para generar la salida, y = x_3\n    y = np.matmul(self.pesos_capa_salida, h) + self.bias_capa_salida\n    # Aplicar función de activación softmax para la clasificación\n    y_pred = self.sigmoide(y) # y = x_4\n\n    return z, h, y_pred\n\n\n# Añadimos nuestro nuevo método\nsetattr(PerceptronMulticapa, 'forward_pass', forward_pass)\n\n\n# seleccionamos una muestra del dataset\n# por ser solo uno se redimensiona para que tenga la estructura de entrada propia\nx_i = X_train[0,:].reshape((-1, 1))\nz, h, y_pred = mlp.forward_pass(x_i)\n\n\nprint('Dimensión biases capa oculta: {}'.format(z.shape))\nprint('Dimensión de la capa oculta: {}'.format(h.shape))\nprint('Predicción: {}'.format(y_pred))\nprint('Capa oculta: {}'.format(h))\n\nDimensión biases capa oculta: (50, 1)\nDimensión de la capa oculta: (50, 1)\nPredicción: [[1.]]\nCapa oculta: [[0.72633077]\n [1.01761811]\n [0.99129875]\n [0.64226347]\n [0.91951467]\n [0.58240149]\n [1.22554416]\n [0.98915428]\n [0.88627666]\n [1.03760999]\n [1.17304912]\n [0.95112959]\n [0.83016191]\n [0.85085938]\n [1.20642355]\n [1.1577875 ]\n [0.60864823]\n [1.01505623]\n [1.07377182]\n [1.06886654]\n [0.82949748]\n [0.61426555]\n [0.80843687]\n [0.89119274]\n [1.12821114]\n [1.03116189]\n [0.96713639]\n [0.82449353]\n [0.94790496]\n [0.87459926]\n [1.02976633]\n [1.1607742 ]\n [0.86948377]\n [0.70204454]\n [0.59561443]\n [1.20847428]\n [0.64438828]\n [0.95081357]\n [1.26279179]\n [1.08640592]\n [1.05700326]\n [1.09879949]\n [0.97640557]\n [0.99963981]\n [1.13251031]\n [0.73290002]\n [1.0450389 ]\n [1.0785398 ]\n [1.0125701 ]\n [0.96240177]]"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#función-para-calcular-el-error",
    "href": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#función-para-calcular-el-error",
    "title": "Implementando un Perceptrón Multi-capa (MLP) desde CERO usando python",
    "section": "3. Función para calcular el error",
    "text": "3. Función para calcular el error\nMétodo que permite conocer el error de una predicción con respecto a la etiqueta real.\n\ndef calcular_perdida_entropia_cruzada(self, y_real, y_pred):\n    epsilon = 1e-12\n    # asegura que los valores de las predicciones esten en un rango\n    # seguro para evitar logaritmos de 0 y 1\n    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n    # calculo de la perdida\n    perdida = -(((1 - y_real) * np.log(1 - y_pred + epsilon)) + (y_real * np.log(y_pred + epsilon)))\n\n    return perdida\n\n\n# Añadimos nuestro nuevo método\nsetattr(PerceptronMulticapa, 'calcular_perdida_entropia_cruzada', calcular_perdida_entropia_cruzada)\n\n\n# Probamos nuestra función de error\ny_real = y_train[0]\nerror = mlp.calcular_perdida_entropia_cruzada(y_real, y_pred)\nprint(f'Error: {error}')\n\nError: [[6.67165212e-11]]"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#propagación-hacía-atrás-backward",
    "href": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#propagación-hacía-atrás-backward",
    "title": "Implementando un Perceptrón Multi-capa (MLP) desde CERO usando python",
    "section": "4. Propagación hacía atrás (backward)",
    "text": "4. Propagación hacía atrás (backward)\nMétodo para propagar los errores hacía atrás.\n\ndef backward_pass(self, x, z, y_real, h, y_pred):\n    # Propagación de error en la capa de salida\n    # Calculo de error en la capa de salida g_out\n    #error_salida =  (y_pred - y_real) * self.derivada_sigmoide(y_pred)\n    error_salida =  y_pred - y_real\n\n    # gradiente de los pesos respecto a la capa de salida\n    # X_in * g_out = error_salida * h.T\n    # X_in = h es la entrada a la capa de salida\n    self.gradiente_pesos_capa_salida = np.matmul(error_salida, h.T)\n    # gradiente de los bias respecto a la capa de salida\n    self.gradiente_bias_capa_salida = error_salida\n\n    # Propagación de error en la capa oculta\n    # gradiente respecto a la capa oculta\n    # (g_out * W) * relu'(X_in)\n    # X_in en esta capa es la salida de aplicar la primera transformación\n    error_oculta = np.matmul(self.pesos_capa_salida.T, error_salida) * self.derivada_relu(z)\n    # gradientes con respecto a la capa oculta, de nuevo g_out * X_in\n    self.gradiente_pesos_capa_oculta = np.matmul(error_oculta, x.T)\n    self.gradiente_bias_capa_oculta = error_oculta\n\n\n# Añadimos nuestro nuevo método\nsetattr(PerceptronMulticapa, 'backward_pass', backward_pass)\n\n\n# calcular propagación de errores\nmlp.backward_pass(x_i, z, y_real, h, y_pred)\n\n\nprint('Dimensión gradientes capa oculta: {}'.format(mlp.gradiente_pesos_capa_oculta.shape))\nprint('Gradientes capa oculta: {}'.format(mlp.gradiente_pesos_capa_oculta))\n\nDimensión gradientes capa oculta: (50, 2)\nGradientes capa oculta: [[-8.14789923e-13  9.33629314e-13]\n [-1.64987027e-11  1.89050846e-11]\n [-8.14956162e-12  9.33819800e-12]\n [-1.31845056e-11  1.51075026e-11]\n [-2.35283225e-11  2.69599942e-11]\n [-6.46280811e-12  7.40542677e-12]\n [-1.06390243e-11  1.21907558e-11]\n [-1.95873816e-11  2.24442561e-11]\n [-5.93150715e-12  6.79663407e-12]\n [-1.99567547e-12  2.28675032e-12]\n [-7.51169840e-12  8.60730064e-12]\n [-4.17960177e-12  4.78920839e-12]\n [-2.41020651e-11  2.76174187e-11]\n [-2.09502196e-11  2.40058678e-11]\n [-1.64207563e-11  1.88157695e-11]\n [-2.25922909e-11  2.58874398e-11]\n [-2.08348990e-11  2.38737273e-11]\n [-4.83675923e-12  5.54221410e-12]\n [-2.31392593e-11  2.65141851e-11]\n [-1.39822466e-11  1.60215964e-11]\n [-2.09325850e-11  2.39856612e-11]\n [-2.32308329e-11  2.66191149e-11]\n [-8.24412154e-12  9.44654975e-12]\n [-2.85305512e-12  3.26918119e-12]\n [-5.90913412e-12  6.77099787e-12]\n [-1.10726102e-11  1.26875814e-11]\n [-2.12067279e-11  2.42997885e-11]\n [-2.23141195e-11  2.55686963e-11]\n [-1.80231392e-13  2.06518646e-13]\n [-1.32409334e-11  1.51721606e-11]\n [-1.08212247e-11  1.23995306e-11]\n [-5.75806219e-12  6.59789168e-12]\n [-3.10746496e-12  3.56069743e-12]\n [-8.75254746e-12  1.00291310e-11]\n [-2.44445826e-11  2.80098934e-11]\n [-8.37891553e-12  9.60100382e-12]\n [-1.34494535e-11  1.54110939e-11]\n [-1.82255044e-11  2.08837453e-11]\n [-9.42696189e-12  1.08019107e-11]\n [-2.51930882e-11  2.88675707e-11]\n [-2.49510873e-11  2.85902733e-11]\n [-6.52736216e-12  7.47939620e-12]\n [-1.28909822e-11  1.47711680e-11]\n [-7.80015802e-12  8.93783290e-12]\n [-7.38438363e-12  8.46141666e-12]\n [-9.56280359e-13  1.09575653e-12]\n [-1.58027281e-11  1.81076003e-11]\n [-1.30317663e-11  1.49324859e-11]\n [-1.33456744e-12  1.52921783e-12]\n [-7.22380571e-12  8.27741800e-12]]"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#entrenamiento",
    "href": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#entrenamiento",
    "title": "Implementando un Perceptrón Multi-capa (MLP) desde CERO usando python",
    "section": "5. Entrenamiento",
    "text": "5. Entrenamiento\nMétodo para iterar sobre todo el conjunto de datos y entrenar la red. Antes se deberá generar otro método que haga la respectiva actualización de pesos, una vez ya los gradientes son calculados.\n\ndef actualizar_pesos(self):\n    # actualizar pesos aplicando gradiente descendiente\n    self.pesos_capa_salida -= self.lr * self.gradiente_pesos_capa_salida\n    self.bias_capa_salida -= self.lr * self.gradiente_bias_capa_salida\n    self.pesos_capa_oculta -= self.lr * self.gradiente_pesos_capa_oculta\n    self.bias_capa_oculta -= self.lr * self.gradiente_bias_capa_oculta\n\ndef entrenar(self, X, y):\n    # almacenar el costo de cada iteración\n    self.costo_historia = []\n\n    # iterar sobre el número de épocas\n    for iteracion in tqdm(range(self.epochs), desc='Iteraciones'):\n        # iterar sobre los datos de entrenamiento\n        error_total = 0 # error para la iteración i\n        # iterar sobre todo el conjunto de datos\n        for i, (x_i, y_i) in tqdm(enumerate(zip(X,y)), desc='Datos', leave=False):\n            # asegurar de que la entrada y la salida solo tenga una columna\n            x_i = x_i.reshape(-1, 1)\n            y_i = y_i.reshape(-1, 1)\n            # aplicar propagación hacia adelante\n            z, h, y_pred = self.forward_pass(x_i)\n            # calcular la perdida de entropia cruzada\n            perdida = self.calcular_perdida_entropia_cruzada(y_i, y_pred)\n            error_total += perdida\n            # aplicar propagación hacia atras\n            self.backward_pass(x_i, z,  y_i, h, y_pred)\n            # actualizar pesos y bias usando gradiente descendiente\n            self.actualizar_pesos()\n\n        # almacenar los costos de cada iteración\n        self.costo = error_total / len(X)\n        self.costo_historia.append(self.costo)\n\n    #print(f'costo: {self.costo[0][0]} en la iteración: {iteracion}')\n\n\n# Añadimos nuestros nuevos métodos\nsetattr(PerceptronMulticapa, 'actualizar_pesos', actualizar_pesos)\nsetattr(PerceptronMulticapa, 'entrenar', entrenar)\n\n\n# realizamos el respectivo entrenamiento\nmlp.entrenar(X_train, y_train)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint(f'Costo final: {mlp.costo}')\n\nCosto final: [[0.44965122]]\n\n\n\n# Visualizar el cambio del costo durante el entrenamiento\nplot_cost_history(np.array(mlp.costo_historia).ravel())"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#evaluación",
    "href": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#evaluación",
    "title": "Implementando un Perceptrón Multi-capa (MLP) desde CERO usando python",
    "section": "Evaluación",
    "text": "Evaluación\nPara evaluar nuestro modelo con la información de test, creamos primero la función de predicción y seguidamente evaluamos el rendimiento en test.\n\ndef prediccion(self, X):\n    predicciones = []\n    for x_i in X:\n        x_i = x_i.reshape(-1, 1)  # Asegurar que x_i sea una columna\n        z, _, y_pred = self.forward_pass(x_i)\n        predicciones.append(y_pred)\n    return np.array(predicciones).flatten()\n\ndef evaluar(self, X, y, umbral):\n    # generar predicciones\n    y_pred = self.prediccion(X)\n    # convertir a 0 y 1 bajo un umbral\n    y_pred = np.where(y_pred &gt;= umbral, 1, 0)\n    # generar reporte de clasificación y matriz de confusión\n    print(classification_report(y, y_pred))\n    cm = confusion_matrix(y, y_pred)\n\n    # visualizacion de la matriz de confusion\n    ConfusionMatrixDisplay(cm).plot()\n    plt.show()\n\n    return y_pred, cm\n\n\n# Añadimos nuestros nuevos métodos\nsetattr(PerceptronMulticapa, 'prediccion', prediccion)\nsetattr(PerceptronMulticapa, 'evaluar', evaluar)\n\n\n# evaluar y visualizar la matriz de confusion usando sklearn\ny_pred, cm = mlp.evaluar(X_test, y_test, umbral=0.5)\n\n              precision    recall  f1-score   support\n\n           0       0.81      0.81      0.81        26\n           1       0.74      0.74      0.74        19\n\n    accuracy                           0.78        45\n   macro avg       0.77      0.77      0.77        45\nweighted avg       0.78      0.78      0.78        45"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#interpretabilidad",
    "href": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#interpretabilidad",
    "title": "Implementando un Perceptrón Multi-capa (MLP) desde CERO usando python",
    "section": "7. Interpretabilidad",
    "text": "7. Interpretabilidad\nGráficamos algunos aspectos de interpretabilidad como la frontera de decisión generada.\n\nplot_decision_boundary(mlp, X, y, h=0.02)"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#clase-python-perceptronmulticapa-completo",
    "href": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#clase-python-perceptronmulticapa-completo",
    "title": "Implementando un Perceptrón Multi-capa (MLP) desde CERO usando python",
    "section": "Clase python PerceptronMulticapa Completo",
    "text": "Clase python PerceptronMulticapa Completo\n\nclass PerceptronMulticapa():\n    def __init__(self, params=None):\n        # Asignación de hiperparámetros\n        self.capa_entrada = params['capa_entrada']\n        self.capa_oculta = params['capa_oculta']\n        self.capa_salida = params['capa_salida']\n        self.epochs = params['epochs']\n        self.lr = params['lr']\n        self.relu = (lambda x: x*(x &gt; 0))\n        self.derivada_relu = (lambda x: 1 * (x&gt;0))\n        self.sigmoide = (lambda x: 1/(1 + np.exp(-x)))\n        self.derivada_sigmoide = (lambda x: x*(1-x))\n\n        # inicialización de pesos y bias\n        self.inicializacion()\n\n    def inicializacion(self):\n        # inicialización de pesos y bias aleatoria\n        np.random.seed(42) # fijar una semilla para reproducir resultados\n\n        # Capa Oculta\n        self.pesos_capa_oculta = np.random.rand(self.capa_oculta, self.capa_entrada)\n        self.bias_capa_oculta = np.ones((self.capa_oculta, 1))\n\n        # Capa de salida\n        self.pesos_capa_salida = np.random.rand(self.capa_salida, self.capa_oculta)\n        self.bias_capa_salida = np.ones((self.capa_salida, 1))\n\n    def forward_pass(self, x):\n        # Realizar la operacion Wx + b de la capa oculta, x = x_0\n        z = np.matmul(self.pesos_capa_oculta, x) + self.bias_capa_oculta\n        # Aplicar función de activación\n        h = self.relu(z) # z = x_1, h = x_2\n\n        # Aplicar la operación Wh + b para generar la salida, y = x_3\n        y = np.matmul(self.pesos_capa_salida, h) + self.bias_capa_salida\n        # Aplicar función de activación softmax para la clasificación\n        y_pred = self.sigmoide(y) # y = x_4\n\n        return z, h, y_pred\n\n    def actualizar_pesos(self):\n        # actualizar pesos aplicando gradiente descendiente\n        self.pesos_capa_salida -= self.lr * self.gradiente_pesos_capa_salida\n        self.bias_capa_salida -= self.lr * self.gradiente_bias_capa_salida\n        self.pesos_capa_oculta -= self.lr * self.gradiente_pesos_capa_oculta\n        self.bias_capa_oculta -= self.lr * self.gradiente_bias_capa_oculta\n\n    def backward_pass(self, x, z, y_real, h, y_pred):\n        # Propagación de error en la capa de salida\n        # Calculo de error en la capa de salida g_out\n        #error_salida =  (y_pred - y_real) * self.derivada_sigmoide(y_pred)\n        error_salida =  y_pred - y_real\n\n        # gradiente de los pesos respecto a la capa de salida\n        # X_in * g_out = error_salida * h.T\n        # X_in = h es la entrada a la capa de salida\n        self.gradiente_pesos_capa_salida = np.matmul(error_salida, h.T)\n        # gradiente de los bias respecto a la capa de salida\n        self.gradiente_bias_capa_salida = error_salida\n\n        # Propagación de error en la capa oculta\n        # gradiente respecto a la capa oculta\n        # (g_out * W) * relu'(X_in)\n        # X_in en esta capa es la salida de aplicar la primera transformación\n        error_oculta = np.matmul(self.pesos_capa_salida.T, error_salida) * self.derivada_relu(z)\n        # gradientes con respecto a la capa oculta, de nuevo g_out * X_in\n        self.gradiente_pesos_capa_oculta = np.matmul(error_oculta, x.T)\n        self.gradiente_bias_capa_oculta = error_oculta\n\n    def entrenar(self, X, y):\n        # almacenar el costo de cada iteración\n        self.costo_historia = []\n\n        # iterar sobre el número de épocas\n        for iteracion in tqdm(range(self.epochs), desc='Iteraciones'):\n            # iterar sobre los datos de entrenamiento\n            error_total = 0 # error para la iteración i\n            # iterar sobre todo el conjunto de datos\n            for i, (x_i, y_i) in tqdm(enumerate(zip(X,y)), desc='Datos', leave=False):\n                # asegurar de que la entrada y la salida solo tenga una columna\n                x_i = x_i.reshape(-1, 1)\n                y_i = y_i.reshape(-1, 1)\n                # aplicar propagación hacia adelante\n                z, h, y_pred = self.forward_pass(x_i)\n                # calcular la perdida de entropia cruzada\n                perdida = self.calcular_perdida_entropia_cruzada(y_i, y_pred)\n                error_total += perdida\n                # aplicar propagación hacia atras\n                self.backward_pass(x_i, z,  y_i, h, y_pred)\n                # actualizar pesos y bias usando gradiente descendiente\n                self.actualizar_pesos()\n\n            # almacenar los costos de cada iteración\n            self.costo = error_total / len(X)\n            self.costo_historia.append(self.costo)\n\n        #print(f'costo: {self.costo[0][0]} en la iteración: {iteracion}')\n\n    def calcular_perdida_entropia_cruzada(self, y_real, y_pred):\n        epsilon = 1e-12\n        # asegura que los valores de las predicciones esten en un rango\n        # seguro para evitar logaritmos de 0 y 1\n        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n        # calculo de la perdida\n        perdida = -(((1 - y_real) * np.log(1 - y_pred + epsilon)) + (y_real * np.log(y_pred + epsilon)))\n\n        return perdida\n\n    def prediccion(self, X):\n        predicciones = []\n        for x_i in X:\n            x_i = x_i.reshape(-1, 1)  # Asegurar que x_i sea una columna\n            z, _, y_pred = self.forward_pass(x_i)\n            predicciones.append(y_pred)\n        return np.array(predicciones).flatten()\n\n    def evaluar(self, X, y, umbral):\n        # generar predicciones\n        y_pred = self.prediccion(X)\n        # convertir a 0 y 1 bajo un umbral\n        y_pred = np.where(y_pred &gt;= umbral, 1, 0)\n        # generar reporte de clasificación y matriz de confusión\n        print(classification_report(y, y_pred))\n        cm = confusion_matrix(y, y_pred)\n\n        # visualizacion de la matriz de confusion\n        ConfusionMatrixDisplay(cm).plot()\n        plt.show()\n\n        return y_pred, cm"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Curso Práctico: Neural Networks y Deep Learning",
    "section": "",
    "text": "¡Bienvenido al sitio web del curso práctico intensivo de Neural Networks y Deep Learning!\nEste curso está diseñado para proporcionarte una comprensión sólida y aplicada de los conceptos fundamentales de las redes neuronales y su aplicación en el campo de la inteligencia artificial, con un enfoque marcado en la implementación práctica y la resolución de problemas reales.\nA lo largo de tres semanas intensivas, explorarás desde los principios básicos hasta arquitecturas avanzadas y técnicas de vanguardia, preparándote para enfrentar desafíos complejos en el mundo del Deep Learning."
  },
  {
    "objectID": "index.html#qué-exploraremos",
    "href": "index.html#qué-exploraremos",
    "title": "Curso Práctico: Neural Networks y Deep Learning",
    "section": "¿Qué Exploraremos?",
    "text": "¿Qué Exploraremos?\nEl curso se estructura en tres unidades clave:\n\nFundamentos de Redes Neuronales: Introduce los conceptos esenciales, la estructura y el entrenamiento de modelos, abordando el perceptrón, la retropropagación y métodos de optimización como el gradiente descendente. Sentarás las bases para entender cómo aprenden las redes mientras aplicas de manera práctica los conceptos.\nArquitecturas de Redes Neuronales: Explorarás diversas arquitecturas, incluyendo Redes Neuronales Densas (DNN), Convolucionales (CNN) y Recurrentes (RNN). Ampliarás tu visión con una introducción a arquitecturas avanzadas como transformers y desarrollarás la habilidad para seleccionar la más adecuada según el problema.\nTécnicas Avanzadas y Robustez: Te centrarás en la construcción de modelos avanzados, cubriendo la generalización, la transferencia de aprendizaje, optimización de hiperparámetros y la explicabilidad. Se introducirá el entrenamiento adversarial y técnicas para crear modelos robustos y transferibles que funcionen bien en entornos de datos diversos."
  },
  {
    "objectID": "index.html#enfoque-práctico",
    "href": "index.html#enfoque-práctico",
    "title": "Curso Práctico: Neural Networks y Deep Learning",
    "section": "Enfoque Práctico",
    "text": "Enfoque Práctico\nEste curso es eminentemente práctico. A través de Implementaciones en Python y trabajo directo con notebooks, aplicarás los conceptos teóricos de inmediato. El objetivo es que no solo comprendas cómo funcionan las redes neuronales, sino que también ganes experiencia práctica en cómo construirlas y utilizarlas para resolver problemas reales.\nCada semana incluye componentes prácticos diseñados para consolidar tu aprendizaje y permitirte experimentar con diferentes arquitecturas y técnicas."
  },
  {
    "objectID": "index.html#estructura-del-sitio",
    "href": "index.html#estructura-del-sitio",
    "title": "Curso Práctico: Neural Networks y Deep Learning",
    "section": "Estructura del Sitio",
    "text": "Estructura del Sitio\n\nSemana 1: Fundamentos y MLP\nSemana 2: Arquitecturas neuronales profundas\nSemana 3: Técnicas Avanzadas y Robustez\n\nExplora cada sección para encontrar los materiales de la semana, los notebooks (renderizados para visualización web) y las slides correspondientes."
  },
  {
    "objectID": "semana_1/index.html",
    "href": "semana_1/index.html",
    "title": "Semana 1: Fundamentos de Redes Neuronales",
    "section": "",
    "text": "Esta semana sentamos las bases del Deep Learning. Cubriremos los conceptos esenciales, la estructura y el entrenamiento de las redes neuronales.\nTemas Clave:\n\nEstructura y entrenamiento de modelos en Deep Learning\nEl Perceptrón\nProceso de Retropropagación\nMétodos de optimización (Gradiente Descendente)\n\nEnfoque Práctico: Comprenderás el funcionamiento interno de una red y aplicarás técnicas de entrenamiento en modelos simples. Realizarás implementaciones prácticas para observar cómo aprenden los modelos.\nMateriales de la Semana:\n\nSlides de la Semana 1 (Próximamente)\nNotebook: Implementando Perceptrón Multicapa desde CERO\nNotebook: MLP usando Frameworks"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1b_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks.html",
    "href": "semana_1/notebooks/Nb_1b_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks.html",
    "title": "Implementado un Perceptrón multi-capa usando frameworks",
    "section": "",
    "text": "Open In Colab\n#@title Importar librerías\n#importar librerías necesarias\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn import set_config\nset_config(display='diagram')\nimport seaborn as sns\nimport random\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Input\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.utils import plot_model\n#@title Funciones complementarias\ndef plot_samples_dataset(X, y):\n    # Convertir las etiquetas a enteros\n    y = y.astype(int)\n\n    # Crear la grilla de 4x4\n    fig, axes = plt.subplots(4, 4, figsize=(6, 6))\n    fig.suptitle('Grilla de imágenes del dataset MNIST')\n\n    # Iterar para mostrar las primeras 16 imágenes con sus etiquetas\n    for i, ax in enumerate(axes.flat):\n        img = X.iloc[i].values.reshape(28, 28)\n        label = y[i]\n        ax.imshow(img, cmap='gray')\n        ax.set_title(f'Label: {label}')\n        ax.axis('off')\n\n    plt.show()\n\ndef plot_curva_aprendizaje(mlp):\n    plt.figure(figsize=(8, 5))\n    plt.plot(mlp.loss_curve_, marker='o')\n    plt.title('Pérdida durante el entrenamiento del MLP por iteración')\n    plt.xlabel('Iteración')\n    plt.ylabel('Pérdida (Loss)')\n    plt.grid()\n    plt.show()\n\ndef plot_matriz_confusion(cm):\n    # Visualizar la matriz de confusión usando Seaborn\n    plt.figure(figsize=(10, 7))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n    plt.xlabel('Etiqueta predicha')\n    plt.ylabel('Etiqueta real')\n    plt.title('Matriz de Confusión para el MLP en el dataset MNIST')\n    plt.show()\n\ndef encontrar_dim_imagen(n_neurons):\n    \"\"\"\n    Encuentra la mejor forma cuadrada (filas, columnas) para una cantidad dada de neuronas.\n    \"\"\"\n    side_length = int(np.sqrt(n_neurons))  # Calcular la raíz cuadrada del número de neuronas\n    if side_length * side_length == n_neurons:\n        return (side_length, side_length)  # Si es un cuadrado perfecto\n    else:\n        # Si no es un cuadrado perfecto, buscamos la mejor aproximación (filas, columnas)\n        for i in range(side_length, 0, -1):\n            if n_neurons % i == 0:\n                return (i, n_neurons // i)  # Devolver filas y columnas\n    return (n_neurons, 1)  # Si no encuentra, retornar en forma de vector (n_neurons, 1)\n\ndef visualizacion_pesos_mlp(mlp):\n    # Definir la figura con 3 filas y 5 columnas\n    fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n\n    # Asignar las dimensiones para visualizar cada capa\n    layer_shapes = [encontrar_dim_imagen(layer.shape[0]) for layer in mlp.coefs_]\n\n    # Recorrer cada capa de coeficientes del MLP\n    for layer_index, (layer_coefs, ax_row) in enumerate(zip(mlp.coefs_, axes)):\n        # Seleccionar aleatoriamente 5 neuronas de la capa actual\n        num_neurons = layer_coefs.shape[1]\n        random_neurons = random.sample(range(num_neurons), 5)\n\n        # Obtener la forma de visualización para esta capa\n        layer_shape = layer_shapes[layer_index]\n\n        vmin, vmax = layer_coefs.min(), layer_coefs.max()\n\n        # Visualizar las neuronas seleccionadas\n        for neuron_index, ax in zip(random_neurons, ax_row):\n            # Seleccionar los pesos de la neurona específica y reestructurarlos en una matriz 2D\n            neuron_weights = layer_coefs[:, neuron_index].reshape(layer_shape)\n            # Dibujar la imagen de los pesos de la neurona\n            ax.matshow(neuron_weights, cmap=plt.cm.gray, vmin=0.5 * vmin, vmax=0.5 * vmax)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.set_title(f'Capa {layer_index+1}, Neurona {neuron_index}')\n\n    plt.suptitle('Visualización de Pesos de las Neuronas en las Capas Ocultas')\n    plt.tight_layout()\n    plt.show()\n\ndef plot_loss_historia_keras(history):\n    # Graficar el histórico de pérdida durante el entrenamiento\n    plt.plot(history.history['loss'], label='Pérdida de Entrenamiento')\n    plt.plot(history.history['val_loss'], label='Pérdida de Validación')\n    plt.title('Pérdida durante el Entrenamiento')\n    plt.xlabel('Época')\n    plt.ylabel('Pérdida')\n    plt.legend()\n    plt.show()\n\ndef plot_acc_historia_keras(history):\n    # Graficar la precisión durante el entrenamiento\n    plt.plot(history.history['accuracy'], label='Precisión de Entrenamiento')\n    plt.plot(history.history['val_accuracy'], label='Precisión de Validación')\n    plt.title('Precisión durante el Entrenamiento')\n    plt.xlabel('Época')\n    plt.ylabel('Precisión')\n    plt.legend()\n    plt.show()\n\ndef visualizacion_pesos_mlp_keras(model):\n    # Obtener los pesos del modelo (par de listas [pesos, biases] para cada capa)\n    weights = model.get_weights()\n\n    # Extraer solo los pesos de cada capa oculta, ignorando los bias\n    layer_weights = [weights[i] for i in range(0, len(weights), 2)]  # Solo los pesos, no los sesgos\n\n    # Definir la figura con 3 filas (una por cada capa) y 5 columnas (5 neuronas al azar)\n    fig, axes = plt.subplots(len(layer_weights), 5, figsize=(15, 9))\n\n    # Calcular las formas de cada capa de manera dinámica\n    layer_shapes = [encontrar_dim_imagen(layer.shape[0]) for layer in layer_weights]\n\n    # Recorrer cada capa y sus pesos\n    for layer_index, (layer_coefs, ax_row) in enumerate(zip(layer_weights, axes)):\n        # Seleccionar aleatoriamente 5 neuronas de la capa actual\n        num_neurons = layer_coefs.shape[1]\n        random_neurons = random.sample(range(num_neurons), 5)\n\n        # Obtener la forma de visualización para esta capa\n        layer_shape = layer_shapes[layer_index]\n        vmin, vmax = layer_coefs.min(), layer_coefs.max()\n\n\n        # Visualizar las neuronas seleccionadas\n        for neuron_index, ax in zip(random_neurons, ax_row):\n            # Seleccionar los pesos de la neurona específica y reestructurarlos en una matriz 2D\n            neuron_weights = layer_coefs[:, neuron_index].reshape(layer_shape)\n            # Dibujar la imagen de los pesos de la neurona\n            ax.matshow(neuron_weights, cmap=plt.cm.gray, vmin=0.5 * vmin, vmax=0.5 * vmax)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.set_title(f'Capa {layer_index+1}, Neurona {neuron_index}')\n\n    plt.suptitle('Visualización de Pesos de las Neuronas en las Capas Ocultas de Keras')\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1b_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks.html#evaluación-completa",
    "href": "semana_1/notebooks/Nb_1b_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks.html#evaluación-completa",
    "title": "Implementado un Perceptrón multi-capa usando frameworks",
    "section": "Evaluación completa",
    "text": "Evaluación completa\nRealizaremos una evaluación completa revisando el rendimiento en ambos conjuntos, seguidamente generaremos el reporte de clasificación y la matriz de confusión.\n\nprint(f\"Training set score: {mlp.score(X_train, y_train):.3f}\")\nprint(f\"Test set score: {mlp.score(X_test, y_test):.3f}\")\n\nTraining set score: 1.000\nTest set score: 0.982\n\n\n\n# Realizar predicciones\ny_pred = mlp.predict(X_test)\n\n# Imprimir el reporte de métricas\nprint(\"Reporte de Clasificación del MLP en MNIST:\\n\")\nprint(classification_report(y_test, y_pred))\n\n# Generar la matriz de confusión\ncm = confusion_matrix(y_test, y_pred)\n\n# visualizar la matriz de confusión\nplot_matriz_confusion(cm)\n\nReporte de Clasificación del MLP en MNIST:\n\n              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99      2058\n           1       0.99      0.99      0.99      2364\n           2       0.98      0.98      0.98      2133\n           3       0.98      0.98      0.98      2176\n           4       0.98      0.98      0.98      1936\n           5       0.98      0.98      0.98      1915\n           6       0.98      0.99      0.99      2088\n           7       0.98      0.98      0.98      2248\n           8       0.98      0.97      0.97      1992\n           9       0.98      0.97      0.98      2090\n\n    accuracy                           0.98     21000\n   macro avg       0.98      0.98      0.98     21000\nweighted avg       0.98      0.98      0.98     21000"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1b_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks.html#visualización-de-pesos",
    "href": "semana_1/notebooks/Nb_1b_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks.html#visualización-de-pesos",
    "title": "Implementado un Perceptrón multi-capa usando frameworks",
    "section": "Visualización de pesos",
    "text": "Visualización de pesos\n\n# pesos de la primera capa oculta. Todas las neuronas conectadas con cada pixel\nprint('Dimensión de la primera capa oculta: {}'.format(mlp.coefs_[0].shape))\nprint('Dimensión de la segunda capa oculta: {}'.format(mlp.coefs_[1].shape))\nprint('Dimensión de la tercera capa oculta: {}'.format(mlp.coefs_[2].shape))\n\nDimensión de la primera capa oculta: (784, 225)\nDimensión de la segunda capa oculta: (225, 100)\nDimensión de la tercera capa oculta: (100, 10)\n\n\n\nvisualizacion_pesos_mlp(mlp)\n\n\n\n\n\n\n\n\n\nTutoriales relacionados\n\nAnálisis de la variación del parametro de regularización alpha\nComparación de las diferentes estrategias de aprendizaje\nAccelaración de Sklearn (GPU) usando la extensión sklearnex\nAccelaración de Sklearn (GPU) usando CuML"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1b_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks.html#evaluación-completa-1",
    "href": "semana_1/notebooks/Nb_1b_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks.html#evaluación-completa-1",
    "title": "Implementado un Perceptrón multi-capa usando frameworks",
    "section": "Evaluación completa",
    "text": "Evaluación completa\n\ny_test_categorical = to_categorical(y_test)\n\nscore = mlp_keras.evaluate(X_test.values.astype(float), y_test_categorical, batch_size=128)\n\nscore\n\n\n165/165 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.9484 - loss: 0.1750\n\n\n\n\n[0.175654336810112, 0.9481428861618042]\n\n\n\n# Realizar predicciones en el conjunto de prueba\ny_pred = mlp_keras.predict(X_test.values.astype(float))\n\n# Convertir las predicciones en etiquetas (la clase con mayor probabilidad)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true = y_test.values.astype(int)  # Las etiquetas reales del conjunto de prueba\n\n\n657/657 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step\n\n\n\n\n\n# Generar el reporte de clasificación\nprint(\"Reporte de Clasificación para el MLP en MNIST:\\n\")\nprint(classification_report(y_true, y_pred_classes))\n\n# Crear la matriz de confusión\ncm = confusion_matrix(y_true, y_pred_classes)\n\n# Visualizar la matriz de confusión usando Seaborn\nplot_matriz_confusion(cm)\n\nReporte de Clasificación para el MLP en MNIST:\n\n              precision    recall  f1-score   support\n\n           0       0.97      0.98      0.97      2058\n           1       0.96      0.98      0.97      2364\n           2       0.95      0.94      0.94      2133\n           3       0.93      0.94      0.93      2176\n           4       0.94      0.95      0.94      1936\n           5       0.95      0.93      0.94      1915\n           6       0.96      0.97      0.97      2088\n           7       0.95      0.96      0.95      2248\n           8       0.95      0.92      0.93      1992\n           9       0.93      0.92      0.93      2090\n\n    accuracy                           0.95     21000\n   macro avg       0.95      0.95      0.95     21000\nweighted avg       0.95      0.95      0.95     21000"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1b_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks.html#visualización-de-pesos-1",
    "href": "semana_1/notebooks/Nb_1b_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks.html#visualización-de-pesos-1",
    "title": "Implementado un Perceptrón multi-capa usando frameworks",
    "section": "Visualización de pesos",
    "text": "Visualización de pesos\n\nvisualizacion_pesos_mlp_keras(mlp_keras)\n\n\n\n\n\n\n\n\n\n✅ Cómo transformar tus datos para usar un MLP estándar (Keras o scikit-learn)\n\n\n\n\n\n\n\n\nTipo de dato o problema\n¿Qué debes hacer para usar un MLP?\nEjemplo sencillo\n\n\n\n\n📝 Texto\nConvertir el texto a vectores. Usa técnicas como Bag of Words (BoW), TF-IDF.\nClasificación de sentimientos: convertir cada comentario en un vector TF-IDF\n\n\n⏱ Series temporales\nDividir en ventanas de tiempo fijas y calcular características estadísticas (media, std, min, max, energía, etc.) por ventana.\nPredicción de fallas: usar estadísticas de 10s de datos de sensores como entrada al MLP\n\n\n❗ Anomalías\nEtiquetar datos anómalos (si puedes).\nDetección de fraude: marcar transacciones normales y anómalas y entrenar un clasificador\n\n\n🖼 Imágenes\nExtraer características manuales (como color, textura, tamaño, etc.) o redimensionar las imágenes y vectorizarlas.\nClasificación de imágenes de zapatos: usar un modelo CNN preentrenado para extraer features\n\n\n🔊 Audio\nExtraer features de audio como MFCCs, espectrogramas, energía, pitch, etc., y construir vectores con esas estadísticas.\nDetección de emociones en voz: usar MFCCs y energía para representar cada audio\n\n\n\n\n\n🧭 Guía paso a paso para construir tu baseline con un MLP\nSigue estos pasos para convertir tu idea o proyecto en un experimento funcional con un Perceptrón Multicapa (MLP), usando scikit-learn o Keras.\n\n\n1️⃣ Define el objetivo de predicción\n\n¿Tu problema es de clasificación o regresión?\n¿Cuál es la variable que quieres predecir?\nEjemplos:\n\nClasificación: ¿Este artículo es de biología o matemáticas?\nRegresión: ¿Cuál será el consumo energético el próximo mes?\n\n\n\n\n\n2️⃣ Identifica tu tipo de datos\n\n¿Qué tipo de datos tienes?\n\nTexto\nSeries temporales\nDatos tabulares\nImágenes\nAudio\n\n\n🔍 Revisa la tabla anterior para ver cómo transformar tus datos para usarlos con un MLP.\n\n\n\n3️⃣ Preprocesa y vectoriza tus datos\n\nNormaliza tus valores si son numéricos (por ejemplo, entre 0 y 1).\nSi tienes texto, usa TF-IDF o BoW.\nSi tienes imágenes, vectorízalas (flatten).\nSi tienes secuencias, divide en ventanas y calcula estadísticas (media, std, etc.).\n\n📌 Asegúrate de que cada fila sea un ejemplo y cada columna una característica.\n\n\n\n4️⃣ Define el tipo de salida y la función de pérdida\n\nClasificación binaria → sigmoid + binary_crossentropy\nClasificación multiclase → softmax + categorical_crossentropy\nRegresión → linear + mean_squared_error o mean_absolute_error\n\n⚠️ Si usas Keras, recuerda convertir las etiquetas con to_categorical() si usas softmax.\n\n\n\n5️⃣ Construye tu MLP\n\nDecide cuántas capas ocultas y neuronas usar (ej. 2 capas de 128 y 64).\nUsa relu como activación oculta y softmax o sigmoid según el caso.\nAñade regularización (Dropout, L2) si hay riesgo de sobreajuste."
  },
  {
    "objectID": "semana_1/slides/slides_semana_1.html#perceptrón-como-clasificador-básico",
    "href": "semana_1/slides/slides_semana_1.html#perceptrón-como-clasificador-básico",
    "title": "Semana 1: Fundamentos de Redes Neuronales",
    "section": "Perceptrón como Clasificador básico",
    "text": "Perceptrón como Clasificador básico"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2a_Implementando_una_CNN_usando_Keras.html",
    "href": "semana_2/notebooks/Nb_2a_Implementando_una_CNN_usando_Keras.html",
    "title": "Implementado una Red Neuronal Convolucional",
    "section": "",
    "text": "Última actualización 09/05/2025\n#@title Importar librerías\n#importar librerías necesarias\nimport random\nfrom random import randint\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport random\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Input\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.datasets import mnist\n#@title Funciones complementarias\ndef plot_samples_dataset(X, y):\n    # Convertir las etiquetas a enteros\n    y = y.astype(int)\n\n    # Crear la grilla de 4x4\n    fig, axes = plt.subplots(4, 4, figsize=(6, 6))\n    fig.suptitle('Grilla de imágenes del dataset MNIST')\n\n    # Iterar para mostrar las primeras 16 imágenes con sus etiquetas\n    for i, ax in enumerate(axes.flat):\n        img = X.iloc[i].values.reshape(28, 28)\n        label = y[i]\n        ax.imshow(img, cmap='gray')\n        ax.set_title(f'Label: {label}')\n        ax.axis('off')\n\n    plt.show()\n\ndef plot_curva_aprendizaje(mlp):\n    plt.figure(figsize=(8, 5))\n    plt.plot(mlp.loss_curve_, marker='o')\n    plt.title('Pérdida durante el entrenamiento del MLP por iteración')\n    plt.xlabel('Iteración')\n    plt.ylabel('Pérdida (Loss)')\n    plt.grid()\n    plt.show()\n\ndef plot_matriz_confusion(cm):\n    # Visualizar la matriz de confusión usando Seaborn\n    plt.figure(figsize=(10, 7))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n    plt.xlabel('Etiqueta predicha')\n    plt.ylabel('Etiqueta real')\n    plt.title('Matriz de Confusión para el MLP en el dataset MNIST')\n    plt.show()\n\ndef encontrar_dim_imagen(n_neurons):\n    \"\"\"\n    Encuentra la mejor forma cuadrada (filas, columnas) para una cantidad dada de neuronas.\n    \"\"\"\n    side_length = int(np.sqrt(n_neurons))  # Calcular la raíz cuadrada del número de neuronas\n    if side_length * side_length == n_neurons:\n        return (side_length, side_length)  # Si es un cuadrado perfecto\n    else:\n        # Si no es un cuadrado perfecto, buscamos la mejor aproximación (filas, columnas)\n        for i in range(side_length, 0, -1):\n            if n_neurons % i == 0:\n                return (i, n_neurons // i)  # Devolver filas y columnas\n    return (n_neurons, 1)  # Si no encuentra, retornar en forma de vector (n_neurons, 1)\n\ndef plot_loss_historia_keras(history):\n    # Graficar el histórico de pérdida durante el entrenamiento\n    plt.plot(history.history['loss'], label='Pérdida de Entrenamiento')\n    plt.plot(history.history['val_loss'], label='Pérdida de Validación')\n    plt.title('Pérdida durante el Entrenamiento')\n    plt.xlabel('Época')\n    plt.ylabel('Pérdida')\n    plt.legend()\n    plt.show()\n\ndef plot_acc_historia_keras(history):\n    # Graficar la precisión durante el entrenamiento\n    plt.plot(history.history['accuracy'], label='Precisión de Entrenamiento')\n    plt.plot(history.history['val_accuracy'], label='Precisión de Validación')\n    plt.title('Precisión durante el Entrenamiento')\n    plt.xlabel('Época')\n    plt.ylabel('Precisión')\n    plt.legend()\n    plt.show()\n\ndef visualizacion_filtros_cnn_keras(model):\n    # Obtener las capas convolucionales del modelo\n    conv_layers = [layer for layer in model.layers if 'conv' in layer.name]\n\n    # Definir la figura con una fila por capa convolucional y varias columnas (5 filtros al azar por capa)\n    fig, axes = plt.subplots(len(conv_layers), 5, figsize=(10, len(conv_layers) * 3))\n\n    # Recorrer cada capa convolucional y sus pesos\n    for layer_index, (layer, ax_row) in enumerate(zip(conv_layers, axes)):\n        # Obtener los pesos de la capa (solo el primer tensor, ignorar bias)\n        layer_weights = layer.get_weights()[0]  # shape: (filter_height, filter_width, input_channels, num_filters)\n\n        # Seleccionar 5 filtros de la capa actual\n        num_filters = layer_weights.shape[-1]\n        random_filters = random.sample(range(num_filters), 5)\n\n        # Obtener los límites para la normalización de las imágenes\n        vmin, vmax = layer_weights.min(), layer_weights.max()\n\n        # Dibujar cada filtro seleccionado\n        for filter_index, ax in zip(random_filters, ax_row):\n            # Extraer el filtro correspondiente (shape: filter_height, filter_width, input_channels)\n            filter_weights = layer_weights[..., filter_index]\n\n            # Promediar los canales para visualizar como imagen en escala de grises\n            if filter_weights.shape[-1] &gt; 1:\n                filter_weights = np.mean(filter_weights, axis=-1)  # Promedio sobre los canales de entrada\n\n            # Dibujar la imagen del filtro\n            ax.matshow(filter_weights, cmap=plt.cm.gray, vmin=0.5 * vmin, vmax=0.5 * vmax)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.set_title(f'Capa {layer_index + 1}, Filtro {filter_index}')\n\n    plt.suptitle('Visualización de Filtros de las Capas Convolucionales de Keras')\n    plt.tight_layout()\n    plt.show()\n\ndef visualizacion_feature_maps(model, image):\n    # Crear un nuevo modelo que toma la misma entrada pero cuya salida son los mapas de características de cada capa convolucional\n    layer_outputs = [layer.output for layer in model.layers if 'conv' in layer.name]\n    feature_map_model = tf.keras.Model(inputs=model.input, outputs=layer_outputs)\n\n    # Obtener los mapas de características al pasar la imagen a través del modelo\n    feature_maps = feature_map_model.predict(np.expand_dims(image, axis=0))  # Añadir batch dimension\n\n    # Recorrer cada capa convolucional y sus feature maps correspondientes\n    for layer_index, feature_map in enumerate(feature_maps):\n        # Número de filtros en la capa actual\n        num_filters = feature_map.shape[-1]\n\n        # Definir la figura con una fila por cada filtro (limitado a 6 para evitar gráficos muy grandes)\n        fig, axes = plt.subplots(1, min(6, num_filters), figsize=(20, 5))\n\n        # Mostrar cada filtro como imagen en escala de grises\n        for i in range(min(6, num_filters)):  # Mostrar un máximo de 6 filtros\n            ax = axes[i]\n            # Extraer el feature map del filtro `i` y mostrarlo\n            ax.matshow(feature_map[0, :, :, i], cmap='viridis')\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.set_title(f'Filtro {i+1}')\n\n        # Título de la capa y ajuste de la visualización\n        plt.suptitle(f'Visualización de Feature Maps - Capa {layer_index+1}')\n        plt.tight_layout()\n        plt.show()"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2a_Implementando_una_CNN_usando_Keras.html#componentes-base-complementarios",
    "href": "semana_2/notebooks/Nb_2a_Implementando_una_CNN_usando_Keras.html#componentes-base-complementarios",
    "title": "Implementado una Red Neuronal Convolucional",
    "section": "Componentes base complementarios",
    "text": "Componentes base complementarios\n\nCapas de Pooling\nCapas de normalización\n\n\n1. Capas de Pooling\n\n\n\n1_vKYHxr5oI9cBw_hGhjQCrA.webp\n\n\n\n\n2. Capas de normalización\nEn nuestro ejemplo, no es necesario normalizar ya que nuestras entradas estan entre 0 y 1 y no tienen un alta complejidad. Pero en otros escenarios, con imágenes más complejas y en formato RGB la normalización se hace más común.\n\n\n\n1_dsl93qeGPteT3Zt7mBy1dQ-1.webp\n\n\n\n# Asumiendo que X_train y y_train ya están definidos como en el ejemplo anterior\n# Preprocesar las etiquetas para que sean categóricas (one-hot encoding)\ny_train_categorical = to_categorical(y_train)\ny_train_categorical\n\narray([[1., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 1., 0., ..., 0., 0., 0.],\n       [1., 0., 0., ..., 0., 0., 0.],\n       [1., 0., 0., ..., 0., 0., 0.]])\n\n\n\n# crear modelo usando el API funcional\ndef cnn_model(input_shape, num_classes):\n    # Definir la entrada\n    inputs = tf.keras.Input(shape=input_shape)\n\n    # Primera capa convolucional y de pooling\n    x = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\",\n                               strides=(1, 1),\n                               padding=\"valid\")(inputs)\n    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n\n    # Segunda capa convolucional y de pooling\n    x = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\",\n                               strides=(1, 1),\n                               padding=\"valid\")(x)\n    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n\n    # Aplanar y añadir Dropout\n    x = tf.keras.layers.Flatten()(x)\n    x = tf.keras.layers.Dropout(0.5)(x)\n\n    # Capa de salida\n    outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n\n    # Crear el modelo usando la API funcional\n    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='cnn_keras')\n\n    return model\n\n\n# Ahora se definen los input shape y el numero de clases\ninput_shape = (dim_imagen[0], dim_imagen[1], n_canales)\nnum_classes = 10\n\n# Crear el modelo cnn\ncnn_keras = cnn_model(input_shape, num_classes)\n\n# Visualizar el resumen del modelo\ncnn_keras.summary()\n\nModel: \"cnn_keras\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (InputLayer)        │ (None, 28, 28, 1)      │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d (Conv2D)                 │ (None, 26, 26, 32)     │           320 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d (MaxPooling2D)    │ (None, 13, 13, 32)     │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_1 (Conv2D)               │ (None, 11, 11, 64)     │        18,496 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_1 (MaxPooling2D)  │ (None, 5, 5, 64)       │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (Flatten)               │ (None, 1600)           │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (Dropout)               │ (None, 1600)           │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (Dense)                   │ (None, 10)             │        16,010 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 34,826 (136.04 KB)\n\n\n\n Trainable params: 34,826 (136.04 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# tambien es posible visualizar nuestra red en formato de grafo\ntf.keras.utils.plot_model(cnn_keras, rankdir='LR',show_dtype=True)\n\n\n\n\n\n\n\n\n\n# Compilar el modelo\ncnn_keras.compile(loss='categorical_crossentropy',\n            optimizer=SGD(),\n            metrics=['accuracy'])\n\n# Entrenar el modelo\nhistory = cnn_keras.fit(X_train, y_train_categorical,\n                    epochs=30,\n                    batch_size=128,\n                    validation_split=0.2,\n                    verbose=1)\n\n\nEpoch 1/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 32s 103ms/step - accuracy: 0.2604 - loss: 2.1329 - val_accuracy: 0.7763 - val_loss: 0.8651\n\nEpoch 2/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 40s 100ms/step - accuracy: 0.7170 - loss: 0.8741 - val_accuracy: 0.8841 - val_loss: 0.4328\n\nEpoch 3/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 41s 102ms/step - accuracy: 0.8282 - loss: 0.5398 - val_accuracy: 0.9079 - val_loss: 0.3207\n\nEpoch 4/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 30s 98ms/step - accuracy: 0.8755 - loss: 0.4041 - val_accuracy: 0.9270 - val_loss: 0.2540\n\nEpoch 5/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 42s 100ms/step - accuracy: 0.8992 - loss: 0.3252 - val_accuracy: 0.9372 - val_loss: 0.2167\n\nEpoch 6/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 41s 101ms/step - accuracy: 0.9138 - loss: 0.2829 - val_accuracy: 0.9444 - val_loss: 0.1931\n\nEpoch 7/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 41s 101ms/step - accuracy: 0.9231 - loss: 0.2526 - val_accuracy: 0.9512 - val_loss: 0.1733\n\nEpoch 8/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 41s 99ms/step - accuracy: 0.9292 - loss: 0.2346 - val_accuracy: 0.9535 - val_loss: 0.1600\n\nEpoch 9/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 30s 96ms/step - accuracy: 0.9350 - loss: 0.2114 - val_accuracy: 0.9570 - val_loss: 0.1478\n\nEpoch 10/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 31s 100ms/step - accuracy: 0.9376 - loss: 0.2025 - val_accuracy: 0.9600 - val_loss: 0.1391\n\nEpoch 11/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 42s 102ms/step - accuracy: 0.9452 - loss: 0.1841 - val_accuracy: 0.9620 - val_loss: 0.1314\n\nEpoch 12/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 40s 99ms/step - accuracy: 0.9466 - loss: 0.1766 - val_accuracy: 0.9627 - val_loss: 0.1269\n\nEpoch 13/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 30s 99ms/step - accuracy: 0.9497 - loss: 0.1683 - val_accuracy: 0.9649 - val_loss: 0.1202\n\nEpoch 14/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 43s 105ms/step - accuracy: 0.9497 - loss: 0.1671 - val_accuracy: 0.9664 - val_loss: 0.1161\n\nEpoch 15/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 30s 96ms/step - accuracy: 0.9530 - loss: 0.1590 - val_accuracy: 0.9683 - val_loss: 0.1109\n\nEpoch 16/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 42s 99ms/step - accuracy: 0.9527 - loss: 0.1602 - val_accuracy: 0.9686 - val_loss: 0.1073\n\nEpoch 17/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 41s 99ms/step - accuracy: 0.9549 - loss: 0.1468 - val_accuracy: 0.9685 - val_loss: 0.1040\n\nEpoch 18/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 42s 102ms/step - accuracy: 0.9561 - loss: 0.1420 - val_accuracy: 0.9698 - val_loss: 0.1018\n\nEpoch 19/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 39s 96ms/step - accuracy: 0.9587 - loss: 0.1382 - val_accuracy: 0.9705 - val_loss: 0.0985\n\nEpoch 20/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 32s 103ms/step - accuracy: 0.9579 - loss: 0.1386 - val_accuracy: 0.9718 - val_loss: 0.0948\n\nEpoch 21/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 40s 99ms/step - accuracy: 0.9611 - loss: 0.1306 - val_accuracy: 0.9710 - val_loss: 0.0932\n\nEpoch 22/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 42s 103ms/step - accuracy: 0.9593 - loss: 0.1307 - val_accuracy: 0.9719 - val_loss: 0.0905\n\nEpoch 23/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 40s 99ms/step - accuracy: 0.9597 - loss: 0.1280 - val_accuracy: 0.9717 - val_loss: 0.0887\n\nEpoch 24/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 30s 99ms/step - accuracy: 0.9629 - loss: 0.1202 - val_accuracy: 0.9738 - val_loss: 0.0870\n\nEpoch 25/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 31s 100ms/step - accuracy: 0.9644 - loss: 0.1226 - val_accuracy: 0.9735 - val_loss: 0.0851\n\nEpoch 26/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 42s 104ms/step - accuracy: 0.9641 - loss: 0.1221 - val_accuracy: 0.9745 - val_loss: 0.0839\n\nEpoch 27/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 39s 99ms/step - accuracy: 0.9659 - loss: 0.1155 - val_accuracy: 0.9742 - val_loss: 0.0822\n\nEpoch 28/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 41s 99ms/step - accuracy: 0.9650 - loss: 0.1146 - val_accuracy: 0.9745 - val_loss: 0.0809\n\nEpoch 29/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 41s 99ms/step - accuracy: 0.9648 - loss: 0.1158 - val_accuracy: 0.9746 - val_loss: 0.0815\n\nEpoch 30/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 31s 100ms/step - accuracy: 0.9655 - loss: 0.1112 - val_accuracy: 0.9764 - val_loss: 0.0780\n\n\n\n\n\nplot_loss_historia_keras(history)\n\n\n\n\n\n\n\n\n\nplot_acc_historia_keras(history)"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2a_Implementando_una_CNN_usando_Keras.html#evaluación-completa",
    "href": "semana_2/notebooks/Nb_2a_Implementando_una_CNN_usando_Keras.html#evaluación-completa",
    "title": "Implementado una Red Neuronal Convolucional",
    "section": "Evaluación completa",
    "text": "Evaluación completa\n\ny_test_categorical = to_categorical(y_test)\n\nscore = cnn_keras.evaluate(X_test, y_test_categorical, batch_size=128)\n\nscore\n\n\n165/165 ━━━━━━━━━━━━━━━━━━━━ 4s 24ms/step - accuracy: 0.9740 - loss: 0.0853\n\n\n\n\n[0.08383515477180481, 0.9749523997306824]\n\n\n\n# Realizar predicciones en el conjunto de prueba\ny_pred = cnn_keras.predict(X_test)\n\n# Convertir las predicciones en etiquetas (la clase con mayor probabilidad)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true = y_test.values.astype(int)  # Las etiquetas reales del conjunto de prueba\n\n\n657/657 ━━━━━━━━━━━━━━━━━━━━ 5s 8ms/step\n\n\n\n\n\n# Generar el reporte de clasificación\nprint(\"Reporte de Clasificación para la red CNN en MNIST:\\n\")\nprint(classification_report(y_true, y_pred_classes))\n\n# Crear la matriz de confusión\ncm = confusion_matrix(y_true, y_pred_classes)\n\n# Visualizar la matriz de confusión usando Seaborn\nplot_matriz_confusion(cm)\n\nReporte de Clasificación para la red CNN en MNIST:\n\n              precision    recall  f1-score   support\n\n           0       0.98      0.99      0.99      2058\n           1       0.98      0.99      0.98      2364\n           2       0.96      0.97      0.96      2133\n           3       0.98      0.96      0.97      2176\n           4       0.98      0.98      0.98      1936\n           5       0.98      0.98      0.98      1915\n           6       0.99      0.98      0.99      2088\n           7       0.97      0.97      0.97      2248\n           8       0.96      0.97      0.96      1992\n           9       0.97      0.96      0.97      2090\n\n    accuracy                           0.97     21000\n   macro avg       0.97      0.97      0.97     21000\nweighted avg       0.97      0.97      0.97     21000"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2a_Implementando_una_CNN_usando_Keras.html#visualización-de-filtros-convolucionales",
    "href": "semana_2/notebooks/Nb_2a_Implementando_una_CNN_usando_Keras.html#visualización-de-filtros-convolucionales",
    "title": "Implementado una Red Neuronal Convolucional",
    "section": "Visualización de filtros convolucionales",
    "text": "Visualización de filtros convolucionales\n\nvisualizacion_filtros_cnn_keras(cnn_keras)"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2a_Implementando_una_CNN_usando_Keras.html#visualización-de-los-features-maps",
    "href": "semana_2/notebooks/Nb_2a_Implementando_una_CNN_usando_Keras.html#visualización-de-los-features-maps",
    "title": "Implementado una Red Neuronal Convolucional",
    "section": "Visualización de los features maps",
    "text": "Visualización de los features maps\n\nvisualizacion_feature_maps(cnn_keras, image=X_test[randint(0, 100),:,:,:])\n\n\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html",
    "href": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html",
    "title": "Explicando la Mejora de los Transformers sobre las RNNs",
    "section": "",
    "text": "Última actualización 07/05/2025\n#@title Importar librerías\n#importar librerías necesarias\nimport os\nimport logging\nimport time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\n\nimport tensorflow_text\n\nimport warnings\nwarnings.filterwarnings('ignore')\n#@title Funciones complementarias\ndef plot_similaridad_positional_encodings(pos_encoding):\n  # normalización de los vectores a 1\n  pos_encoding/=tf.norm(pos_encoding, axis=1, keepdims=True)\n  # seleccionamos el vector de la posición 1000\n  p = pos_encoding[1000]\n  # cálculo de la similitud del producto punto\n  dots = tf.einsum('pd,d -&gt; p', pos_encoding, p)\n\n  # visualización de la relación de los vectores con sus palabras\n  # vecinas, por definición tendran mucha similaridad.\n  plt.subplot(2,1,1)\n  plt.plot(dots)\n  plt.ylim([0,1])\n  plt.plot([950, 950, float('nan'), 1050, 1050],\n          [0,1,float('nan'),0,1], color='k', label='Zoom')\n  plt.legend()\n  plt.subplot(2,1,2)\n  plt.plot(dots)\n  plt.xlim([950, 1050])\n  plt.ylim([0,1])\n\ndef plot_distribucion_longitudes_tokens(all_lengths):\n  plt.hist(all_lengths, np.linspace(0, 500, 101))\n  plt.ylim(plt.ylim())\n  max_length = max(all_lengths)\n  plt.plot([max_length, max_length], plt.ylim())\n  plt.title(f'Número máximo de tokens por muestra: {max_length}');\n\ndef plot_positional_encodings(pos_encoding):\n  # Gráficar las dimensiones\n  plt.pcolormesh(pos_encoding.numpy().T, cmap='RdBu')\n  plt.ylabel('Profundidad')\n  plt.xlabel('Posición')\n  plt.colorbar()\n  plt.show()\n\ndef plot_lr_planificador(learning_rate):\n  plt.plot(learning_rate(tf.range(40000, dtype=tf.float32)))\n  plt.ylabel('Learning Rate');\n  plt.xlabel('Paso de entrenamiento');\n\ndef plot_attention_head(in_tokens, translated_tokens, attention):\n  # Saltar el token start.\n  translated_tokens = translated_tokens[1:]\n\n  ax = plt.gca()\n  ax.matshow(attention)\n  ax.set_xticks(range(len(in_tokens)))\n  ax.set_yticks(range(len(translated_tokens)))\n\n  labels = [label.decode('utf-8') for label in in_tokens.numpy()]\n  ax.set_xticklabels(\n      labels, rotation=90)\n\n  labels = [label.decode('utf-8') for label in translated_tokens.numpy()]\n  ax.set_yticklabels(labels)\n\ndef plot_attention_weights(sentence, translated_tokens, attention_heads):\n  in_tokens = tf.convert_to_tensor([sentence])\n  in_tokens = tokenizers.pt.tokenize(in_tokens).to_tensor()\n  in_tokens = tokenizers.pt.lookup(in_tokens)[0]\n\n  fig = plt.figure(figsize=(12, 15))\n\n  for h, head in enumerate(attention_heads):\n    ax = fig.add_subplot(2, 4, h+1)\n\n    plot_attention_head(in_tokens, translated_tokens, head)\n\n    ax.set_xlabel(f'Head {h+1}')\n\n  plt.tight_layout()\n  plt.show()\n\ndef plot_traduccion(sentence, tokens, ground_truth):\n  print(f'{\"Input:\":15s}: {sentence}')\n  print(f'{\"Predicción\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n  print(f'{\"Ground truth\":15s}: {ground_truth}')\nLos Transformers: Una Nueva Era en el Procesamiento de Secuencias\nLas Redes Neuronales Recurrentes (RNNs) han sido una herramienta fundamental para el procesamiento de datos secuenciales, como el lenguaje natural. Sin embargo, los Transformers, propuestos en el artículo “Attention is all you need”, han revolucionado este campo, ofreciendo ventajas significativas sobre las RNNs. Los Transformers son redes neuronales profundas que reemplazan las CNNs y las RNNs. Estos introducen la auto-atención (self-attention) permite a los Transformers transmitir información fácilmente a través de las secuencias de entrada.\n¿Por qué los Transformers son importantes?\nEn este notebook, exploraremos en profundidad los componentes clave de los Transformers y compararemos su funcionamiento con el de las RNNs, destacando las razones de su superioridad."
  },
  {
    "objectID": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#esquema-de-trabajo",
    "href": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#esquema-de-trabajo",
    "title": "Explicando la Mejora de los Transformers sobre las RNNs",
    "section": "Esquema de trabajo",
    "text": "Esquema de trabajo\nCon lo anterior en mente, y una vez visto un poco las diferencias entre RNNs y Transformers, vamos a abordar los siguientes contenidos de manera detallada:\n\nPrepararás los datos.\nImplementarás los componentes necesarios:\n\nEmbeddings posicionales.\nCapas de atención.\nEl codificador y el decodificador.\n\nConstruirás y entrenarás el Transformer.\nGenerarás traducciones.\nExportarás el modelo.\n\n\nCarga y Preprocesamiento de Datos\nCargaremos un conjunto de datos de traducción Portugués-Inglés y utilizaremos un tokenizador para preparar el texto para el modelo. Este conjunto de datos contiene aproximadamente 52.000 ejemplos de entrenamiento, 1.200 de validación y 1.800 de prueba.\n\nexamples, metadata = tfds.load('ted_hrlr_translate/pt_to_en',\n                               with_info=True,\n                               as_supervised=True)\n\ntrain_examples, val_examples = examples['train'], examples['validation']\n\n\n# Mostrar algunos ejemplos de parejas de oraciones\nfor pt_examples, en_examples in train_examples.batch(3).take(1):\n  print('-&gt; Ejemplos en portugués:')\n  for pt in pt_examples.numpy():\n    print(pt.decode('utf-8'))\n  print()\n\n  print('-&gt; Traducción al inglés:')\n  for en in en_examples.numpy():\n    print(en.decode('utf-8'))\n\n-&gt; Ejemplos en portugués:\ne quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\nmas e se estes fatores fossem ativos ?\nmas eles não tinham a curiosidade de me testar .\n\n-&gt; Traducción al inglés:\nand when you improve searchability , you actually take away the one advantage of print , which is serendipity .\nbut what if it were active ?\nbut they did n't test for curiosity .\n\n\n\nTokenizadores\nEn nuestro ejemplo vamos a usar los tokenizadores construidos en el tutorial subword tokenizer. Ese tutorial optimiza dos objetos text.BertTokenizer (uno para inglés, otro para portugués) para este conjunto de datos y los exporta en formato saved_model de TensorFlow.\n\nmodel_name = 'ted_hrlr_translate_pt_en_converter'\ntf.keras.utils.get_file(\n    f'{model_name}.zip',\n    f'https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip',\n    cache_dir='.', cache_subdir='', extract=True\n)\n\n'./ted_hrlr_translate_pt_en_converter_extracted'\n\n\n\n# cargar los tokenizadores\ntokenizers = tf.saved_model.load(f'{model_name}_extracted/{model_name}')\n\n\n# ambos tokenizadores tienen los mismo métodos\n[item for item in dir(tokenizers.en) if not item.startswith('_')]\n\n['detokenize',\n 'get_reserved_tokens',\n 'get_vocab_path',\n 'get_vocab_size',\n 'lookup',\n 'tokenize',\n 'tokenizer',\n 'vocab']\n\n\n\n# por ejemplo revisemos los vocab\ntokenizers.en.vocab.shape, tokenizers.pt.vocab.shape\n\n(TensorShape([7010]), TensorShape([7765]))\n\n\nEl método tokenize convierte un grupo de oraciones en un grupo de identificadores de tokens de una misma longitud (padding). Este método separa los signos de puntuación, las minúsculas y normaliza el texto de entrada antes de la tokenización. Esta normalización no es visible aquí porque los datos de entrada ya están normalizados.\n\nprint('-&gt; Esto es un grupo de cadenas:')\nfor en in en_examples.numpy():\n  print(en.decode('utf-8'))\n\n-&gt; Esto es un grupo de cadenas:\nand when you improve searchability , you actually take away the one advantage of print , which is serendipity .\nbut what if it were active ?\nbut they did n't test for curiosity .\n\n\n\nencoded = tokenizers.en.tokenize(en_examples)\n\nprint('-&gt; Este es el grupo de cedena de ID de tokens (con padding)')\nfor row in encoded.to_list():\n  print(row)\n\n-&gt; Este es el grupo de cedena de ID de tokens (con padding)\n[2, 72, 117, 79, 1259, 1491, 2362, 13, 79, 150, 184, 311, 71, 103, 2308, 74, 2679, 13, 148, 80, 55, 4840, 1434, 2423, 540, 15, 3]\n[2, 87, 90, 107, 76, 129, 1852, 30, 3]\n[2, 87, 83, 149, 50, 9, 56, 664, 85, 2512, 15, 3]\n\n\nEl método detokenize convierte los ID de token de nuevo en texto legible normal:\n\nround_trip = tokenizers.en.detokenize(encoded)\n\nprint('-&gt; Correspondiente texto:')\nfor line in round_trip.numpy():\n  print(line.decode('utf-8'))\n\n-&gt; Correspondiente texto:\nand when you improve searchability , you actually take away the one advantage of print , which is serendipity .\nbut what if it were active ?\nbut they did n ' t test for curiosity .\n\n\nEl método lookup convierte de token-IDs a token-texto:\n\nprint('-&gt; Este es el texto dividido en tokens:')\ntokens = tokenizers.en.lookup(encoded)\ntokens\n\n-&gt; Este es el texto dividido en tokens:\n\n\n&lt;tf.RaggedTensor [[b'[START]', b'and', b'when', b'you', b'improve', b'search', b'##ability',\n  b',', b'you', b'actually', b'take', b'away', b'the', b'one', b'advantage',\n  b'of', b'print', b',', b'which', b'is', b's', b'##ere', b'##nd', b'##ip',\n  b'##ity', b'.', b'[END]']                                                 ,\n [b'[START]', b'but', b'what', b'if', b'it', b'were', b'active', b'?',\n  b'[END]']                                                           ,\n [b'[START]', b'but', b'they', b'did', b'n', b\"'\", b't', b'test', b'for',\n  b'curiosity', b'.', b'[END]']                                          ]&gt;\n\n\nLa salida demuestra el aspecto de \"subword\" de la tokenización de subpalabras.\nPor ejemplo, la palabra 'searchability' se descompone en 'search' y '##ability', y la palabra 'serendipity' en 's', '##ere', '##nd', '##ip' e '##ity'.\nTen en cuenta que el texto tokenizado incluye los tokens '[START]' y '[END]'.\nLa distribución de tokens por ejemplo en el conjunto de datos es la siguiente:\n\nlengths = []\n\nfor pt_examples, en_examples in train_examples.batch(1024):\n  pt_tokens = tokenizers.pt.tokenize(pt_examples)\n  lengths.append(pt_tokens.row_lengths())\n\n  en_tokens = tokenizers.en.tokenize(en_examples)\n  lengths.append(en_tokens.row_lengths())\n  print('.', end='', flush=True)\n\n...................................................\n\n\n\nall_lengths = np.concatenate(lengths)\n\nplot_distribucion_longitudes_tokens(all_lengths)\n\n\n\n\n\n\n\n\n\n\nConfiguración del pipeline de datos usando tf.data\nLa siguiente función toma batches de texto como entrada y los convierte a un formato adecuado para el entrenamiento.\n\nLos tokeniza en lotes de diferentes dimensiones (ragged).\nRecorta cada uno para que no tenga más de MAX_TOKENS.\nDivide los tokens objetivo (inglés) en entradas y etiquetas. Estos se desplazan un paso de modo que en cada ubicación de entrada, la etiqueta es el ID del siguiente token.\nConvierte los RaggedTensor en Tensor densos con padding.\nDevuelve un par (entradas, etiquetas).\n\n\nMAX_TOKENS=64\ndef prepare_batch(pt, en):\n    pt = tokenizers.pt.tokenize(pt) # la sálida tiene diferentes longitudes\n    pt = pt[:, :MAX_TOKENS]    # Truncar al max # de tokens\n    pt = pt.to_tensor()  # Convertir a un tensor denso sin padding\n\n    en = tokenizers.en.tokenize(en)\n    en = en[:, :(MAX_TOKENS+1)] # para poder hacer el shift\n    en_inputs = en[:, :-1].to_tensor()  # Elimina los tokens [END]\n    en_labels = en[:, 1:].to_tensor()   # Eliminar los tokens [START]\n\n    return (pt, en_inputs), en_labels\n\nLa siguiente función convierte un conjunto de datos de ejemplos de texto en datos de lotes para el entrenamiento.\n\nTokeniza el texto y filtra las secuencias que son demasiado largas.\nEl método cache asegura que ese trabajo solo se ejecute una vez.\nLuego, shuffle y preparar el batch.\nFinalmente, prefetch ejecuta el conjunto de datos en paralelo con el modelo para asegurar que los datos estén disponibles cuando se necesiten. Consulta Mejorar rendimiento con tf.data para más detalles.\n\n\nBUFFER_SIZE = 10000\nBATCH_SIZE = 64\n\n\ndef make_batches(ds):\n  return (\n      ds\n      .shuffle(BUFFER_SIZE)\n      .batch(BATCH_SIZE)\n      .map(prepare_batch, tf.data.AUTOTUNE)\n      .prefetch(buffer_size=tf.data.AUTOTUNE))\n\n\n\nProbar nuestro pipeline de datos\n\n# Crear los conjuntos de entrenamiento y validación\ntrain_batches = make_batches(train_examples)\nval_batches = make_batches(val_examples)\n\nComo se verían las entradas y sálidas en nuestro pipeline de datos?\n\n\n\nInputs en la parte inferior, labels en la parte superior.\n\n\n\n\n\n\n\n\nEsta configuración se llama “teacher forcing” porque, independientemente de la salida del modelo en cada paso de tiempo, recibe el valor verdadero como entrada para el siguiente paso de tiempo. Esta es una forma simple y eficiente de entrenar un modelo de generación de texto. Es eficiente porque no necesitas ejecutar el modelo secuencialmente; las salidas en las diferentes ubicaciones de la secuencia se pueden calcular en paralelo.\n\n# visualizar un ejemplo de nuestros datos para entrenar\nfor (pt, en), en_labels in train_batches.take(1):\n  break\n\nprint(pt.shape)\nprint(en.shape)\nprint(en_labels.shape)\n\n(64, 64)\n(64, 64)\n(64, 64)\n\n\nLas etiquetas en y en_labels son las mismas, sólo que desplazadas en 1:\n\nprint(en[0][:10])\nprint(en_labels[0][:10])\n\ntf.Tensor([ 2 90 80 81 85 30  0  0  0  0], shape=(10,), dtype=int64)\ntf.Tensor([90 80 81 85 30  3  0  0  0  0], shape=(10,), dtype=int64)\n\n\n\n\n\nDefinir los componentes del Transformer\nDentro de un Transformer pasan muchas cosas. Las cosas importantes que hay que recordar son:\n\nSigue el mismo patrón general que un modelo estándar secuencia-a-secuencia con un codificador y un decodificador.\nSi trabajas paso a paso, todo tendrá sentido.\n\n\n\n\nDiagrama original del Transformer\n\n\nRepresentación de un Transformer de 4 capas\n\n\n\n\n\n\n\n\n\n\n\n\nLa capa para la codificación de la posición\nLas entradas tanto del codificador como del descodificador utilizan la misma lógica de incrustación y codificación posicional\n\n\n\nThe embedding and positional encoding layer\n\n\n\n\n\n\n\n\nDada una secuencia de tokens, tanto los tokens de entrada (portugués) como los tokens objetivo (inglés) deben convertirse en vectores utilizando una capa tf.keras.layers.Embedding.\nLas capas de atención utilizadas en todo el modelo ven su entrada como un conjunto de vectores, sin ningún orden. Dado que el modelo no contiene ninguna capa recurrente o convolucional, necesita alguna forma de identificar el orden de las palabras; de lo contrario, vería la secuencia de entrada como una instancia de bolsa de palabras, cómo estás, cómo tú estás, tú cómo estás, y así sucesivamente, son indistinguibles.\nPor lo tanto, el Transformer agrega una “Codificación Posicional” a los vectores de embedding. Utiliza un conjunto de senos y cosenos en diferentes frecuencias (a lo largo de la secuencia). Por definición, los elementos cercanos tendrán codificaciones posicionales similares.\nEl paper original usa la siguiente formúla para la codificación posicional:\n\\[\\Large{PE_{(pos, 2i)} = \\sin(pos / 10000^{2i / d_{model}})} \\] \\[\\Large{PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i / d_{model}})} \\]\nNota: El código a continuación lo implementa, pero en lugar de intercalar los senos y cosenos, los vectores de senos y cosenos simplemente se concatenan. Permutar los canales de esta manera es funcionalmente equivalente, y un poco más fácil de implementar y mostrar en las gráficas siguientes.\nExplicación Paso a Paso: Imaginemos que tenemos una frase: “El gato está en la alfombra”.\nY supongamos que d_model = 4. Esto significa que cada posición (cada palabra) se representará con un vector de 4 números.\n\nPosiciones:\n\n\n“El” está en la posición 0.\n“gato” está en la posición 1.\n“está” está en la posición 2.\n“en” está en la posición 3.\n“la” está en la posición 4.\n“alfombra” está en la posición 5.\n\n\nDimensiones:\n\nComo d_model = 4, tendremos dimensiones 0, 1, 2, y 3 en nuestro vector de codificación posicional.\n\nAplicando la Fórmula (Ejemplo: la palabra “gato” en la posición 1):\n\n\nPara la dimensión 0 (2i = 0, entonces i = 0):\n\nPE(1, 0) = sin(1 / 10000^(2*0 / 4)) = sin(1 / 10000^0) = sin(1 / 1) = sin(1) ≈ 0.841\n\nPara la dimensión 1 (2i + 1 = 1, entonces i = 0):\n\nPE(1, 1) = cos(1 / 10000^(2*0 / 4)) = cos(1 / 10000^0) = cos(1 / 1) = cos(1) ≈ 0.540\n\nPara la dimensión 2 (2i = 2, entonces i = 1):\n\nPE(1, 2) = sin(1 / 10000^(2*1 / 4)) = sin(1 / 10000^(1/2)) = sin(1 / 100) = sin(0.01) ≈ 0.01\n\nPara la dimensión 3 (2i + 1 = 3, entonces i = 1):\n\nPE(1, 3) = cos(1 / 10000^(2*1 / 4)) = cos(1 / 10000^(1/2)) = cos(1 / 100) = cos(0.01) ≈ 0.99995\nEntonces, la codificación posicional para la palabra “gato” (posición 1) sería aproximadamente el vector: [0.841, 0.540, 0.01, 0.99995].\n\ndef positional_encoding(length, depth):\n  depth = depth/2 # mitad de las dimensiones para cada función\n\n  positions = np.arange(length)[:, np.newaxis]     # shape = (seq, 1)\n  depths = np.arange(depth)[np.newaxis, :]/depth   # shape = (1, depth)\n\n  angle_rates = 1 / (10000**depths)         # (1, depth)\n  angle_rads = positions * angle_rates      # (pos, depth)\n\n  pos_encoding = np.concatenate(\n      [np.sin(angle_rads), np.cos(angle_rads)],\n      axis=-1)\n\n  return tf.cast(pos_encoding, dtype=tf.float32)\n\nLa función de codificación de posición es una pila de senos y cosenos que vibran a distintas frecuencias según su ubicación a lo largo de la profundidad del vector de incrustación. Vibran a través del eje de posición.\n\n# para el ejemplo de nuestra frase: El gato esta en la alfombra\n# en este caso se concatenaron las funciones\npositional_encoding(length=6, depth=4)\n\n&lt;tf.Tensor: shape=(6, 4), dtype=float32, numpy=\narray([[ 0.        ,  0.        ,  1.        ,  1.        ],\n       [ 0.84147096,  0.00999983,  0.5403023 ,  0.99995   ],\n       [ 0.9092974 ,  0.01999867, -0.41614684,  0.9998    ],\n       [ 0.14112   ,  0.0299955 , -0.9899925 ,  0.99955004],\n       [-0.7568025 ,  0.03998933, -0.6536436 ,  0.9992001 ],\n       [-0.9589243 ,  0.04997917,  0.2836622 ,  0.99875027]],\n      dtype=float32)&gt;\n\n\n\npos_encoding = positional_encoding(length=2048, depth=512)\n\n# Revisar la dimensión\nprint(pos_encoding.shape)\n\nplot_positional_encodings(pos_encoding)\n\n(2048, 512)\n\n\n\n\n\n\n\n\n\nPor definición, estos vectores se alinean bien con los vectores cercanos a lo largo del eje de posición. A continuación, los vectores de codificación de posición se normalizan y el vector de la posición 1000 se compara, por producto punto, con todos los demás:\n\nplot_similaridad_positional_encodings(pos_encoding)\n\n\n\n\n\n\n\n\nAhora creemos la capa: PositionEmbedding\n\nclass PositionalEmbedding(tf.keras.layers.Layer):\n  def __init__(self, vocab_size, d_model):\n    super().__init__()\n    self.d_model = d_model\n    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n\n  def compute_mask(self, *args, **kwargs):\n    return self.embedding.compute_mask(*args, **kwargs)\n\n  def call(self, x):\n    length = tf.shape(x)[1]\n    x = self.embedding(x)\n    # Este factor establece la escala relativa de la incrustación y la codificación_positonal.\n    # Es decir asegurar que tengan escalas similares\n    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n    # Se suma las posiciones a los embeddings de los tokens\n    x = x + self.pos_encoding[tf.newaxis, :length, :]\n    return x\n\n\nembed_pt = PositionalEmbedding(vocab_size=tokenizers.pt.get_vocab_size().numpy(), d_model=512)\nembed_en = PositionalEmbedding(vocab_size=tokenizers.en.get_vocab_size().numpy(), d_model=512)\n\npt_emb = embed_pt(pt)\nen_emb = embed_en(en)\n\n\n# la máscara de cada oración, recordar que las oraciones no tiene la misma\n# longitud, asi que se aplica la máscara\nen_emb._keras_mask\n\n&lt;tf.Tensor: shape=(64, 64), dtype=bool, numpy=\narray([[ True,  True,  True, ..., False, False, False],\n       [ True,  True,  True, ..., False, False, False],\n       [ True,  True,  True, ..., False, False, False],\n       ...,\n       [ True,  True,  True, ..., False, False, False],\n       [ True,  True,  True, ..., False, False, False],\n       [ True,  True,  True, ..., False, False, False]])&gt;\n\n\n\npt_emb\n\n&lt;tf.Tensor: shape=(64, 64, 512), dtype=float32, numpy=\narray([[[ 0.81105447, -0.7717942 ,  0.5229695 , ...,  0.02146667,\n          0.24027777,  0.26449662],\n        [ 1.8670975 ,  1.6279981 ,  0.97901094, ...,  1.3392618 ,\n          0.8188071 ,  0.21744752],\n        [-0.11729413,  0.5008898 ,  0.32620972, ...,  0.27159345,\n          1.1697826 ,  1.4658518 ],\n        ...,\n        [-1.3773707 ,  0.633837  ,  1.1719698 , ...,  1.6278234 ,\n          0.05931401,  0.90110606],\n        [-1.1504335 , -0.23321328,  1.8688756 , ...,  1.6278226 ,\n          0.0593133 ,  0.90110534],\n        [-0.24389721, -0.9982976 ,  1.8318357 , ...,  1.6278219 ,\n          0.05931258,  0.9011047 ]],\n\n       [[ 0.81105447, -0.7717942 ,  0.5229695 , ...,  0.02146667,\n          0.24027777,  0.26449662],\n        [-0.03075731,  0.79514253,  1.7891788 , ...,  0.0534128 ,\n          0.01318026,  0.86922586],\n        [ 1.0960544 ,  0.75714344,  0.7072108 , ...,  0.3786983 ,\n          1.0271425 ,  1.3102653 ],\n        ...,\n        [-1.3773707 ,  0.633837  ,  1.1719698 , ...,  1.6278234 ,\n          0.05931401,  0.90110606],\n        [-1.1504335 , -0.23321328,  1.8688756 , ...,  1.6278226 ,\n          0.0593133 ,  0.90110534],\n        [-0.24389721, -0.9982976 ,  1.8318357 , ...,  1.6278219 ,\n          0.05931258,  0.9011047 ]],\n\n       [[ 0.81105447, -0.7717942 ,  0.5229695 , ...,  0.02146667,\n          0.24027777,  0.26449662],\n        [ 1.7584805 ,  1.4909694 , -0.1760881 , ...,  2.1180818 ,\n          0.4953122 ,  0.23008263],\n        [ 0.55767405,  1.4934946 ,  0.02770001, ...,  0.84077555,\n          0.9217277 ,  1.9566159 ],\n        ...,\n        [-1.3773707 ,  0.633837  ,  1.1719698 , ...,  1.6278234 ,\n          0.05931401,  0.90110606],\n        [-1.1504335 , -0.23321328,  1.8688756 , ...,  1.6278226 ,\n          0.0593133 ,  0.90110534],\n        [-0.24389721, -0.9982976 ,  1.8318357 , ...,  1.6278219 ,\n          0.05931258,  0.9011047 ]],\n\n       ...,\n\n       [[ 0.81105447, -0.7717942 ,  0.5229695 , ...,  0.02146667,\n          0.24027777,  0.26449662],\n        [ 0.7150625 ,  1.6491615 ,  1.213108  , ...,  1.0735046 ,\n          0.3283956 ,  0.01745564],\n        [ 1.1696244 ,  1.2925339 ,  1.6480486 , ...,  0.24500364,\n          1.4749924 ,  1.9941585 ],\n        ...,\n        [-1.3773707 ,  0.633837  ,  1.1719698 , ...,  1.6278234 ,\n          0.05931401,  0.90110606],\n        [-1.1504335 , -0.23321328,  1.8688756 , ...,  1.6278226 ,\n          0.0593133 ,  0.90110534],\n        [-0.24389721, -0.9982976 ,  1.8318357 , ...,  1.6278219 ,\n          0.05931258,  0.9011047 ]],\n\n       [[ 0.81105447, -0.7717942 ,  0.5229695 , ...,  0.02146667,\n          0.24027777,  0.26449662],\n        [-0.23110986,  0.5054298 , -0.25015187, ...,  1.1486163 ,\n          1.1958522 ,  1.1460018 ],\n        [ 1.4417007 , -0.06407106,  0.7872464 , ...,  0.72261333,\n          0.1937148 ,  2.0049632 ],\n        ...,\n        [-1.3773707 ,  0.633837  ,  1.1719698 , ...,  1.6278234 ,\n          0.05931401,  0.90110606],\n        [-1.1504335 , -0.23321328,  1.8688756 , ...,  1.6278226 ,\n          0.0593133 ,  0.90110534],\n        [-0.24389721, -0.9982976 ,  1.8318357 , ...,  1.6278219 ,\n          0.05931258,  0.9011047 ]],\n\n       [[ 0.81105447, -0.7717942 ,  0.5229695 , ...,  0.02146667,\n          0.24027777,  0.26449662],\n        [ 1.7306108 ,  1.4167533 ,  1.2637417 , ..., -0.04352736,\n          0.04924804,  0.62587   ],\n        [ 0.03706914,  0.90970105,  1.9453614 , ...,  0.0534128 ,\n          0.01318026,  0.86922586],\n        ...,\n        [-1.3773707 ,  0.633837  ,  1.1719698 , ...,  1.6278234 ,\n          0.05931401,  0.90110606],\n        [-1.1504335 , -0.23321328,  1.8688756 , ...,  1.6278226 ,\n          0.0593133 ,  0.90110534],\n        [-0.24389721, -0.9982976 ,  1.8318357 , ...,  1.6278219 ,\n          0.05931258,  0.9011047 ]]], dtype=float32)&gt;\n\n\n\n\nCapas de Adición y normalización\n\n\n\nAdd y normalize\n\n\n\n\n\n\n\n\nEstos bloques de “Add & Norm” se encuentran distribuidos a lo largo de todo el modelo Transformer. Cada uno combina una conexión residual y pasa el resultado a través de una capa de LayerNormalization.\nLa manera más sencilla de organizar el código es estructurándolo alrededor de estos bloques residuales. En las siguientes secciones, definiremos clases de capas personalizadas para cada uno de ellos.\nLos bloques residuales “Add & Norm” se incluyen para que el entrenamiento sea eficiente. La conexión residual proporciona una ruta directa para el gradiente (y asegura que los vectores sean actualizados por las capas de atención en lugar de ser reemplazados), mientras que la normalización mantiene una escala razonable para las salidas.\nNota: Las implementaciones que se muestran a continuación utilizan la capa Add para asegurar que las máscaras de Keras se propaguen correctamente (el operador + no lo hace).\n\n\nBases de la capa de atención\nLas capas de atención se utilizan a lo largo de todo el modelo Transformer. Todas ellas son idénticas, excepto por cómo se configura la atención. Cada una contiene una capa layers.MultiHeadAttention, una capa layers.LayerNormalization y una capa layers.Add.\n\n\n\nCapa de atención básica\n\n\n\n\n\n\n\n\nPara implementar estas capas de atención, comenzaremos con una clase base simple que sólo contenga definidos los componentes. Cada caso de uso se implementará como una subclase (framework).\n\nclass BaseAttention(tf.keras.layers.Layer):\n  def __init__(self, **kwargs):\n    super().__init__()\n    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n    self.layernorm = tf.keras.layers.LayerNormalization()\n    self.add = tf.keras.layers.Add()\n\n\nAtención, cómo funciona?\nAntes de entrar en los detalles de cada uso, aquí tienes un breve repaso de cómo funciona la atención:\n\n\n\nCapa de atención básica\n\n\n\n\n\n\n\n\nHay dos entradas: 1. La secuencia de consulta (query sequence): la secuencia que se está procesando; la secuencia que “atiende” (abajo). 2. La secuencia de contexto (context sequence): la secuencia a la que se está “atendiendo” (izquierda).\nLa salida tiene la misma forma que la secuencia de consulta.\nUna analogía común es que esta operación se asemeja a una búsqueda en un diccionario. Una búsqueda en un diccionario difuso, diferenciable y vectorizado.\nAquí tienes un diccionario regular de Python, con 3 claves y 3 valores al que se le pasa una única consulta.\nd = {'color': 'azul', 'edad': 22, 'tipo': 'pickup'}\nresult = d['color']\n\nEl query es lo que se esta tratando de encontrar.\nThe key es el tipo de información que tiene el diccionario\nThe value es la información\n\nCuando buscas una query (consulta) en un diccionario normal, el diccionario encuentra la key (clave) coincidente y devuelve su value (valor) asociado. La query tiene una key coincidente o no la tiene.\nPuedes imaginar un diccionario difuso donde las claves no tienen que coincidir perfectamente. Si buscaras d[\"especie\"] en el diccionario de arriba, quizás querrías que devolviera \"pickup\" ya que es la mejor coincidencia para la consulta.\nUna capa de atención realiza una búsqueda difusa como esta, pero no solo busca la mejor clave. Combina los values basándose en qué tan bien la query coincide con cada key.\n¿Cómo funciona esto? En una capa de atención, la query, la key y el value son cada uno vectores. En lugar de realizar una búsqueda de asignación, la capa de atención combina los vectores de la query y la key para determinar qué tan bien coinciden, obteniendo una “puntuación de atención” (attention score). La capa devuelve el promedio de todos los values, ponderado por las “puntuaciones de atención”.\nCada posición en la secuencia de consulta (query sequence) proporciona un vector de query. La secuencia de contexto (context sequence) actúa como el diccionario. Cada posición en la secuencia de contexto proporciona un vector de key y un vector de value.\nLos vectores de entrada no se utilizan directamente; la capa layers.MultiHeadAttention incluye capas layers.Dense para proyectar los vectores de entrada antes de utilizarlos.\n\n\nCapa de atención cruzada\nEn el centro literal del Transformer está la capa de atención cruzada. Esta capa conecta el codificador y el decodificador. Esta capa es el uso más directo de la atención en el modelo, realiza la misma tarea que el bloque de atención en el Tutorial NMT con atención.\n\n\n\nAtención cruzada\n\n\n\n\n\n\n\n\nPara implementar esto, pasas la secuencia objetivo x como la query (consulta) y la secuencia de context (contexto) como la key/value (clave/valor) al llamar a la capa mha:\n\nclass CrossAttention(BaseAttention):\n  def call(self, x, context):\n    attn_output, attn_scores = self.mha(\n        query=x,\n        key=context,\n        value=context,\n        return_attention_scores=True)\n\n    # Almacenar los scores de atención para\n    # visualizarlos más adelante\n    self.last_attn_scores = attn_scores\n\n    x = self.add([x, attn_output])\n    x = self.layernorm(x)\n\n    return x\n\nLa siguiente caricatura muestra cómo fluye la información a través de esta capa. Las columnas representan la suma ponderada de la secuencia contextual.\nPara simplificar, no se muestran las conexiones residuales.\n\n\n\nAtención cruzada\n\n\n\n\n\n\n\n\nLa longitud de la salida es la longitud de la secuencia de query (consulta), y no la longitud de la secuencia de key/value (clave/valor) del contexto.\nEl diagrama se simplifica aún más a continuación. No es necesario dibujar la matriz completa de “pesos de atención”.\nEl punto clave es que cada ubicación en la query puede “ver” todos los pares de key/value en el contexto, pero no se intercambia información entre las diferentes consultas.\n\n\n\nCada elemento del query puede ver todo el contexto.\n\n\n\n\n\n\n\n\nEjemplo:\n\nsample_ca = CrossAttention(num_heads=2, key_dim=512)\n\nprint(pt_emb.shape)\nprint(en_emb.shape)\n# cada token de la frase en inglés\n# será operados con todos los tokens de\n# la oración en portugués, esto pasa así\n# solo en entrenamiento\nprint(sample_ca(en_emb, pt_emb).shape)\n\n(64, 64, 512)\n(64, 64, 512)\n(64, 64, 512)\n\n\n\n\nCapa global de auto-atención\nEsta capa se encarga de procesar la secuencia contextual y de propagar la información a lo largo de la misma:\n\n\n\nLa capa global de auto-atención\n\n\n\n\n\n\n\n\nDado que la secuencia contextual es fija mientras se genera la traducción (entrenamiento), se permite que la información fluya en ambas direcciones.\nAntes de los Transformers y la autoatención, los modelos solían utilizar RNNs o CNNs para realizar esta tarea (bidireccionalidad):\n\n\n\nRNNs y CNNs bidireccionales\n\n\n\n\n\n\n\n\n\n\n\n\n\nRNNs and CNNs have their limitations.\nLas RNNs y las CNNs tienen sus limitaciones:\n\nLa RNN permite que la información fluya a lo largo de toda la secuencia, pero atraviesa muchos pasos de procesamiento para llegar allí (limitando el flujo del gradiente). Estos pasos de la RNN deben ejecutarse secuencialmente, por lo que la RNN es menos capaz de aprovechar los dispositivos paralelos modernos.\nEn la CNN, cada ubicación puede procesarse en paralelo, pero solo proporciona un campo receptivo limitado. El campo receptivo solo crece linealmente con el número de capas CNN. Se necesita apilar varias capas convolucionales para transmitir información a través de la secuencia.\n\nPor otro lado, la capa de auto-atención global permite que cada elemento de la secuencia acceda directamente a todos los demás elementos de la secuencia, con solo unas pocas operaciones, y todas las salidas se pueden calcular en paralelo.\nPara implementar esta capa, solo necesitas pasar la secuencia objetivo, x, como los argumentos query (consulta) y value (valor) a la capa mha:\n\nclass GlobalSelfAttention(BaseAttention):\n  def call(self, x):\n    attn_output = self.mha(\n        query=x,\n        value=x,\n        key=x)\n    x = self.add([x, attn_output])\n    x = self.layernorm(x)\n    return x\n\n\nsample_gsa = GlobalSelfAttention(num_heads=2, key_dim=512)\n\nprint(pt_emb.shape)\nprint(sample_gsa(pt_emb).shape)\n\n(64, 64, 512)\n(64, 64, 512)\n\n\nSiguiendo con el mismo estilo de antes, podriamos dibujarlo así:\n\n\n\nLa capa global de auto-atención\n\n\n\n\n\n\n\n\nDe nuevo, las conexiones residuales se omiten para mayor claridad.\nEs más compacto e igual de preciso dibujarlo así:\n\n\n\nLa capa de auto-atención global\n\n\n\n\n\n\n\n\n\n\nCapa de auto-atención causal\nEsta capa realiza un trabajo similar al de la capa de autoatención global, para la secuencia de salida:\n\n\n\nCapa causal de auto-atención\n\n\n\n\n\n\n\n\nThis needs to be handled differently from the encoder’s global self-attention layer.\nLike the text generation tutorial, and the NMT with attention tutorial, Transformers are an “autoregressive” model: They generate the text one token at a time and feed that output back to the input. To make this efficient, these models ensure that the output for each sequence element only depends on the previous sequence elements; the models are “causal”.\nUna RNN unidireccional es causal por definición. Para hacer una convolución causal, solo necesitas rellenar (pad) la entrada y desplazar la salida para que se alinee correctamente (puedes usar layers.Conv1D(padding='causal')).\n\n\n\nCausalidad en RNNs y CNNs\n\n\n\n\n\n\n\n\n\n\n\n\n\nUn modelo causal es eficiente de dos maneras:\n\nEn el entrenamiento, te permite calcular la pérdida para cada posición en la secuencia de salida ejecutando el modelo una sola vez.\nDurante la inferencia, para cada nuevo token generado, solo necesitas calcular sus salidas; las salidas de los elementos de la secuencia anterior se pueden reutilizar.\n\nPara una RNN, solo necesitas el estado de la RNN para tener en cuenta los cálculos previos (pasa return_state=True al constructor de la capa RNN).\nPara una CNN, necesitarías seguir el enfoque de Fast Wavenet.\n\n\nPara construir una capa de auto-atención causal, necesitas usar una máscara apropiada al calcular las puntuaciones de atención y sumar los values de atención.\nEsto se gestiona automáticamente si pasas use_causal_mask = True a la capa MultiHeadAttention cuando la llamas:\n\nclass CausalSelfAttention(BaseAttention):\n  def call(self, x):\n    attn_output = self.mha(\n        query=x,\n        value=x,\n        key=x,\n        use_causal_mask = True)\n    x = self.add([x, attn_output])\n    x = self.layernorm(x)\n    return x\n\nLa máscara causal garantiza que cada lugar sólo tenga acceso a los lugares que le preceden:\n\n\n\nCapa de auto-atención causal.\n\n\n\n\n\n\n\n\nDe nuevo, las conexiones residuales se omiten por simplicidad.\nLa representación más compacta de esta capa sería:\n\n\n\nCapa de auto-atención causal.\n\n\n\n\n\n\n\n\nEjemplo:\n\nsample_csa = CausalSelfAttention(num_heads=2, key_dim=512)\n\nprint(en_emb.shape)\nprint(sample_csa(en_emb).shape)\n\n(64, 64, 512)\n(64, 64, 512)\n\n\nLa salida de los primeros elementos de la secuencia no depende de los elementos posteriores, por lo que no debería importar si recorta los elementos antes o después de aplicar la capa:\n\n# ejemplo donde la sálida debe ser cercana a cero\n# ya que no deberia influir información posterior\nout1 = sample_csa(embed_en(en[:, :3]))\nout2 = sample_csa(embed_en(en))[:, :3]\n\ntf.reduce_max(abs(out1 - out2)).numpy()\n# nota omitir el warning por ahora\n\nnp.float32(7.1525574e-07)\n\n\nNota: Cuando se utilizan máscaras Keras, los valores de salida en lugares no válidos no están bien definidos. Por lo tanto, lo anterior puede no ser válido para las regiones enmascaradas.\n\n\n\nLa red feed forward (MLP)\nEl Transformer también incluye esta red neuronal feed-forward point-wise tanto en el codificador como en el decodificador:\n\n\n\nLa red feed forward (MLP)\n\n\n\n\n\n\n\n\nLa red consiste en dos capas lineales (tf.keras.layers.Dense) con una función de activación ReLU entre ellas, y una capa de dropout. Al igual que con las capas de atención, el código aquí también incluye la conexión residual y la normalización:\n\nclass FeedForward(tf.keras.layers.Layer):\n  def __init__(self, d_model, dff, dropout_rate=0.1):\n    super().__init__()\n    self.seq = tf.keras.Sequential([\n      tf.keras.layers.Dense(dff, activation='relu'),\n      tf.keras.layers.Dense(d_model),\n      tf.keras.layers.Dropout(dropout_rate)\n    ])\n    self.add = tf.keras.layers.Add()\n    self.layer_norm = tf.keras.layers.LayerNormalization()\n\n  def call(self, x):\n    x = self.add([x, self.seq(x)])\n    x = self.layer_norm(x)\n    return x\n\nPrueba:\n\nsample_ffn = FeedForward(512, 2048)\n\nprint(en_emb.shape)\nprint(sample_ffn(en_emb).shape)\n\n(64, 64, 512)\n(64, 64, 512)\n\n\n\n\nLa capa encoder\nEl codificador contiene una pila de N capas de codificador. Donde cada EncoderLayer contiene una capa GlobalSelfAttention y una capa FeedForward:\n\n\n\nLa capa encoder\n\n\n\n\n\n\n\n\nAquí la estructura de la capa EncoderLayer:\n\nclass EncoderLayer(tf.keras.layers.Layer):\n  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n    super().__init__()\n\n    self.self_attention = GlobalSelfAttention(\n        num_heads=num_heads,\n        key_dim=d_model,\n        dropout=dropout_rate)\n\n    self.ffn = FeedForward(d_model, dff)\n\n  def call(self, x):\n    x = self.self_attention(x)\n    x = self.ffn(x)\n    return x\n\nPrueba:\n\nsample_encoder_layer = EncoderLayer(d_model=512, num_heads=8, dff=2048)\n\nprint(pt_emb.shape)\nprint(sample_encoder_layer(pt_emb).shape)\n\n(64, 64, 512)\n(64, 64, 512)\n\n\n\n\nEl módulo Encoder\nConstruyamos el encoder, agregandole la parte de embedings y la codificación posicional.\n\n\n\nEl encoder\n\n\n\n\n\n\n\n\nEl codificador consiste en:\n\nUna capa PositionalEmbedding en la entrada.\nUna pila de capas EncoderLayer.\n\n\nclass Encoder(tf.keras.layers.Layer):\n  def __init__(self, *, num_layers, d_model, num_heads,\n               dff, vocab_size, dropout_rate=0.1):\n    super().__init__()\n\n    self.d_model = d_model\n    self.num_layers = num_layers\n\n    self.pos_embedding = PositionalEmbedding(\n        vocab_size=vocab_size, d_model=d_model)\n\n    self.enc_layers = [\n        EncoderLayer(d_model=d_model,\n                     num_heads=num_heads,\n                     dff=dff,\n                     dropout_rate=dropout_rate)\n        for _ in range(num_layers)]\n    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n\n  def call(self, x):\n    # `x` es token-IDs shape: (batch, seq_len)\n    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n\n    # añadir dropout.\n    x = self.dropout(x)\n\n    for i in range(self.num_layers):\n      x = self.enc_layers[i](x)\n\n    return x  # Shape `(batch_size, seq_len, d_model)`.\n\nProbar:\n\n# Instanciar el Encoder.\nsample_encoder = Encoder(num_layers=4,\n                         d_model=512,\n                         num_heads=8,\n                         dff=2048,\n                         vocab_size=8500)\n# Fijar training en false\nsample_encoder_output = sample_encoder(pt, training=False)\n\n# Dimensiones\nprint(pt.shape)\nprint(sample_encoder_output.shape)  # Shape `(batch_size, input_seq_len, d_model)`.\n# ignorar los warnings\n\n(64, 64)\n(64, 64, 512)\n\n\n\n\nLa capa decoder\nEl decodificador es ligeramente más compleja, con cada DecoderLayer conteniendo una capa CausalSelfAttention, una capa CrossAttention y una capa FeedForward:\n\n\n\nLa capa decoder\n\n\n\n\n\n\n\n\n\nclass DecoderLayer(tf.keras.layers.Layer):\n  def __init__(self,\n               *,\n               d_model,\n               num_heads,\n               dff,\n               dropout_rate=0.1):\n    super(DecoderLayer, self).__init__()\n\n    self.causal_self_attention = CausalSelfAttention(\n        num_heads=num_heads,\n        key_dim=d_model,\n        dropout=dropout_rate)\n\n    self.cross_attention = CrossAttention(\n        num_heads=num_heads,\n        key_dim=d_model,\n        dropout=dropout_rate)\n\n    self.ffn = FeedForward(d_model, dff)\n\n  def call(self, x, context):\n    x = self.causal_self_attention(x=x)\n    x = self.cross_attention(x=x, context=context)\n\n    # Solo para efectos de visualización posterior\n    self.last_attn_scores = self.cross_attention.last_attn_scores\n\n    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n    return x\n\nProbar la capa del decoder:\n\nsample_decoder_layer = DecoderLayer(d_model=512, num_heads=8, dff=2048)\n\nsample_decoder_layer_output = sample_decoder_layer(\n    x=en_emb, context=pt_emb)\n\nprint(en_emb.shape)\nprint(pt_emb.shape)\nprint(sample_decoder_layer_output.shape)  # `(batch_size, seq_len, d_model)`\n\n(64, 64, 512)\n(64, 64, 512)\n(64, 64, 512)\n\n\n\n\nEl módulo decoder\nDe forma similar al Codificador, el Decodificador consiste en un PositionalEmbedding y una pila de DecoderLayer’s:\n\n\n\nCapa Decoder + Embedding + PE\n\n\n\n\n\n\n\n\nDefinimos el Decoder extendiendo tf.keras.layers.Layer:\n\nclass Decoder(tf.keras.layers.Layer):\n  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n               dropout_rate=0.1):\n    super(Decoder, self).__init__()\n\n    self.d_model = d_model\n    self.num_layers = num_layers\n\n    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n                                             d_model=d_model)\n    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n    self.dec_layers = [\n        DecoderLayer(d_model=d_model, num_heads=num_heads,\n                     dff=dff, dropout_rate=dropout_rate)\n        for _ in range(num_layers)]\n\n    self.last_attn_scores = None\n\n  def call(self, x, context):\n    # `x` es token-IDs shape (batch, target_seq_len)\n    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n\n    x = self.dropout(x)\n\n    for i in range(self.num_layers):\n      x  = self.dec_layers[i](x, context)\n\n    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n\n    # EL shape de x es (batch_size, target_seq_len, d_model).\n    return x\n\nProbar:\n\n# Instanciar el decoder\nsample_decoder = Decoder(num_layers=4,\n                         d_model=512,\n                         num_heads=8,\n                         dff=2048,\n                         vocab_size=8000)\n\noutput = sample_decoder(\n    x=en,\n    context=pt_emb)\n\n# Shapes.\nprint(en.shape)\nprint(pt_emb.shape)\nprint(output.shape)\n\n(64, 64)\n(64, 64, 512)\n(64, 64, 512)\n\n\n\nsample_decoder.last_attn_scores.shape  # (batch, heads, target_seq, input_seq)\n\nTensorShape([64, 8, 64, 64])\n\n\nUna vez creados el codificador y el decodificador Transformer, es hora de construir el modelo Transformer y entrenarlo."
  },
  {
    "objectID": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#el-transformer",
    "href": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#el-transformer",
    "title": "Explicando la Mejora de los Transformers sobre las RNNs",
    "section": "El Transformer",
    "text": "El Transformer\nYou now have Encoder and Decoder. To complete the Transformer model, you need to put them together and add a final linear (Dense) layer which converts the resulting vector at each location into output token probabilities.\nThe output of the decoder is the input to this final linear layer.\n\n\n\nEl transformer\n\n\n\n\n\n\n\n\nUn Transformer con una capa tanto en el Codificador como en el Decodificador se parece casi exactamente al modelo del tutorial de RNN+atención. Un Transformer multi-capa tiene más capas, pero fundamentalmente está haciendo lo mismo.\n\n\n\nTransformer de una capa\n\n\nTransformer de 4 capas\n\n\n\n\n\n\n\n\n\n\n\n\nRNN + Modelo de atención\n\n\n\n\n\n\n\n\nCrea el Transformer extendiendo tf.keras.Model:\n\nNota: El artículo original, sección 3.4, comparte la matriz de pesos entre la capa de embedding y la capa lineal final. Para mantener las cosas simples, este tutorial utiliza dos matrices de pesos separadas.\n\n\nclass Transformer(tf.keras.Model):\n  def __init__(self, *, num_layers, d_model, num_heads, dff,\n               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n    super().__init__()\n    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n                           num_heads=num_heads, dff=dff,\n                           vocab_size=input_vocab_size,\n                           dropout_rate=dropout_rate)\n\n    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n                           num_heads=num_heads, dff=dff,\n                           vocab_size=target_vocab_size,\n                           dropout_rate=dropout_rate)\n\n    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n\n  def call(self, inputs):\n    # Para usar el `.fit` del modelo keras usted debe pasar\n    # todas las inputs como el primer argumento\n    context, x  = inputs\n\n    context = self.encoder(context)  # (batch_size, context_len, d_model)\n\n    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n\n    # Capa final densa lineal\n    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n\n    try:\n      # Eliminar las máscaras para que no afecten el cálculo del loss y métricas\n      # b/250038731 --&gt; relacionado con este bug que fue reportado\n      del logits._keras_mask\n    except AttributeError:\n      pass\n\n    # retornar la salida final\n    return logits\n\n\nHiperparámetros\nPara mantener este ejemplo pequeño y relativamente rápido, el número de capas (num_layers), la dimensionalidad de los embeddings (d_model) y la dimensionalidad interna de la capa FeedForward (dff) se han reducido.\nEl modelo base descrito en el artículo original del Transformer utilizaba num_layers=6, d_model=512 y dff=2048.\nEl número de cabezas de auto-atención será (num_heads=4).\n\nnum_layers = 2\nd_model = 128\ndff = 256\nnum_heads = 4\ndropout_rate = 0.1\n\n\n\nProbemos el transformer\nInstanciar el modelo Transformer:\n\ntransformer = Transformer(\n    num_layers=num_layers,\n    d_model=d_model,\n    num_heads=num_heads,\n    dff=dff,\n    input_vocab_size=tokenizers.pt.get_vocab_size().numpy(),\n    target_vocab_size=tokenizers.en.get_vocab_size().numpy(),\n    dropout_rate=dropout_rate)\n\nProbar\n\noutput = transformer((pt, en))\n\nprint(en.shape)\nprint(pt.shape)\nprint(output.shape)\n\n(64, 64)\n(64, 64)\n(64, 64, 7010)\n\n\n\n# acceder a los scores de atención\nattn_scores = transformer.decoder.dec_layers[-1].last_attn_scores\nprint(attn_scores.shape)  # (batch, heads, target_seq, input_seq)\n\n(64, 4, 64, 64)"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#training",
    "href": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#training",
    "title": "Explicando la Mejora de los Transformers sobre las RNNs",
    "section": "Training",
    "text": "Training\nTiempo de entrenar!!\n\nConfigurar el optimizador\nUtilizar el optimizador Adam con un planificador personalizado para la tasa de aprendizaje según la fórmula del Transformer original. paper.\n\\[\\Large{lrate = d_{model}^{-0.5} * \\min(step{\\_}num^{-0.5}, step{\\_}num \\cdot warmup{\\_}steps^{-1.5})}\\]\n\n\nExplicación esquema de Learning Rate del Transformer\nParámetros:\n\nd_model (Dimensionalidad del Modelo):\n\nIntuición: “Ancho” del modelo. Mayor d_model = representaciones más ricas, más capacidad.\nEfecto: Actúa como un factor de escala base para el learning rate.\n\nSi d_model es grande, este factor será pequeño, lo que tiende a reducir el learning rate general. La intuición es que modelos más grandes pueden necesitar pasos de aprendizaje más pequeños para evitar inestabilidad debido a su mayor número de parámetros.\nSi d_model es pequeño, este factor será más grande, lo que tiende a aumentar el learning rate general.\n\n\nstep_num (Número de Paso de Entrenamiento):\n\nIntuición: Progreso del entrenamiento (iteración actual).\nEfecto: Determina la fase del learning rate:\n\nFase de Decaimiento (\\({step\\\\_num}^{-0.5}\\)): Este término hace que el learning rate disminuya a medida que avanza el entrenamiento. La intuición es que al principio queremos dar pasos de aprendizaje más grandes para explorar el espacio de parámetros rápidamente. A medida que nos acercamos a un buen conjunto de pesos, queremos dar pasos más pequeños para “afinar” los valores y evitar oscilar alrededor del mínimo óptimo. La disminución es más pronunciada al principio y se ralentiza con el tiempo.\nFase de Calentamiento (\\({step\\\\_num * warmup\\\\_steps}^{-1.5}\\)): Este término está activo principalmente durante la fase de “calentamiento”. Hace que el learning rate aumente linealmente con el número de paso hasta que step_num se acerca a warmup_steps.\n\n\nwarmup_steps (Pasos de Calentamiento):\n\nIntuición: Duración de la fase inicial de aumento gradual del learning rate.\nEfecto: Controla cuántos pasos se tarda en alcanzar el learning rate “base”. Ayuda a evitar inestabilidad al inicio.\n\n\nEn resumen:\nEl learning rate:\n\nAumenta gradualmente durante los primeros warmup_steps para estabilizar el entrenamiento inicial.\nDisminuye gradualmente después de warmup_steps para permitir una convergencia fina.\nSu magnitud general está influenciada por la dimensionalidad del modelo (d_model).\n\n\nclass CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n  def __init__(self, d_model, warmup_steps=4000):\n    super().__init__()\n\n    self.d_model = d_model\n    self.d_model = tf.cast(self.d_model, tf.float32)\n\n    self.warmup_steps = warmup_steps\n\n  def __call__(self, step):\n    step = tf.cast(step, dtype=tf.float32)\n    arg1 = tf.math.rsqrt(step)\n    arg2 = step * (self.warmup_steps ** -1.5)\n\n    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n\nInstanciar el optimizador (tf.keras.optimizers.Adam):\n\nlearning_rate = CustomSchedule(d_model)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n                                     epsilon=1e-9)\n\nProbar el scheduler para el learning rate:\n\nplot_lr_planificador(learning_rate)\n\n\n\n\n\n\n\n\n\n\nAjustar función de pérdida y métricas\nDado que las secuencias objetivo están rellenadas (padded), es importante aplicar una máscara de padding al calcular la pérdida. Utiliza la función de pérdida de entropía cruzada (tf.keras.losses.SparseCategoricalCrossentropy):\n\ndef masked_loss(label, pred):\n  mask = label != 0 # Indica padding\n  # from_logits=True indica que las predicciones no han pasado por softmax.\n  # reduction='none' hace que se devuelva la pérdida por cada ejemplo.\n  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n      from_logits=True, reduction='none')\n\n  loss = loss_object(label, pred)\n  mask = tf.cast(mask, dtype=loss.dtype)\n  # Aplica la máscara a la pérdida, multiplicando las pérdidas de las posiciones\n  # de padding por cero, lo que las elimina del cálculo total.\n  loss *= mask\n\n  # Calcula la pérdida promedio solo sobre las posiciones no enmascaradas.\n  # Suma todas las pérdidas y divide por el número de posiciones no enmascaradas.\n  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n  return loss\n\n\ndef masked_accuracy(label, pred):\n  # Obtiene la clase predicha con la probabilidad más alta (el índice máximo)\n  # a lo largo del eje de vocabulario (axis=2).\n  pred = tf.argmax(pred, axis=2)\n  # Convierte las etiquetas al mismo tipo de dato que las predicciones para comparar.\n  label = tf.cast(label, pred.dtype)\n  # Crea un tensor booleano donde True indica que la predicción coincide con la etiqueta.\n  match = label == pred\n\n  # Crea una máscara donde True indica que la etiqueta no es padding (no es 0).\n  mask = label != 0\n\n  # Combina la máscara con las coincidencias. Solo consideramos como \"match\"\n  # las predicciones correctas en las posiciones que no son padding.\n  match = match & mask\n\n  # Convierte los booleanos de 'match' y 'mask' a float para poder calcular la media.\n  match = tf.cast(match, dtype=tf.float32)\n  mask = tf.cast(mask, dtype=tf.float32)\n  # Calcula la precisión promedio solo sobre las posiciones no enmascaradas.\n  # Suma las coincidencias (1 para cada predicción correcta no enmascarada)\n  # y divide por el número total de posiciones no enmascaradas.\n  return tf.reduce_sum(match)/tf.reduce_sum(mask)\n\n\n\nEntrenar el modelo\nCon todo listo, vamos a compilar usando model.compile, y luego entrenar con model.fit:\n\ntransformer.compile(\n    loss=masked_loss,\n    optimizer=optimizer,\n    metrics=[masked_accuracy])\n\n\ntransformer.fit(train_batches,\n                epochs=3,\n                validation_data=val_batches)\n\n\nEpoch 1/3\n\n810/810 ━━━━━━━━━━━━━━━━━━━━ 1037s 1s/step - loss: 7.7200 - masked_accuracy: 0.0833 - val_loss: 4.9830 - val_masked_accuracy: 0.2540\n\nEpoch 2/3\n\n810/810 ━━━━━━━━━━━━━━━━━━━━ 531s 633ms/step - loss: 4.6781 - masked_accuracy: 0.2912 - val_loss: 3.9608 - val_masked_accuracy: 0.3668\n\nEpoch 3/3\n\n810/810 ━━━━━━━━━━━━━━━━━━━━ 424s 463ms/step - loss: 3.6996 - masked_accuracy: 0.4002 - val_loss: 3.4028 - val_masked_accuracy: 0.4305\n\n\n\n\n&lt;keras.src.callbacks.history.History at 0x7bffd0607690&gt;"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#ejecutar-inferencia",
    "href": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#ejecutar-inferencia",
    "title": "Explicando la Mejora de los Transformers sobre las RNNs",
    "section": "Ejecutar inferencia",
    "text": "Ejecutar inferencia\nAhora puedes probar el modelo realizando una traducción. Los siguientes pasos se utilizan para la inferencia:\n\nCodifica la frase de entrada utilizando el tokenizador portugués (tokenizers.pt). Esta es la entrada del codificador.\nLa entrada del decodificador se inicializa con el token [START].\nCalcula las máscaras de padding y las máscaras causales (para la auto-atención causal).\nEl decodificador luego genera las predicciones observando la salida del codificador y su propia salida (auto-atención).\nConcatena el token predicho a la entrada del decodificador y lo pasa de nuevo al decodificador.\nEn este enfoque, el decodificador predice el siguiente token basándose en los tokens que predijo previamente.\n\nNota: El modelo está optimizado para un entrenamiento eficiente y realiza una predicción del siguiente token para cada token en la salida simultáneamente. Esto es redundante durante la inferencia, y solo se utiliza la última predicción. Este modelo puede hacerse más eficiente para la inferencia si solo se calcula la última predicción cuando se ejecuta en modo de inferencia (training=False).\nDefine la clase Translator extendiendo tf.Module:\n\nclass Translator(tf.Module):\n  def __init__(self, tokenizers, transformer):\n    self.tokenizers = tokenizers\n    self.transformer = transformer\n\n  def __call__(self, sentence, max_length=MAX_TOKENS):\n    # La frase de entrada es portugués, por lo que se añaden los tokens `[START]` y `[END]`.\n    assert isinstance(sentence, tf.Tensor)\n    if len(sentence.shape) == 0:\n      sentence = sentence[tf.newaxis]\n\n    sentence = self.tokenizers.pt.tokenize(sentence).to_tensor()\n\n    encoder_input = sentence\n\n    # Como el lenguaje de sálida es inglés\n    # Inicializar con el token `[START]`\n    start_end = self.tokenizers.en.tokenize([''])[0]\n    start = start_end[0][tf.newaxis]\n    end = start_end[1][tf.newaxis]\n\n    # con el TensorArray es posible hacer seguimiento al blucle dinámico\n    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n    output_array = output_array.write(0, start)\n\n    for i in tf.range(max_length):\n      # convierte la lista de tokens generados secuencialmente\n      # (almacenada en el TensorArray) en un tensor con la forma (1, secuencia_de_tokens),\n      # donde la secuencia de tokens representa la traducción generada hasta ese punto.\n      output = tf.transpose(output_array.stack())\n      predictions = self.transformer([encoder_input, output], training=False)\n\n      # Seleccionar las predicciones para el último token de `seq_len`.\n      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n      # Encontrar la pos del token id con la mayor probabilidad\n      predicted_id = tf.argmax(predictions, axis=-1)\n\n      # Concatenar la predicción con los anteriores tokens del decoder.\n      output_array = output_array.write(i+1, predicted_id[0])\n\n      if predicted_id == end:\n        break\n\n    output = tf.transpose(output_array.stack())\n    # La dimensión de sálida es `(1, tokens)`.\n    text = tokenizers.en.detokenize(output)[0]  # Shape: `()`.\n\n    tokens = tokenizers.en.lookup(output)[0]\n\n    # `@tf.function` optimiza la función, dificultando el acceso\n    # a valores dinámicos en bucles. Recalculamos la atención\n    # final fuera del bucle para obtener `attention_weights`.\n    self.transformer([encoder_input, output[:,:-1]], training=False)\n    attention_weights = self.transformer.decoder.last_attn_scores\n\n    return text, tokens, attention_weights\n\nNota: Esta función utiliza un bucle unrolled, no un bucle dinámico. Genera MAX_TOKENS en cada llamada. Consulta el tutorial de NMT con atención para ver un ejemplo de implementación con un bucle dinámico, que puede ser mucho más eficiente.\nProbemos la traducción:\n\ntranslator = Translator(tokenizers, transformer)\n\nEjemplo 1:\n\nsentence = 'este é um problema que temos que resolver.'\nground_truth = 'this is a problem we have to solve .'\n\ntranslated_text, translated_tokens, attention_weights = translator(\n    tf.constant(sentence))\nplot_traduccion(sentence, translated_text, ground_truth)\n\nInput:         : este é um problema que temos que resolver.\nPredicción     : this is a problem that we have to solve .\nGround truth   : this is a problem we have to solve .\n\n\nEjemplo 2:\n\nsentence = 'os meus vizinhos ouviram sobre esta ideia.'\nground_truth = 'and my neighboring homes heard about this idea .'\n\ntranslated_text, translated_tokens, attention_weights = translator(\n    tf.constant(sentence))\nplot_traduccion(sentence, translated_text, ground_truth)\n\nInput:         : os meus vizinhos ouviram sobre esta ideia.\nPredicción     : my friends heard about this idea .\nGround truth   : and my neighboring homes heard about this idea .\n\n\nEjemplo 3:\n\nsentence = 'vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.'\nground_truth = \"so i'll just share with you some stories very quickly of some magical things that have happened.\"\n\ntranslated_text, translated_tokens, attention_weights = translator(\n    tf.constant(sentence))\nplot_traduccion(sentence, translated_text, ground_truth)\n\nInput:         : vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.\nPredicción     : i ' m going very quickly quickly quickly to share with some stories of things , things that will be going to be going to be going to be going to be going to be going to be able to be able to be able to be very good .\nGround truth   : so i'll just share with you some stories very quickly of some magical things that have happened."
  },
  {
    "objectID": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#crear-los-plots-de-atención",
    "href": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#crear-los-plots-de-atención",
    "title": "Explicando la Mejora de los Transformers sobre las RNNs",
    "section": "Crear los plots de atención",
    "text": "Crear los plots de atención\nUsando la clase traductor Translator que almacena los scores de atención, podemos usarlos para ver su relevancia:\nPor ejemplo:\n\nsentence = 'este é o primeiro livro que eu fiz.'\nground_truth = \"this is the first book i've ever done.\"\n\ntranslated_text, translated_tokens, attention_weights = translator(\n    tf.constant(sentence))\nplot_traduccion(sentence, translated_text, ground_truth)\n\nInput:         : este é o primeiro livro que eu fiz.\nPredicción     : this is the first book that i did i did .\nGround truth   : this is the first book i've ever done.\n\n\nCrear una función que grafique la atención cuando se genera un token:\n\nhead = 0\n# Shape: `(batch=1, num_heads, seq_len_q, seq_len_k)`.\nattention_heads = tf.squeeze(attention_weights, 0)\nattention = attention_heads[head]\nattention.shape\n\nTensorShape([12, 11])\n\n\nSon las inputs tokens en portugués:\n\nin_tokens = tf.convert_to_tensor([sentence])\nin_tokens = tokenizers.pt.tokenize(in_tokens).to_tensor()\nin_tokens = tokenizers.pt.lookup(in_tokens)[0]\nin_tokens\n\n&lt;tf.Tensor: shape=(11,), dtype=string, numpy=\narray([b'[START]', b'este', b'e', b'o', b'primeiro', b'livro', b'que',\n       b'eu', b'fiz', b'.', b'[END]'], dtype=object)&gt;\n\n\nEstas son las sálidas (tokens en inglés, traducción)\n\ntranslated_tokens\n\n&lt;tf.Tensor: shape=(13,), dtype=string, numpy=\narray([b'[START]', b'this', b'is', b'the', b'first', b'book', b'that',\n       b'i', b'did', b'i', b'did', b'.', b'[END]'], dtype=object)&gt;\n\n\n\nplot_attention_weights(sentence,\n                       translated_tokens,\n                       attention_weights[0])"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#exportar-el-modelo",
    "href": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#exportar-el-modelo",
    "title": "Explicando la Mejora de los Transformers sobre las RNNs",
    "section": "Exportar el modelo",
    "text": "Exportar el modelo\nHemos probado el modelo y la inferencia funciona. A continuación, puedes exportarlo como un tf.saved_model. Para aprender cómo guardar y cargar un modelo en formato SavedModel, consulta esta guía.\nCrea una clase llamada ExportTranslator extendiendo la subclase tf.Module con un @tf.function en el método __call__:\n\nclass ExportTranslator(tf.Module):\n  def __init__(self, translator):\n    self.translator = translator\n\n  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n  def __call__(self, sentence):\n    (result,\n     tokens,\n     attention_weights) = self.translator(sentence, max_length=MAX_TOKENS)\n\n    return result\n\n\n# Empaqueta el objeto `translator` en la nueva clase `ExportTranslator` creada:\ntranslator = ExportTranslator(translator)\n\nDado que el modelo está decodificando las predicciones utilizando tf.argmax, las predicciones son deterministas. El modelo original y uno recargado desde su SavedModel deberían dar predicciones idénticas:\n\ntranslator('este é o primeiro livro que eu fiz.').numpy()\n\nb'this is the first book that i did i did .'\n\n\n\ntf.saved_model.save(translator, export_dir='translator')\n\n\nreloaded = tf.saved_model.load('translator')\n\n\nreloaded('este é o primeiro livro que eu fiz.').numpy()\n\nb'this is the first book that i did i did .'"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#conclusión",
    "href": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#conclusión",
    "title": "Explicando la Mejora de los Transformers sobre las RNNs",
    "section": "Conclusión",
    "text": "Conclusión\nEn este tutorial aprendiste sobre:\n\nLos Transformers y su importancia en el aprendizaje automático\nAtención, auto-atención y atención multi-cabeza\nCodificación posicional con embeddings\nLa arquitectura codificador-decodificador del Transformer original\nEnmascaramiento en la auto-atención\nCómo juntar todo para traducir texto\n\nLas desventajas de esta arquitectura son:\n\nPara una serie temporal, la salida para un paso de tiempo se calcula a partir de la historia completa en lugar de solo las entradas y el estado oculto actual. Esto podría ser menos eficiente.\nSi la entrada tiene una relación temporal/espacial, como texto o imágenes, debe añadirse alguna codificación posicional o el modelo efectivamente verá una bolsa de palabras.\n\nEste notebook se basó en el notebook de Neural Machine Translation with a Transformer and Keras para el curso de Deep Learning práctico en 3 semanas."
  },
  {
    "objectID": "semana_3/notebooks/Nb_3a_Introduccion_transfer_learning_finetuning.html",
    "href": "semana_3/notebooks/Nb_3a_Introduccion_transfer_learning_finetuning.html",
    "title": "Introducción a pre-trained models, transfer learning and fine tuning",
    "section": "",
    "text": "Última actualización 04/08/2025\n#@title Eliminar librerías (Problema de compatibilidad reciente con numpy )\nfrom IPython.display import clear_output\n\n!pip uninstall  -y numpy\n!pip -q install numpy==1.26.4\n\nclear_output()\n#@title Ejecutar esta celda para reiniciar el entorno de ejecución\nimport os\nos.kill(os.getpid(), 9)\n#@title Instalar librerías necesarias\n\n!pip -q install keras-nlp\n!pip -q install gensim\n!pip -q install pytorch-tabnet\n\n\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n\nimport keras_hub\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom tensorflow.keras import layers, models, optimizers, callbacks\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.utils import to_categorical\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import backend as K\nfrom gensim.models import KeyedVectors\nfrom IPython.display import clear_output\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import fetch_covtype\nfrom pytorch_tabnet.tab_model import TabNetClassifier\nimport torch\n\nclear_output()\n\nprint(\"Se han importado todas las librerías correctamente\")\n\nSe han importado todas las librerías correctamente\n#@title Definir funciones complementarias\ndef plot_graphs(history, metric):\n  plt.plot(history.history[metric])\n  plt.plot(history.history['val_'+metric], '')\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(metric)\n  plt.legend([metric, 'val_'+metric])\n\ndef plot_curvas(history):\n  plt.figure(figsize=(12, 6))\n  plt.subplot(1, 2, 1)\n  plot_graphs(history, 'accuracy')\n  plt.ylim(None, 1)\n  plt.subplot(1, 2, 2)\n  plot_graphs(history, 'loss')\n  plt.ylim(0, None)\n\ndef cosine_similarity(vec_a, vec_b):\n  \"\"\"Compute cosine similarity between vec_a and vec_b\"\"\"\n  return np.dot(vec_a, vec_b) / (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))\n\n# dibujamos ciertos ejemplos de entrenamiento\ndef plot_imagenes(train_images,train_labels):\n  plt.figure(figsize=(10, 4))\n  for i in range(10):\n      plt.subplot(2, 5, i + 1)\n      plt.grid(False)\n      plt.xticks([])\n      plt.yticks([])\n      plt.imshow(train_images[i], cmap=plt.cm.gray)\n      plt.xlabel(train_labels[i])\n  plt.show()\n\ndef plot_predicciones(model,test_images):\n  plt.figure(figsize=(10, 4))\n  for i in range(10):\n      plt.subplot(2, 5, i + 1)\n      plt.grid(False)\n      plt.xticks([])\n      plt.yticks([])\n      plt.imshow(test_images[i], cmap=plt.cm.gray)\n      pred = np.argmax(model.predict(np.expand_dims(test_images[i], axis=0), verbose=False))\n      plt.xlabel(f\"Pred: {pred}\")\n  plt.show()"
  },
  {
    "objectID": "semana_3/notebooks/Nb_3a_Introduccion_transfer_learning_finetuning.html#transfer-learning-y-fine-tuning-en-clasificación-de-imágenes",
    "href": "semana_3/notebooks/Nb_3a_Introduccion_transfer_learning_finetuning.html#transfer-learning-y-fine-tuning-en-clasificación-de-imágenes",
    "title": "Introducción a pre-trained models, transfer learning and fine tuning",
    "section": "Transfer Learning y fine tuning en Clasificación de imágenes",
    "text": "Transfer Learning y fine tuning en Clasificación de imágenes\n\nDescargar los datos\n\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\nprint(\"Train shape: \", train_images.shape)\nprint(\"Test shape: \", test_images.shape)\n\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n\n11490434/11490434 ━━━━━━━━━━━━━━━━━━━━ 2s 0us/step\n\nTrain shape:  (60000, 28, 28)\n\nTest shape:  (10000, 28, 28)\n\n\n\n\n\nplot_imagenes(train_images,train_labels)\n\n\n\n\n\n\n\n\n\n# Redimensionamos las imagenes a 32x32 (el minimo tamaño que soporta vgg16)\ntrain_images = tf.image.grayscale_to_rgb(tf.expand_dims(train_images, axis=-1))\ntest_images = tf.image.grayscale_to_rgb(tf.expand_dims(test_images, axis=-1))\n# Agregamos 3 canales de color (RGB) debido a que la red tambien lo necesita\ntrain_images = tf.image.resize(train_images, [32, 32])\ntest_images = tf.image.resize(test_images, [32, 32])\n# Normalizamos las imagenes entre [0, 1] para que el aprendizaje sea mas suave\ntrain_images = train_images / 255.0\ntest_images = test_images / 255.0\nprint(\"Train shape: \", train_images.shape)\nprint(\"Test shape: \", test_images.shape)\n\nTrain shape:  (60000, 32, 32, 3)\nTest shape:  (10000, 32, 32, 3)\n\n\n\n# Convertimos las etiquetas a formato one-hot encoding\ntrain_labels_ohc = to_categorical(train_labels, 10)\ntest_labels_ohc = to_categorical(test_labels, 10)\nprint(train_labels_ohc[:,1])\n\n[0. 0. 0. ... 0. 0. 0.]\n\n\n\n\nTransfer-learning\nEn esta sección, utilizaremos el modelo pre-entrenado VGG16, el cual es una red convolucional que es usada para reconocimiento de imágenes. Este modelo fue entrenado con ImageNet, por lo cual es un modelo bastante robusto.\nObjetivo: - El objetivo será cargar dicho modelo pre-entrenado y utilizarlo sobre el dataset MNIST, el cual contiene 70.000 imagenes de digitos escritos a mano. Así, poder clasificar dichos números con nuestro modelo entrenado previamente.\n\n# Cargamos el modelo VGG16 preentrenado, excluyendo las capas superiores (top=False)\n# Recuerde que las capas superiores son las que definen el tipo de problema a solucionar\n# Como nuestro problema es de 10 categorias (10 digitos), agregaremos nuestras propias capas superiores\nvgg16_base = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\nvgg16_base.summary()\n\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n\n58889256/58889256 ━━━━━━━━━━━━━━━━━━━━ 3s 0us/step\n\n\n\n\nModel: \"vgg16\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (InputLayer)        │ (None, 32, 32, 3)      │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_conv1 (Conv2D)           │ (None, 32, 32, 64)     │         1,792 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_conv2 (Conv2D)           │ (None, 32, 32, 64)     │        36,928 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_pool (MaxPooling2D)      │ (None, 16, 16, 64)     │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_conv1 (Conv2D)           │ (None, 16, 16, 128)    │        73,856 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_conv2 (Conv2D)           │ (None, 16, 16, 128)    │       147,584 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_pool (MaxPooling2D)      │ (None, 8, 8, 128)      │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv1 (Conv2D)           │ (None, 8, 8, 256)      │       295,168 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv2 (Conv2D)           │ (None, 8, 8, 256)      │       590,080 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv3 (Conv2D)           │ (None, 8, 8, 256)      │       590,080 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_pool (MaxPooling2D)      │ (None, 4, 4, 256)      │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block4_conv1 (Conv2D)           │ (None, 4, 4, 512)      │     1,180,160 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block4_conv2 (Conv2D)           │ (None, 4, 4, 512)      │     2,359,808 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block4_conv3 (Conv2D)           │ (None, 4, 4, 512)      │     2,359,808 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block4_pool (MaxPooling2D)      │ (None, 2, 2, 512)      │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block5_conv1 (Conv2D)           │ (None, 2, 2, 512)      │     2,359,808 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block5_conv2 (Conv2D)           │ (None, 2, 2, 512)      │     2,359,808 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block5_conv3 (Conv2D)           │ (None, 2, 2, 512)      │     2,359,808 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block5_pool (MaxPooling2D)      │ (None, 1, 1, 512)      │             0 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 14,714,688 (56.13 MB)\n\n\n\n Trainable params: 14,714,688 (56.13 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# Ahora congelamos los pesos del modelo, pues solo queremos agrega una nueva capa\n# con 10 neuronas, donde cada una representará el digito que queremos predecir\nvgg16_base.trainable = False\nmodel = models.Sequential([\n    vgg16_base,\n    layers.Flatten(),\n    layers.Dropout(0.5),\n    layers.Dense(10, activation='softmax')  # 10 clases de salida\n])\n\n\n# Compilamos y entrenamos los pesos de nuestra última capa\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model.fit(train_images, train_labels_ohc, epochs=5,\n                    batch_size=64, validation_data=(test_images, test_labels_ohc))\n\n\nEpoch 1/5\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 13s 9ms/step - accuracy: 0.5480 - loss: 1.4039 - val_accuracy: 0.8680 - val_loss: 0.5527\n\nEpoch 2/5\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 5s 5ms/step - accuracy: 0.8086 - loss: 0.6411 - val_accuracy: 0.8936 - val_loss: 0.4141\n\nEpoch 3/5\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 5s 6ms/step - accuracy: 0.8306 - loss: 0.5443 - val_accuracy: 0.9095 - val_loss: 0.3555\n\nEpoch 4/5\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 5s 5ms/step - accuracy: 0.8401 - loss: 0.5059 - val_accuracy: 0.9176 - val_loss: 0.3233\n\nEpoch 5/5\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 5s 5ms/step - accuracy: 0.8428 - loss: 0.4853 - val_accuracy: 0.9235 - val_loss: 0.3022\n\n\n\n\n\n# Medimos la precisión del modelo en el conjunto de prueba\ntest_loss, test_acc = model.evaluate(test_images, test_labels_ohc)\n\nprint(f\"Precisión en el conjunto de prueba: {test_acc}\")\n\n\n313/313 ━━━━━━━━━━━━━━━━━━━━ 2s 4ms/step - accuracy: 0.9145 - loss: 0.3246\n\nPrecisión en el conjunto de prueba: 0.9235000014305115\n\n\n\n\n\n# Mostramos el modelo\nmodel.summary()\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ vgg16 (Functional)              │ (None, 1, 1, 512)      │    14,714,688 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (Flatten)               │ (None, 512)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_12 (Dropout)            │ (None, 512)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (Dense)                   │ (None, 10)             │         5,130 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 14,730,080 (56.19 MB)\n\n\n\n Trainable params: 5,130 (20.04 KB)\n\n\n\n Non-trainable params: 14,714,688 (56.13 MB)\n\n\n\n Optimizer params: 10,262 (40.09 KB)\n\n\n\n\n# Dibujamos ciertas imágenes con sus predicciones\nplot_predicciones(model,test_images)\n\n\n\n\n\n\n\n\nObservaciones:\n\nNote que al cargar un modelo pre-entrenado, logramos tener unos pesos que ya saben encontrar ciertos tipos de características dentro de las imágenes. Es por ello que cuando entrenamos nuestra capa superior (10 neuronas), solo hacen falta 5 épocas para alcanzar un accuracy del 92.14% en el conjunto de prueba.\nCabe resaltar que utilizamos un modelo pre-entrenado y agregamos una capa superior para adaptarlo a nuestro problema. Esto se podría considerar transfer learning tambien.\n\n\n\nTuning o Re-entrenamiento\nPara dejar mas claro el concepto de transfer learning lo que haremos es coger el mismo modelo definido anteriormente, solo que esta vez si entrenaremos los pesos del modelo pre-entrenado, para así alcanzar un mejor rendimiento.\n\n# Reiniciar el backend para que las ejecuciones anteriores no interfieran\nK.clear_session()\n\n\n# Definimos el modelo, especificando que queremos entrenar el modelo VGG16\nvgg16_base.trainable = True\nmodel_2 = models.Sequential([\n    vgg16_base,\n    layers.Flatten(),\n    layers.Dropout(0.5),\n    layers.Dense(10, activation='softmax')  # 10 clases de salida\n])\n\n\n# Compilamos y entrenamos los pesos de nuestra última capa\nmodel_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model_2.fit(train_images, train_labels_ohc, epochs=5,\n                    batch_size=64, validation_data=(test_images, test_labels_ohc))\n\n\nEpoch 1/5\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 31s 22ms/step - accuracy: 0.1100 - loss: 2.3122 - val_accuracy: 0.1135 - val_loss: 2.3011\n\nEpoch 2/5\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 15s 16ms/step - accuracy: 0.1129 - loss: 2.3012 - val_accuracy: 0.1135 - val_loss: 2.3010\n\nEpoch 3/5\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 15s 16ms/step - accuracy: 0.1132 - loss: 2.3009 - val_accuracy: 0.1135 - val_loss: 2.3011\n\nEpoch 4/5\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 15s 16ms/step - accuracy: 0.1119 - loss: 2.3012 - val_accuracy: 0.1135 - val_loss: 2.3010\n\nEpoch 5/5\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 15s 16ms/step - accuracy: 0.1119 - loss: 2.3014 - val_accuracy: 0.1135 - val_loss: 2.3011\n\n\n\n\n\n# Medimos la precisión del modelo 2 en el conjunto de prueba\ntest_loss, test_acc = model_2.evaluate(test_images, test_labels_ohc)\n\nprint(f\"Precisión en el conjunto de prueba: {test_acc}\")\n\n\n313/313 ━━━━━━━━━━━━━━━━━━━━ 2s 4ms/step - accuracy: 0.1160 - loss: 2.3010\n\nPrecisión en el conjunto de prueba: 0.11349999904632568\n\n\n\n\n\n# Imprimamos la estructura del modelo 2\nmodel_2.summary()\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ vgg16 (Functional)              │ (None, 1, 1, 512)      │    14,714,688 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (Flatten)               │ (None, 512)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (Dropout)               │ (None, 512)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (Dense)                   │ (None, 10)             │         5,130 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 44,159,456 (168.45 MB)\n\n\n\n Trainable params: 14,719,818 (56.15 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n Optimizer params: 29,439,638 (112.30 MB)\n\n\n\nObservaciones:\n\nAntes de hablar del mal rendimiento del modelo (un 9.7% de accuracy en el conjunto de prueba). Hay que hablar de que ahora demoró mas entrenandose. Esto se debe a que ahora, se ajustaron todos los parámeros posibles, no como en el modelo anterior que solo ajustamos los parámetros de la capa superior.\nUna de las razones por las cuales se obtuvo un accuracy muy bajo, es debido a que empezamos a ajustar el modelo pre-entrenado, pero pasamos de tener 512 neuronas como salida del modelo pre-entrenado, a solo tener 10. Entonces ese error se propagó y ajusto erróneamente los pesos ya entrenados. Lo cual llevó a que el modelo no mejorara.\nPara mitigar este error, utilizaremos fine tunning ajustando mas la capa superior. Así podremos tener un mejor rendimiento de nuestro modelo de transfer learning\n\n\n\nFine tunning\n\nPara el ajuste fino, lo que haremos es lo siguiente:\n\nCongelaremos las primeras capas del modelo pre-entrenado\nAgregaremos unas capas superiores al modelo.\nEntrenaremos el modelo así.\nDespués, descongelaremos capas superiores del modelo pre-entrenado y hacemos ese ajuste fino (entrenamos) para aumentar el acierto del modelo.\n\n\n\n# Reiniciar el backend para que las ejecuciones anteriores no interfieran\nK.clear_session()\n\n\nvgg16_base = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n\nfor layer in vgg16_base.layers[:15]:  # Congelar las primeras 15 capas\n    layer.trainable = False\n\n# Agregamos mas neuronas después de nuestro modelo pre-entrenado, para hacer un ajuste mas fino\nmodel_3 = models.Sequential([\n    vgg16_base,\n    layers.Flatten(),\n    layers.Dense(512, activation='relu'),  # Incrementamos el número de unidades para mayor capacidad de representación\n    layers.Dropout(0.5),                   # Aumentamos el Dropout para evitar el sobreajuste\n    layers.Dense(10, activation='softmax') # Capa final con 10 clases\n])\n\n\n# compilamos el modelo y definimos una parada temprana para mitigar el sobreajuste\nmodel_3.compile(optimizer=optimizers.Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\nearly_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\n\n# Entrenamos nuestro modelo\nhistory = model_3.fit(train_images, train_labels_ohc, batch_size=64,\n                      epochs=20,\n                      validation_data=(test_images, test_labels_ohc),\n                      callbacks=[early_stopping])\n\n\nEpoch 1/20\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 17s 13ms/step - accuracy: 0.8968 - loss: 0.3208 - val_accuracy: 0.9841 - val_loss: 0.0493\n\nEpoch 2/20\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 7s 8ms/step - accuracy: 0.9850 - loss: 0.0495 - val_accuracy: 0.9894 - val_loss: 0.0372\n\nEpoch 3/20\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 7s 8ms/step - accuracy: 0.9884 - loss: 0.0397 - val_accuracy: 0.9913 - val_loss: 0.0275\n\nEpoch 4/20\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 7s 8ms/step - accuracy: 0.9912 - loss: 0.0292 - val_accuracy: 0.9925 - val_loss: 0.0288\n\nEpoch 5/20\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 7s 8ms/step - accuracy: 0.9929 - loss: 0.0242 - val_accuracy: 0.9854 - val_loss: 0.0503\n\nEpoch 6/20\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 7s 8ms/step - accuracy: 0.9937 - loss: 0.0216 - val_accuracy: 0.9908 - val_loss: 0.0320\n\nEpoch 7/20\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 7s 8ms/step - accuracy: 0.9937 - loss: 0.0199 - val_accuracy: 0.9897 - val_loss: 0.0335\n\nEpoch 8/20\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 7s 8ms/step - accuracy: 0.9947 - loss: 0.0184 - val_accuracy: 0.9914 - val_loss: 0.0270\n\nEpoch 9/20\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 7s 8ms/step - accuracy: 0.9949 - loss: 0.0169 - val_accuracy: 0.9919 - val_loss: 0.0288\n\nEpoch 10/20\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 7s 8ms/step - accuracy: 0.9958 - loss: 0.0136 - val_accuracy: 0.9917 - val_loss: 0.0338\n\nEpoch 11/20\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 7s 8ms/step - accuracy: 0.9959 - loss: 0.0132 - val_accuracy: 0.9929 - val_loss: 0.0277\n\nEpoch 12/20\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 7s 8ms/step - accuracy: 0.9962 - loss: 0.0136 - val_accuracy: 0.9919 - val_loss: 0.0294\n\nEpoch 13/20\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 7s 8ms/step - accuracy: 0.9960 - loss: 0.0127 - val_accuracy: 0.9929 - val_loss: 0.0228\n\nEpoch 14/20\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 7s 8ms/step - accuracy: 0.9974 - loss: 0.0090 - val_accuracy: 0.9888 - val_loss: 0.0443\n\nEpoch 15/20\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 7s 8ms/step - accuracy: 0.9968 - loss: 0.0114 - val_accuracy: 0.9921 - val_loss: 0.0278\n\nEpoch 16/20\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 7s 8ms/step - accuracy: 0.9973 - loss: 0.0094 - val_accuracy: 0.9903 - val_loss: 0.0392\n\nEpoch 17/20\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 7s 8ms/step - accuracy: 0.9968 - loss: 0.0105 - val_accuracy: 0.9924 - val_loss: 0.0302\n\nEpoch 18/20\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 7s 8ms/step - accuracy: 0.9976 - loss: 0.0091 - val_accuracy: 0.9930 - val_loss: 0.0325\n\n\n\n\n\n# Evaluamos el accuracy del modelo en los datos de prueba\ntest_loss, test_acc = model_3.evaluate(test_images, test_labels_ohc)\nprint(f\"Precisión después del fine-tuning avanzado: {test_acc}\")\n\n\n313/313 ━━━━━━━━━━━━━━━━━━━━ 2s 4ms/step - accuracy: 0.9915 - loss: 0.0283\n\nPrecisión después del fine-tuning avanzado: 0.9927999973297119"
  },
  {
    "objectID": "semana_3/notebooks/Nb_3a_Introduccion_transfer_learning_finetuning.html#transfer-learning-y-fine-tuning-en-clasificación-de-texto",
    "href": "semana_3/notebooks/Nb_3a_Introduccion_transfer_learning_finetuning.html#transfer-learning-y-fine-tuning-en-clasificación-de-texto",
    "title": "Introducción a pre-trained models, transfer learning and fine tuning",
    "section": "Transfer learning y fine tuning en Clasificación de texto",
    "text": "Transfer learning y fine tuning en Clasificación de texto\n\nDescargar el dataset\n\n# Obtener desde tensorflow datasets\ndataset, info = tfds.load('imdb_reviews', with_info=True,\n                          as_supervised=True)\ntrain_dataset, test_dataset = dataset['train'], dataset['test']\n\nWARNING:absl:Variant folder /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0 has no dataset_info.json\n\n\nDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\n\n\n\nBUFFER_SIZE = 10000\nBATCH_SIZE = 64\n\n# optimización para train\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\n# optimización para test\ntest_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\n\n\nMétodo manual - Usando embebidos directamente\nEs necesario descargar los correspondientes vectores pre-entrenados para ser usados posteriormente en nuestros proyectos.\nFastText:\nFastText, desarrollado por el equipo de IA de Facebook (FAIR), es un algoritmo popular para aprender embeddings de palabras. Tiene algunas características distintivas:\n\nBasado en Subpalabras (N-gramas de Caracteres): A diferencia de Word2Vec, que trata cada palabra como una unidad atómica, FastText representa cada palabra como una bolsa de n-gramas de caracteres. Por ejemplo, la palabra “manzana” con n-gramas de 3 caracteres (trigramas) se podría representar como &lt;ma, man, anz, nza, zan, ana, na&gt; (donde &lt; y &gt; marcan el inicio y fin de la palabra). El vector de la palabra “manzana” se forma sumando los vectores de sus n-gramas.Esto permite a FastText generar embeddings para palabras que no vio durante el entrenamiento (palabras fuera de vocabulario u OOV), ya que es probable que sus n-gramas sí hayan sido vistos en otras palabras. También maneja mejor palabras raras y lenguajes morfológicamente ricos (donde las palabras cambian mucho su forma, como el español o el alemán).\nCorpus: Provienen de Common Crawl, un corpus masivo que contiene datos rastreados de la web.\nAlgoritmo: Se entrenan utilizando arquitecturas similares a Word2Vec, como CBOW (Continuous Bag-of-Words) o Skip-gram.\n\nCBOW: Intenta predecir la palabra central a partir de las palabras de su contexto.\nSkip-gram: Intenta predecir las palabras del contexto a partir de la palabra central.\n\n\n\nDescargar embeddings Fasttext\n\n# Descargar vectores embebidos en inglés y español\n# Descargar los vectores FastText de inglés y español\n!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.vec.gz\n\n# Descomprimir los archivos\n!gunzip cc.en.300.vec.gz # vectores de dim 300 en inglés\n!gunzip cc.es.300.vec.gz # vectores de dim 300 es español\n\n--2025-05-13 16:28:14--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\nResolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 108.157.254.124, 108.157.254.15, 108.157.254.102, ...\nConnecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|108.157.254.124|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1325960915 (1.2G) [binary/octet-stream]\nSaving to: ‘cc.en.300.vec.gz’\n\ncc.en.300.vec.gz    100%[===================&gt;]   1.23G  23.3MB/s    in 61s     \n\n2025-05-13 16:29:15 (20.8 MB/s) - ‘cc.en.300.vec.gz’ saved [1325960915/1325960915]\n\n--2025-05-13 16:29:16--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.vec.gz\nResolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 108.157.254.15, 108.157.254.124, 108.157.254.121, ...\nConnecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|108.157.254.15|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1285580896 (1.2G) [binary/octet-stream]\nSaving to: ‘cc.es.300.vec.gz’\n\ncc.es.300.vec.gz    100%[===================&gt;]   1.20G  22.9MB/s    in 53s     \n\n2025-05-13 16:30:09 (23.1 MB/s) - ‘cc.es.300.vec.gz’ saved [1285580896/1285580896]\n\n\n\n\nprint(f\"Número de palabras en la matriz de inglés:\")\n!head -n 1 cc.en.300.vec\n\nprint(f\"Número de palabras en la matriz de español:\")\n!head -n 1 cc.es.300.vec\n\nNúmero de palabras en la matriz de inglés:\n2000000 300\nNúmero de palabras en la matriz de español:\n2000000 300\n\n\n\n# Cargar los embeddings preentrenados de FastText (esto puede tardar un poco)\n# limitar a 50.000 palabras más frecuentes\nembedding_en = KeyedVectors.load_word2vec_format('cc.en.300.vec',\n                                                 binary=False,\n                                                 limit=50000)\n\nembedding_es = KeyedVectors.load_word2vec_format('cc.es.300.vec',\n                                                 binary=False,\n                                                 limit=50000)\n\n\n\nRelación semántica de los embeddings\nSemántica en Inglés\n\n# Palabras similares\npalabra_en = \"king\"\nif palabra_en in embedding_en:\n    similares_en = embedding_en.most_similar(palabra_en, topn=5)\n    print(f\"Palabras más similares a '{palabra_en}': {similares_en}\")\n\nPalabras más similares a 'king': [('kings', 0.7550534605979919), ('queen', 0.7069182991981506), ('King', 0.6591336727142334), ('prince', 0.6495301723480225), ('monarch', 0.618451714515686)]\n\n\n\n# Analogías: rey es a hombre lo que reina es a ? (mujer)\n# king - man + woman = queen\nif all(word in embedding_en for word in [\"king\", \"man\", \"woman\"]):\n    analogia_en = embedding_en.most_similar(positive=['king', 'woman'],\n                                            negative=['man'], topn=3)\n    print(f\"'king' - 'man' + 'woman' se parece a: {analogia_en}\")\n\n'king' - 'man' + 'woman' se parece a: [('queen', 0.7554903030395508), ('princess', 0.5755002498626709), ('monarch', 0.5741325616836548)]\n\n\nSemántica en español\n\nif embedding_es:\n  if all(word in embedding_es for word in [\"manzana\", \"pera\"]):\n      distancia = embedding_es.similarity(\"manzana\", \"pera\")\n      # Un valor más cercano a 1 indica más similitud\n      print(f\"Similitud entre 'manzana' y 'pera': {distancia:.4f}\")\n\nSimilitud entre 'manzana' y 'pera': 0.6270\n\n\nLimitaciones\n\nprint(f\"--- Demostrando limitaciones entre lenguajes\")\n\n# Palabras a comparar\npalabra_en = \"water\"\npalabra_es = \"agua\"\n\n# obtener los embeddings\nvector_en = embedding_en.get_vector(palabra_en)\nvector_es = embedding_es.get_vector(palabra_es)\n\n# calcular las similitudes\nsimilitud_cruzada = cosine_similarity(vector_en, vector_es)\nprint(f\"Similitud coseno entre '{palabra_en}' (vector EN) y '{palabra_es}' (vector ES): {similitud_cruzada:.4f}\")\n\n--- Demostrando limitaciones entre lenguajes\nSimilitud coseno entre 'water' (vector EN) y 'agua' (vector ES): -0.0832\n\n\n\n\nCrear Vocabulario y matriz de embeddings\n\nVOCAB_SIZE = 10000\nencoder = tf.keras.layers.TextVectorization(\n    max_tokens=VOCAB_SIZE)\n\n# Crea la capa y pasa el texto del conjunto de datos al método .adapt de la capa\nencoder.adapt(train_dataset.map(lambda text, label: text))\n\n\n# Obtener el vocabulario del encoder\nvocab = encoder.get_vocabulary()\n# Dimensiones de los embeddings preentrenados\nembedding_dim = 300  # Dimensión de los embeddings de FastText\n\n# 1. Crear la matriz de embeddings. Inicializar con ceros\n# ya que esto maneja el token de padding (índice 0) correctamente por defecto.\nembedding_matrix = np.zeros((len(vocab), embedding_dim))\n\n# 2. Llenar la matriz con los vectores de FastText y manejar tokens especiales\n# solo vamos a usar los de inglés\nfor i, word in enumerate(vocab):\n    if word in embedding_en:\n        # La palabra está en FastText, usa su vector\n        embedding_matrix[i] = embedding_en[word]\n    else:\n        # Otras palabras en nuestro vocabulario pero no en FastText\n        embedding_matrix[i] = np.random.uniform(-0.25, 0.25, embedding_dim)\n\n\nembedding_matrix.shape\n\n(10000, 300)\n\n\n\n\nCrear y entrenar modelo\n\nK.clear_session()\ndef rnn(pretrained_vector_matrix):\n    # Ahora utilizamos la API funcional de Keras\n    inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)  # El input será una cadena de texto\n    x = encoder(inputs)  # Aplicamos el encoder\n\n    x = tf.keras.layers.Embedding(\n        input_dim=len(encoder.get_vocabulary()),  # Tamaño del vocabulario\n        output_dim=pretrained_vector_matrix.shape[1],  # Dimensión de los embeddings (300)\n        embeddings_initializer=tf.keras.initializers.Constant(pretrained_vector_matrix),\n        trainable=True,  # Ajustar los vectores a nuestro dataset\n        mask_zero=True)(x)  # Capa de Embedding\n\n    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False),\n                                      merge_mode='concat')(x)  # Capa LSTM Bidireccional\n\n    x = tf.keras.layers.Dense(64, activation='relu')(x)  # Capa densa\n    x = tf.keras.layers.Dropout(0.6)(x)  # Capa de Dropout para regularizar\n\n    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x) # Capa de salida\n\n    # Definimos el modelo\n    model = tf.keras.Model(inputs, outputs)\n\n    return model\n\n# crear la red RNN con LSTM\nmodel = rnn(embedding_matrix)\n\n# Compilamos el modelo con una lr baja para evitar sobre entrenar\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n              optimizer=tf.keras.optimizers.Adam(3e-5),\n              metrics=['accuracy'])\n\n# Verificamos la estructura del modelo\nmodel.summary()\n\nModel: \"functional\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer         │ (None, 1)         │          0 │ -                 │\n│ (InputLayer)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ text_vectorization… │ (None, None)      │          0 │ input_layer[0][0] │\n│ (TextVectorization) │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding           │ (None, None, 300) │  3,000,000 │ text_vectorizati… │\n│ (Embedding)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ not_equal           │ (None, None)      │          0 │ text_vectorizati… │\n│ (NotEqual)          │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bidirectional       │ (None, 128)       │    186,880 │ embedding[0][0],  │\n│ (Bidirectional)     │                   │            │ not_equal[0][0]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense (Dense)       │ (None, 64)        │      8,256 │ bidirectional[0]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout (Dropout)   │ (None, 64)        │          0 │ dense[0][0]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_1 (Dense)     │ (None, 1)         │         65 │ dropout[0][0]     │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n\n\n\n Total params: 3,195,201 (12.19 MB)\n\n\n\n Trainable params: 3,195,201 (12.19 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# hacer una prueba sin usar padding\n# El texto crudo que quieres predecir\nsample_text = ('The movie was cool. The animation and the graphics were out of this world.')\n\n# No es necesario hacer la vectorización manual aquí, simplemente pasa el texto crudo al modelo\npredictions = model.predict(tf.constant([sample_text]))\n\n# Imprime la predicción\nprint(predictions[0])\n\n\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 367ms/step\n\n[0.53357685]\n\n\n\n\n\n# Entrenar con early stopping para evitar sobre entrenamiento\nearly_stopping_callback = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',  # Métrica a monitorear\n    patience=3,          # Número de épocas sin mejora después de las cuales se detendrá el entrenamiento\n    verbose=1,           # Imprime un mensaje cuando el entrenamiento se detiene\n    restore_best_weights=True # Restaura los pesos del modelo de la época con el mejor valor de la métrica monitoreada\n)\n\n# Entrenar\nhistory = model.fit(train_dataset, epochs=10,\n                    validation_data=test_dataset,\n                    validation_steps=30,\n                    callbacks=[early_stopping_callback])\n\n\nEpoch 1/10\n\n391/391 ━━━━━━━━━━━━━━━━━━━━ 31s 72ms/step - accuracy: 0.5101 - loss: 0.6927 - val_accuracy: 0.6089 - val_loss: 0.6839\n\nEpoch 2/10\n\n391/391 ━━━━━━━━━━━━━━━━━━━━ 28s 71ms/step - accuracy: 0.5805 - loss: 0.6813 - val_accuracy: 0.6719 - val_loss: 0.6526\n\nEpoch 3/10\n\n391/391 ━━━━━━━━━━━━━━━━━━━━ 28s 72ms/step - accuracy: 0.7269 - loss: 0.5806 - val_accuracy: 0.8219 - val_loss: 0.4358\n\nEpoch 4/10\n\n391/391 ━━━━━━━━━━━━━━━━━━━━ 27s 70ms/step - accuracy: 0.8348 - loss: 0.4090 - val_accuracy: 0.8484 - val_loss: 0.3730\n\nEpoch 5/10\n\n391/391 ━━━━━━━━━━━━━━━━━━━━ 28s 71ms/step - accuracy: 0.8681 - loss: 0.3452 - val_accuracy: 0.8620 - val_loss: 0.3445\n\nEpoch 6/10\n\n391/391 ━━━━━━━━━━━━━━━━━━━━ 28s 71ms/step - accuracy: 0.8893 - loss: 0.3033 - val_accuracy: 0.8687 - val_loss: 0.3326\n\nEpoch 7/10\n\n391/391 ━━━━━━━━━━━━━━━━━━━━ 27s 70ms/step - accuracy: 0.8995 - loss: 0.2783 - val_accuracy: 0.8724 - val_loss: 0.3172\n\nEpoch 8/10\n\n391/391 ━━━━━━━━━━━━━━━━━━━━ 28s 71ms/step - accuracy: 0.9145 - loss: 0.2510 - val_accuracy: 0.8797 - val_loss: 0.3110\n\nEpoch 9/10\n\n391/391 ━━━━━━━━━━━━━━━━━━━━ 28s 71ms/step - accuracy: 0.9228 - loss: 0.2317 - val_accuracy: 0.8781 - val_loss: 0.3139\n\nEpoch 10/10\n\n391/391 ━━━━━━━━━━━━━━━━━━━━ 27s 70ms/step - accuracy: 0.9324 - loss: 0.2044 - val_accuracy: 0.8797 - val_loss: 0.3228\n\nRestoring model weights from the end of the best epoch: 8.\n\n\n\n\n\ntest_loss, test_acc = model.evaluate(test_dataset)\n\nprint('Test Loss:', test_loss)\nprint('Test Accuracy:', test_acc)\n\n\n391/391 ━━━━━━━━━━━━━━━━━━━━ 13s 32ms/step - accuracy: 0.8787 - loss: 0.3016\n\nTest Loss: 0.30069151520729065\n\nTest Accuracy: 0.8774799704551697\n\n\n\n\n\n# Visualizar acc y loss\nplot_curvas(history)\n\n\n\n\n\n\n\n\n\n\n\nMétodo Modular - Usando arquitecturas\nRecientemente, ha crecido la creación de nuevas herramientas que facilitan el uso de modelos pre-entrenados en diferentes tareas de Deep Learning. A continuación, introduccimos KerasHub una librería que ha sido propuesta para hacer uso de modelos pre-entrenado usando Keras. Esta pensanda para ser modular y esta enfocada para tareas de Clasificación de imágenes, Clasificación de texto y Tareas más especializadas sobre generación de secuencias como Audio o Texto.\n¿Qué es KerasHub?\nKerasHub es una biblioteca que proporciona modelos preentrenados para diversas tareas de aprendizaje automático. Estos modelos están implementados como capas (keras.Layer) y modelos (keras.Model) de Keras, lo que permite una integración sencilla en proyectos existentes. La biblioteca incluye modelos para clasificación de imágenes, generación de texto, clasificación de texto y más, todos accesibles a través de una API.\nKerasHub organiza los modelos en componentes modulares:\nTask: Clase de alto nivel que encapsula el modelo y el preprocesamiento para una tarea específica (por ejemplo, ImageClassifier, TextClassifier).\nBackbone: Modelo base que extrae características de los datos de entrada.\nPreprocessor: Capa que realiza el preprocesamiento necesario en los datos de entrada (por ejemplo, redimensionamiento de imágenes, tokenización de texto).\nTokenizer: Convierte texto en secuencias de tokens.\nImageConverter: Redimensiona y normaliza imágenes. keras.io\nCada uno de estos componentes puede cargarse desde un preset utilizando from_preset().\n\n\n\nEstructura KerasHub\n\n\nA continuación vamos a importar la arquitectura completa basada en BERT para clasificar nuestro ejercicio anterior.\nArquitectura del modelo: la estructura del modelo BERT.\nPesos preentrenados: valores aprendidos durante el entrenamiento en grandes conjuntos de datos.\nPreprocesamiento: procesos necesarios para preparar los datos de entrada, como tokenización y normalización.\nKerasHub descargará automáticamente el modelo pre-entrenado desde Hugging Face Hub.\n\nUsando BERT en inferencia\nBERT (Bidirectional Encoder Representations from Transformers) es un modelo de representación del lenguaje que revolucionó el procesamiento de lenguaje natural (NLP) debido a tres aportes clave:\n\nRepresentaciones Bidireccionales Profundas: A diferencia de modelos anteriores como GPT (unidireccional) o ELMo, BERT es completamente bidireccional, permitiendo que el modelo entienda mejor el contexto de cada palabra teniendo en cuenta tanto la izquierda como la derecha.\nPre-entrenamiento Universal y Fine-tuning Eficiente: BERT puede preentrenarse en texto no etiquetado y luego afinarse con una capa de salida adicional para tareas específicas como preguntas y respuestas (QA), inferencia, y reconocimiento de entidades nombradas (NER), sin necesidad de arquitecturas especializadas para cada tarea.\n\n\n\n\nBERT.png\n\n\nEntrenamiento en 2 fases:\n\nPre-entrenamiento (ver parte izquierda de la imagen):\n\n\nMasked Language Model (MLM): Se ocultan aleatoriamente el 15% de los tokens para que el modelo los prediga, permitiendo el aprendizaje bidireccional.\nNext Sentence Prediction (NSP): El modelo predice si una oración B sigue a una oración A. Esto lo entrena para tareas que implican relaciones entre frases.\n\n\nFine-tuning (ver parte derecha de la imagen):\n\n\nSe adapta el modelo preentrenado a tareas específicas añadiendo una capa de salida.\nTodos los parámetros se afinan con el conjunto de datos etiquetado de la tarea específica (e.g., QA, NER, clasificación).\n\n\nclassifier_BERT = keras_hub.models.BertTextClassifier.from_preset(\n    \"bert_base_en_uncased\",\n    #activation='softmax', # El modelo original no la incluye\n    num_classes=2,\n)\n\nDownloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_base_en_uncased/2/download/config.json...\n\n\n100%|██████████| 510/510 [00:00&lt;00:00, 1.29MB/s]\n\n\n# Si se añade la función de activación softmax, las predicciones tienen la prob\n# Pero no parecen estar muy convincentes, parecen estar muy neutras.\n\nclassifier_BERT.predict(test_dataset)\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 873s 1s/step\narray([[0.6197734 , 0.3802266 ],\n       [0.57702965, 0.42297035],\n       [0.46357015, 0.5364299 ],\n       ...,\n       [0.602075  , 0.39792505],\n       [0.6158818 , 0.38411826],\n       [0.62885654, 0.37114346]], dtype=float32)\n\n# Evaluar para determinar el rendimiento inicial\nclassifier_BERT.evaluate(test_dataset)\n\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 865s 1s/step - loss: 0.9045 - sparse_categorical_accuracy: 0.4979\n\n\n\n\n[0.9011728167533875, 0.49983999133110046]\n\n\nEfectivamente el rendimiento no es el adecuado. Quiere decir que es génerico el resultado.\n\n\nY si hacemos un poco de Fine-tuning?\nSi queremos hacer fine-tuning sobre más capas:\nfor layer in classifier_BERT.backbone.layers[:-4]:\n    layer.trainable = False\nPero solo lo haremos sobre la final:\n\n# Congelamos todo el backbone y solo entrenamos la capa final\nclassifier_BERT.backbone.trainable = False\n\n\n# Re compilar con una nueva tasa de aprendizaje más pequeña\nclassifier_BERT.compile(\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=tf.keras.optimizers.Adam(3e-4),\n)\n\n\n# Entrenar con early stopping para evitar sobre entrenamiento\nearly_stopping_callback = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',  # Métrica a monitorear\n    patience=3,          # Número de épocas sin mejora después de las cuales se detendrá el entrenamiento\n    verbose=1,           # Imprime un mensaje cuando el entrenamiento se detiene\n    restore_best_weights=True # Restaura los pesos del modelo de la época con el mejor valor de la métrica monitoreada\n)\n\n# Ajustamos el clasificador con pocas epocas\nclassifier_BERT.fit(train_dataset, validation_data=test_dataset,\n               epochs=15,\n               callbacks=[early_stopping_callback])\n\n\nEpoch 1/15\n\n391/391 ━━━━━━━━━━━━━━━━━━━━ 591s 1s/step - loss: 0.6668 - sparse_categorical_accuracy: 0.5892 - val_loss: 0.6053 - val_sparse_categorical_accuracy: 0.6562\n\nEpoch 2/15\n\n391/391 ━━━━━━━━━━━━━━━━━━━━ 522s 1s/step - loss: 0.5861 - sparse_categorical_accuracy: 0.7168 - val_loss: 0.5421 - val_sparse_categorical_accuracy: 0.7560\n\nEpoch 3/15\n\n391/391 ━━━━━━━━━━━━━━━━━━━━ 522s 1s/step - loss: 0.5484 - sparse_categorical_accuracy: 0.7403 - val_loss: 0.5594 - val_sparse_categorical_accuracy: 0.6960\n\nEpoch 4/15\n\n391/391 ━━━━━━━━━━━━━━━━━━━━ 522s 1s/step - loss: 0.5295 - sparse_categorical_accuracy: 0.7512 - val_loss: 0.5033 - val_sparse_categorical_accuracy: 0.7643\n\nEpoch 5/15\n\n391/391 ━━━━━━━━━━━━━━━━━━━━ 522s 1s/step - loss: 0.5126 - sparse_categorical_accuracy: 0.7631 - val_loss: 0.4770 - val_sparse_categorical_accuracy: 0.7936\n\nEpoch 6/15\n\n391/391 ━━━━━━━━━━━━━━━━━━━━ 522s 1s/step - loss: 0.5102 - sparse_categorical_accuracy: 0.7595 - val_loss: 0.4724 - val_sparse_categorical_accuracy: 0.7898\n\nEpoch 7/15\n\n391/391 ━━━━━━━━━━━━━━━━━━━━ 522s 1s/step - loss: 0.4965 - sparse_categorical_accuracy: 0.7730 - val_loss: 0.4728 - val_sparse_categorical_accuracy: 0.7831\n\nEpoch 8/15\n\n391/391 ━━━━━━━━━━━━━━━━━━━━ 522s 1s/step - loss: 0.4888 - sparse_categorical_accuracy: 0.7708 - val_loss: 0.4520 - val_sparse_categorical_accuracy: 0.8011\n\nEpoch 9/15\n\n391/391 ━━━━━━━━━━━━━━━━━━━━ 522s 1s/step - loss: 0.4899 - sparse_categorical_accuracy: 0.7735 - val_loss: 0.4458 - val_sparse_categorical_accuracy: 0.8042\n\nEpoch 10/15\n\n391/391 ━━━━━━━━━━━━━━━━━━━━ 522s 1s/step - loss: 0.4772 - sparse_categorical_accuracy: 0.7774 - val_loss: 0.4375 - val_sparse_categorical_accuracy: 0.8110\n\nEpoch 11/15\n\n391/391 ━━━━━━━━━━━━━━━━━━━━ 522s 1s/step - loss: 0.4774 - sparse_categorical_accuracy: 0.7795 - val_loss: 0.4443 - val_sparse_categorical_accuracy: 0.8003\n\nEpoch 12/15\n\n391/391 ━━━━━━━━━━━━━━━━━━━━ 522s 1s/step - loss: 0.4755 - sparse_categorical_accuracy: 0.7779 - val_loss: 0.4296 - val_sparse_categorical_accuracy: 0.8130\n\nEpoch 13/15\n\n391/391 ━━━━━━━━━━━━━━━━━━━━ 522s 1s/step - loss: 0.4725 - sparse_categorical_accuracy: 0.7803 - val_loss: 0.4310 - val_sparse_categorical_accuracy: 0.8094\n\nEpoch 14/15\n\n391/391 ━━━━━━━━━━━━━━━━━━━━ 522s 1s/step - loss: 0.4725 - sparse_categorical_accuracy: 0.7784 - val_loss: 0.4270 - val_sparse_categorical_accuracy: 0.8111\n\nEpoch 15/15\n\n391/391 ━━━━━━━━━━━━━━━━━━━━ 522s 1s/step - loss: 0.4698 - sparse_categorical_accuracy: 0.7800 - val_loss: 0.4229 - val_sparse_categorical_accuracy: 0.8147\n\nRestoring model weights from the end of the best epoch: 15.\n\n\n\n\n&lt;keras.src.callbacks.history.History at 0x78998cceaa90&gt;\n\n\n\n# Finalmente evaluamos el rendimiento después de hacer fine tuning\nclassifier_BERT.evaluate(test_dataset)\n\n\n391/391 ━━━━━━━━━━━━━━━━━━━━ 235s 600ms/step - loss: 0.4238 - sparse_categorical_accuracy: 0.8131\n\n\n\n\n[0.4228881299495697, 0.8147199749946594]\n\n\n\n\nUsando tú propio BERT\nTendrás que hacer los siguientes pasos:\n\nConfigurar el tokenizador con el vocabulario que hayas definido (como lo hicimos en el ejemplo base.\nCrear un preprocesador\nDefinir el backbone, con las dimensiones deseadas (ya sabes de que hablamos si conoces Transformers),\nConstruir el clasificador y entrenar en tus datos)\n\ntokenizer = keras_hub.models.BertTokenizer(\n    vocabulary=vocab,\n\npreprocessor = keras_hub.models.BertTextClassifierPreprocessor(\n    tokenizer=tokenizer,\n    sequence_length=128,\n)\nbackbone = keras_hub.models.BertBackbone(\n    vocabulary_size=30552,\n    num_layers=4,\n    num_heads=4,\n    hidden_dim=256,\n    intermediate_dim=512,\n    max_sequence_length=128,\n)\nclassifier = keras_hub.models.BertTextClassifier(\n    backbone=backbone,\n    preprocessor=preprocessor,\n    num_classes=4,\n)\nclassifier.fit(x=features, y=labels, batch_size=2)\nPara más información consultar: Getting Started con Keras Hub\nAdemás puede consultar todos los modelos pre-entrenados en Keras presets y las arquitecturas disponibles Keras architectures\nPara más sobre BERT."
  },
  {
    "objectID": "semana_3/notebooks/Nb_3a_Introduccion_transfer_learning_finetuning.html#pre-training-en-datos-tabulares-usando-tabnet",
    "href": "semana_3/notebooks/Nb_3a_Introduccion_transfer_learning_finetuning.html#pre-training-en-datos-tabulares-usando-tabnet",
    "title": "Introducción a pre-trained models, transfer learning and fine tuning",
    "section": "Pre-training en Datos tabulares usando TabNet",
    "text": "Pre-training en Datos tabulares usando TabNet\n\ndata = fetch_covtype(as_frame=True)\nX = data.data.astype(np.float32)\ny = data.target.astype(int) - 1  # etiquetas 0‑6 para las 7 clases\n\nX\n\n\n    \n\n\n\n\n\n\nElevation\nAspect\nSlope\nHorizontal_Distance_To_Hydrology\nVertical_Distance_To_Hydrology\nHorizontal_Distance_To_Roadways\nHillshade_9am\nHillshade_Noon\nHillshade_3pm\nHorizontal_Distance_To_Fire_Points\n...\nSoil_Type_30\nSoil_Type_31\nSoil_Type_32\nSoil_Type_33\nSoil_Type_34\nSoil_Type_35\nSoil_Type_36\nSoil_Type_37\nSoil_Type_38\nSoil_Type_39\n\n\n\n\n0\n2596.0\n51.0\n3.0\n258.0\n0.0\n510.0\n221.0\n232.0\n148.0\n6279.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n2590.0\n56.0\n2.0\n212.0\n-6.0\n390.0\n220.0\n235.0\n151.0\n6225.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n2804.0\n139.0\n9.0\n268.0\n65.0\n3180.0\n234.0\n238.0\n135.0\n6121.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n2785.0\n155.0\n18.0\n242.0\n118.0\n3090.0\n238.0\n238.0\n122.0\n6211.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n2595.0\n45.0\n2.0\n153.0\n-1.0\n391.0\n220.0\n234.0\n150.0\n6172.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n581007\n2396.0\n153.0\n20.0\n85.0\n17.0\n108.0\n240.0\n237.0\n118.0\n837.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n581008\n2391.0\n152.0\n19.0\n67.0\n12.0\n95.0\n240.0\n237.0\n119.0\n845.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n581009\n2386.0\n159.0\n17.0\n60.0\n7.0\n90.0\n236.0\n241.0\n130.0\n854.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n581010\n2384.0\n170.0\n15.0\n60.0\n5.0\n90.0\n230.0\n245.0\n143.0\n864.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n581011\n2383.0\n165.0\n13.0\n60.0\n4.0\n67.0\n231.0\n244.0\n141.0\n875.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n581012 rows × 54 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\n\nclf = TabNetClassifier(\n    optimizer_fn=torch.optim.Adam,\n    optimizer_params=dict(lr=2e-2),\n    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n    scheduler_params={\"step_size\":10, \"gamma\":0.9},\n    mask_type='entmax', #sparsemax\n    device_name='cuda'\n)\n\n/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n  warnings.warn(f\"Device used : {self.device}\")\n\n\n\nclf.fit(\n    X_train=X_train.values, y_train=y_train.values,\n    eval_set=[(X_valid.values, y_valid.values)],\n    eval_name=['valid'],\n    eval_metric=['accuracy'],\n    max_epochs=50,\n    num_workers=2\n)\n\nepoch 0  | loss: 0.72523 | valid_accuracy: 0.7489  |  0:00:20s\nepoch 1  | loss: 0.55862 | valid_accuracy: 0.78073 |  0:00:40s\nepoch 2  | loss: 0.51353 | valid_accuracy: 0.79955 |  0:01:01s\nepoch 3  | loss: 0.48882 | valid_accuracy: 0.81313 |  0:01:21s\nepoch 4  | loss: 0.46969 | valid_accuracy: 0.82237 |  0:01:41s\nepoch 5  | loss: 0.45493 | valid_accuracy: 0.82435 |  0:02:01s\nepoch 6  | loss: 0.44548 | valid_accuracy: 0.82876 |  0:02:22s\nepoch 7  | loss: 0.43394 | valid_accuracy: 0.83884 |  0:02:42s\nepoch 8  | loss: 0.42682 | valid_accuracy: 0.84025 |  0:03:02s\nepoch 9  | loss: 0.42305 | valid_accuracy: 0.84224 |  0:03:23s\nepoch 10 | loss: 0.41117 | valid_accuracy: 0.84793 |  0:03:43s\nepoch 11 | loss: 0.40697 | valid_accuracy: 0.8532  |  0:04:04s\nepoch 12 | loss: 0.40315 | valid_accuracy: 0.85364 |  0:04:24s\nepoch 13 | loss: 0.40081 | valid_accuracy: 0.8563  |  0:04:44s\nepoch 14 | loss: 0.39851 | valid_accuracy: 0.84837 |  0:05:04s\nepoch 15 | loss: 0.39728 | valid_accuracy: 0.85664 |  0:05:25s\nepoch 16 | loss: 0.39151 | valid_accuracy: 0.85597 |  0:05:45s\nepoch 17 | loss: 0.38807 | valid_accuracy: 0.86308 |  0:06:06s\nepoch 18 | loss: 0.38705 | valid_accuracy: 0.85793 |  0:06:26s\nepoch 19 | loss: 0.38338 | valid_accuracy: 0.85365 |  0:06:47s\nepoch 20 | loss: 0.38018 | valid_accuracy: 0.86545 |  0:07:07s\nepoch 21 | loss: 0.37847 | valid_accuracy: 0.86482 |  0:07:28s\nepoch 22 | loss: 0.37902 | valid_accuracy: 0.86319 |  0:07:48s\nepoch 23 | loss: 0.3751  | valid_accuracy: 0.87118 |  0:08:08s\nepoch 24 | loss: 0.37328 | valid_accuracy: 0.86573 |  0:08:29s\nepoch 25 | loss: 0.3702  | valid_accuracy: 0.86972 |  0:08:49s\nepoch 26 | loss: 0.37631 | valid_accuracy: 0.86124 |  0:09:10s\nepoch 27 | loss: 0.37374 | valid_accuracy: 0.87088 |  0:09:31s\nepoch 28 | loss: 0.37042 | valid_accuracy: 0.86521 |  0:09:51s\nepoch 29 | loss: 0.37133 | valid_accuracy: 0.86584 |  0:10:12s\nepoch 30 | loss: 0.36739 | valid_accuracy: 0.87149 |  0:10:33s\nepoch 31 | loss: 0.36381 | valid_accuracy: 0.86824 |  0:10:54s\nepoch 32 | loss: 0.37192 | valid_accuracy: 0.871   |  0:11:14s\nepoch 33 | loss: 0.36089 | valid_accuracy: 0.87442 |  0:11:35s\nepoch 34 | loss: 0.36402 | valid_accuracy: 0.87417 |  0:11:56s\nepoch 35 | loss: 0.36055 | valid_accuracy: 0.87339 |  0:12:16s\n\n\n\nParámetros de TabNet (Instanciar)\n\n\n\n\n\n\n\nParámetro\nDescripción\n\n\n\n\nn_d / n_a\nDimensión de la capa de decisión y atención. Usualmente entre 8 y 64. Recomendado: n_d = n_a.\n\n\nn_steps\nNúmero de pasos (bloques de atención). Sugerido: 3–10.\n\n\ngamma\nCoeficiente de reuso de características en las máscaras. Rango: 1.0–2.0 (default 1.3).\n\n\ncat_idxs\nÍndices de las variables categóricas (requerido para embeddings).\n\n\ncat_dims\nNúmero de categorías únicas por variable categórica.\n\n\ncat_emb_dim\nDimensiones de embedding por variable categórica. Default: 1.\n\n\nn_shared / n_independent\nCapas GLU compartidas e independientes por paso. Usual: 1–5.\n\n\nepsilon\nEstabilidad numérica. No modificar (default 1e-15).\n\n\nseed\nSemilla para reproducibilidad.\n\n\nmomentum\nMomentum para batch norm. Rango típico: 0.01–0.4. Default: 0.02.\n\n\nclip_value\nValor para gradient clipping. Default: None.\n\n\nlambda_sparse\nCoeficiente para pérdida de dispersión. Mayor → más enmascaramiento. Default: 1e-3.\n\n\noptimizer_fn\nOptimizador de PyTorch. Default: torch.optim.Adam.\n\n\noptimizer_params\nParámetros del optimizador. Default: {'lr': 2e-2}.\n\n\nscheduler_fn / scheduler_params\nScheduler de tasa de aprendizaje y sus parámetros. Ej: {\"gamma\": 0.95, \"step_size\": 10}.\n\n\nmodel_name\nNombre del modelo para guardar. Default: \"DreamQuarkTabNet\".\n\n\nverbose\nNivel de verbosidad. 0 (silencio) o 1 (mostrar épocas).\n\n\ndevice_name\n\"cpu\", \"cuda\" o \"auto\".\n\n\nmask_type\nTipo de máscara: \"sparsemax\" o \"entmax\".\n\n\ngrouped_features\nAgrupación de variables correlacionadas (ej. PCA o TF-IDF).\n\n\nn_shared_decoder / n_indep_decoder\nSolo para preentrenamiento. Bloques GLU compartidos/independientes en decoder.\n\n\n\n\n\nParámetros de entrenamiento (fit)\n\n\n\n\n\n\n\nParámetro\nDescripción\n\n\n\n\nX_train, y_train\nDatos y etiquetas de entrenamiento.\n\n\neval_set\nLista de tuplas (X, y) para evaluación. La última se usa para early stopping.\n\n\neval_name\nNombres para los conjuntos de evaluación.\n\n\neval_metric\nLista de métricas (ej. \"accuracy\", \"auc\", \"rmse\"). La última se usa para early stopping.\n\n\nmax_epochs\nNúmero máximo de épocas. Default: 200.\n\n\npatience\nÉpocas sin mejora antes de detener. Default: 10.\n\n\nweights\nSolo para clasificación. 0: sin ponderación, 1: ponderación automática, dict: pesos por clase.\n\n\nloss_fn\nFunción de pérdida. Default: mse (regresión) o cross_entropy (clasificación).\n\n\nbatch_size\nTamaño del batch. Sugerido: grande (ej. 1024).\n\n\nvirtual_batch_size\nPara Ghost Batch Norm. Debe dividir batch_size. Default: 128.\n\n\nnum_workers\nNúmero de workers para DataLoader.\n\n\ndrop_last\nSi se debe descartar el último batch incompleto.\n\n\ncallbacks\nLista de callbacks personalizados.\n\n\npretraining_ratio\nSolo TabNetPretrainer: proporción de características a enmascarar (entre 0 y 1).\n\n\nwarm_start\nPermite continuar entrenamiento previo.\n\n\ncompute_importance\nSi se deben calcular importancias de variables. Default: True."
  },
  {
    "objectID": "semana_3/notebooks/Nb_3a_Introduccion_transfer_learning_finetuning.html#conclusiones",
    "href": "semana_3/notebooks/Nb_3a_Introduccion_transfer_learning_finetuning.html#conclusiones",
    "title": "Introducción a pre-trained models, transfer learning and fine tuning",
    "section": "Conclusiones",
    "text": "Conclusiones\n\nNote que al agregarle al modelo mas capas superiores, logramos mitigar el problema que presentamos en nuestro modelo 2, logrando un accuracy del 99.30% en nuestro datos de prueba.\nComo conlusión, cuando hagamos uso de modelo pre-entrenados. Tenemos que hacer uso de todas las herramientas que disponemos, como lo son el transfer learning y el fine tunning, una caracteristica muy importante que siempre hay que aplicar.\nEn el modelo de clasificación de texto usando el método manual, logró una mejora significativa usando los embeddings de FastText y ajustandolo a los datos. Sin embargo, tiende a sobre entrenar (algo con lo que se debe luchar aplicando técnicas de regularización). Por otro lado, vemos que con el método a nivel de arquitectura (que es como funcionan los modelos hoy en día) ya es mucho más directo pero el nivel de computación requerido es mayor y funcionan mejor con una mayor cantidad de datos."
  }
]