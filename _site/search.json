[
  {
    "objectID": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html",
    "href": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html",
    "title": "Introducción a la explicabilidad e interpretabilidad en modelos",
    "section": "",
    "text": "Open In Colab\nEn este notebook encontrarás material introductorio para entender los conceptos de expicabilidad e interpretabilidad en modelos de inteligencia artificial.\nAbordaremos el siguiente paso a paso:\n#importamos las librerias necesarias a utilizar\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\n\n%matplotlib inline"
  },
  {
    "objectID": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html#dataset",
    "href": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html#dataset",
    "title": "Introducción a la explicabilidad e interpretabilidad en modelos",
    "section": "Dataset",
    "text": "Dataset\nEl dataset cuenta con 178 registros, cada uno con 13 caracteristicas:\n\nAlcohol\nMalic Acid\nAsh\nAlcalinity of Ash\nMagnesium\nTotal Phenols\nFlavanoids\nNonflavanoid Phenols\nProanthocyanins\nColour Intensity\nHue\nOD280/OD315 of diluted wines\nProline\n\nEl dataset contiene 3 clases diferentes.\n\n#Cargamos el conjunto de datos y procesamos\nwine = load_wine()\nX, y = wine.data, wine.target\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nencoder = OneHotEncoder(sparse_output=False)\ny = encoder.fit_transform(y.reshape(-1, 1))\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nprint(\"Dimension datos de entrenamiento: \", X_train.shape)\nprint(\"Dimension datos de prueba: \", X_test.shape)\n\nDimension datos de entrenamiento:  (124, 13)\nDimension datos de prueba:  (54, 13)\n\n\n\n#Definimos la red neuronal a entrenar y compilamos el modelo\nmodel = Sequential()\n\nmodel.add(Dense(64, input_shape=(X_train.shape[1],), activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(y_train.shape[1], activation='softmax'))\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n\n#Entrenamos la red\nhistory = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n\n\nEpoch 1/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 5s 457ms/step - accuracy: 0.4024 - loss: 1.0690 - val_accuracy: 0.4800 - val_loss: 0.9569\n\nEpoch 2/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 1s 10ms/step - accuracy: 0.5859 - loss: 0.9022 - val_accuracy: 0.7200 - val_loss: 0.8264\n\nEpoch 3/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.5531 - loss: 0.9294 - val_accuracy: 0.6800 - val_loss: 0.7181\n\nEpoch 4/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.6510 - loss: 0.7846 - val_accuracy: 0.8000 - val_loss: 0.6285\n\nEpoch 5/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.7954 - loss: 0.6378 - val_accuracy: 0.8400 - val_loss: 0.5544\n\nEpoch 6/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.8550 - loss: 0.5862 - val_accuracy: 0.8800 - val_loss: 0.4913\n\nEpoch 7/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.8047 - loss: 0.5755 - val_accuracy: 0.9200 - val_loss: 0.4398\n\nEpoch 8/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9135 - loss: 0.4490 - val_accuracy: 0.9200 - val_loss: 0.3962\n\nEpoch 9/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.8631 - loss: 0.4508 - val_accuracy: 0.9200 - val_loss: 0.3582\n\nEpoch 10/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9351 - loss: 0.3836 - val_accuracy: 0.9200 - val_loss: 0.3259\n\nEpoch 11/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9187 - loss: 0.3678 - val_accuracy: 0.9600 - val_loss: 0.2962\n\nEpoch 12/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9166 - loss: 0.3108 - val_accuracy: 0.9600 - val_loss: 0.2697\n\nEpoch 13/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9599 - loss: 0.2927 - val_accuracy: 0.9600 - val_loss: 0.2453\n\nEpoch 14/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9816 - loss: 0.2769 - val_accuracy: 0.9600 - val_loss: 0.2245\n\nEpoch 15/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.9754 - loss: 0.2512 - val_accuracy: 0.9600 - val_loss: 0.2060\n\nEpoch 16/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9598 - loss: 0.2288 - val_accuracy: 0.9600 - val_loss: 0.1885\n\nEpoch 17/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9661 - loss: 0.2238 - val_accuracy: 0.9600 - val_loss: 0.1739\n\nEpoch 18/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - accuracy: 0.9846 - loss: 0.1901 - val_accuracy: 0.9600 - val_loss: 0.1605\n\nEpoch 19/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - accuracy: 0.9228 - loss: 0.2142 - val_accuracy: 0.9600 - val_loss: 0.1489\n\nEpoch 20/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9784 - loss: 0.1613 - val_accuracy: 0.9600 - val_loss: 0.1389\n\nEpoch 21/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.9475 - loss: 0.1762 - val_accuracy: 0.9600 - val_loss: 0.1297\n\nEpoch 22/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9753 - loss: 0.1485 - val_accuracy: 0.9600 - val_loss: 0.1224\n\nEpoch 23/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9629 - loss: 0.1458 - val_accuracy: 0.9600 - val_loss: 0.1158\n\nEpoch 24/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9845 - loss: 0.1045 - val_accuracy: 0.9600 - val_loss: 0.1095\n\nEpoch 25/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9815 - loss: 0.1060 - val_accuracy: 0.9600 - val_loss: 0.1036\n\nEpoch 26/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9753 - loss: 0.1269 - val_accuracy: 0.9600 - val_loss: 0.0981\n\nEpoch 27/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9506 - loss: 0.1140 - val_accuracy: 1.0000 - val_loss: 0.0927\n\nEpoch 28/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.9690 - loss: 0.1359 - val_accuracy: 1.0000 - val_loss: 0.0889\n\nEpoch 29/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9784 - loss: 0.1052 - val_accuracy: 1.0000 - val_loss: 0.0858\n\nEpoch 30/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9845 - loss: 0.0811 - val_accuracy: 0.9600 - val_loss: 0.0830\n\nEpoch 31/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9845 - loss: 0.0730 - val_accuracy: 0.9600 - val_loss: 0.0809\n\nEpoch 32/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 14ms/step - accuracy: 0.9845 - loss: 0.0843 - val_accuracy: 0.9600 - val_loss: 0.0789\n\nEpoch 33/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step - accuracy: 0.9939 - loss: 0.0606 - val_accuracy: 0.9600 - val_loss: 0.0763\n\nEpoch 34/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 1.0000 - loss: 0.0582 - val_accuracy: 0.9600 - val_loss: 0.0733\n\nEpoch 35/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9845 - loss: 0.0953 - val_accuracy: 0.9600 - val_loss: 0.0708\n\nEpoch 36/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9690 - loss: 0.0910 - val_accuracy: 0.9600 - val_loss: 0.0694\n\nEpoch 37/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.9960 - loss: 0.0598 - val_accuracy: 0.9600 - val_loss: 0.0684\n\nEpoch 38/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9939 - loss: 0.0693 - val_accuracy: 0.9600 - val_loss: 0.0696\n\nEpoch 39/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.9939 - loss: 0.0582 - val_accuracy: 0.9600 - val_loss: 0.0703\n\nEpoch 40/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9815 - loss: 0.0470 - val_accuracy: 0.9600 - val_loss: 0.0717\n\nEpoch 41/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9753 - loss: 0.0844 - val_accuracy: 1.0000 - val_loss: 0.0715\n\nEpoch 42/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 1.0000 - loss: 0.0556 - val_accuracy: 1.0000 - val_loss: 0.0696\n\nEpoch 43/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9939 - loss: 0.0501 - val_accuracy: 1.0000 - val_loss: 0.0650\n\nEpoch 44/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 1.0000 - loss: 0.0542 - val_accuracy: 1.0000 - val_loss: 0.0614\n\nEpoch 45/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 1.0000 - loss: 0.0597 - val_accuracy: 1.0000 - val_loss: 0.0559\n\nEpoch 46/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9753 - loss: 0.0617 - val_accuracy: 1.0000 - val_loss: 0.0512\n\nEpoch 47/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9845 - loss: 0.0432 - val_accuracy: 1.0000 - val_loss: 0.0481\n\nEpoch 48/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 1.0000 - loss: 0.0344 - val_accuracy: 1.0000 - val_loss: 0.0460\n\nEpoch 49/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 1.0000 - loss: 0.0337 - val_accuracy: 1.0000 - val_loss: 0.0444\n\nEpoch 50/50\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 1.0000 - loss: 0.0323 - val_accuracy: 1.0000 - val_loss: 0.0441\n\n\n\n\n\ny_pred = model.predict(X_test)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_test_classes = np.argmax(y_test, axis=1)\n\naccuracy = accuracy_score(y_test_classes, y_pred_classes)\nprint(f\"Accuracy en el conjunto de prueba: {accuracy:.4f}\")\n\n\n2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step \n\nAccuracy en el conjunto de prueba: 1.0000\n\n\n\n\n\nplt.figure(figsize=(6, 3))\nplt.plot(history.history['accuracy'], label='accuray en entrenamiento')\nplt.plot(history.history['val_accuracy'], label='accuracy de validación')\nplt.xlabel('épocas')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('Rendimiento del modelo durante el entrenamiento')\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nAhora intentemos explicar/interpretar la red. Que caracteristicas son las que están aportando mas valor a la predicciones. Así, a primera vista, podemos ver que la red neural tiene un rendimiento perfecto, pero no sabemos que es lo que hace. Vamos a convertir esa caja negra el algo mas explicable.\n\n\nwine.feature_names\n\n['alcohol',\n 'malic_acid',\n 'ash',\n 'alcalinity_of_ash',\n 'magnesium',\n 'total_phenols',\n 'flavanoids',\n 'nonflavanoid_phenols',\n 'proanthocyanins',\n 'color_intensity',\n 'hue',\n 'od280/od315_of_diluted_wines',\n 'proline']\n\n\n\n#instalamos SHAP\n!pip install shap\n\n\nCollecting shap\n\n  Downloading shap-0.46.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.26.4)\n\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.13.1)\n\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.3.2)\n\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (2.1.4)\n\nRequirement already satisfied: tqdm&gt;=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.5)\n\nRequirement already satisfied: packaging&gt;20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (24.1)\n\nCollecting slicer==0.0.8 (from shap)\n\n  Downloading slicer-0.0.8-py3-none-any.whl.metadata (4.0 kB)\n\nRequirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.60.0)\n\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n\nRequirement already satisfied: llvmlite&lt;0.44,&gt;=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba-&gt;shap) (0.43.0)\n\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;shap) (2.8.2)\n\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;shap) (2024.1)\n\nRequirement already satisfied: tzdata&gt;=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;shap) (2024.1)\n\nRequirement already satisfied: joblib&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-&gt;shap) (1.4.2)\n\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-&gt;shap) (3.5.0)\n\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.8.2-&gt;pandas-&gt;shap) (1.16.0)\n\nDownloading shap-0.46.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (540 kB)\n\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 540.1/540.1 kB 18.0 MB/s eta 0:00:00\n\nDownloading slicer-0.0.8-py3-none-any.whl (15 kB)\n\nInstalling collected packages: slicer, shap\n\nSuccessfully installed shap-0.46.0 slicer-0.0.8\n\n\n\n\n\n#Computemos los valores SHAP de nuestro modelo\nimport shap\nexplainer = shap.Explainer(model, X_train,feature_names=wine.feature_names)\nshap_values = explainer(X_train)\n\nPermutationExplainer explainer: 125it [00:11,  1.92it/s]\n\n\n\nPor ejemplo. Para nuestro modelo, la caracteristica que mas aporta para que se prediga de la clase 1, es proline, y la que menos aporta es malic_acid.\n\n\nshap.plots.bar(shap_values[:,:,0], max_display=X_train.shape[1])\n\n\n\n\n\n\n\n\n\nPara la clase 2, la caracteristica que mas aporta es alcohol y la que menos aporta a dicha predicción es nonflavanoid_phenols.\n\n\nshap.plots.bar(shap_values[:,:,1], max_display=X_train.shape[1])\n\n\n\n\n\n\n\n\n\nPara la tercera clase, la caracteristica mas importante es huge y la menos importante es magnesium\n\n\nshap.plots.bar(shap_values[:,:,2], max_display=X_train.shape[1])\n\n\n\n\n\n\n\n\n\nAhora tomemos un dato de entrenamiento y veamos como las caracteristicas influyeron para que la red se inclinara por la categoría cierta.\n\n\nclase = y_train[4]\nprint(\"clase: \", clase)\nshap.plots.bar(shap_values[4,:,np.argmax(clase)], max_display=X_train.shape[1])\n\nclase:  [0. 1. 0.]\n\n\n\n\n\n\n\n\n\n\nDel gráfico anterior podemos evidenciar que este ejemplo en contreto tuvo caracteristicas que no influyeron en la decision del modelo (alcalinity_os_ash, malic_acid, nonflavanoid_phenols). Por otro lado la caracteristica que mas influyó en este dato en particular fue proline.\nTambien podriamos ver para cada clase como aporta cada caracteristica a la predicción del modelo.\n\n\nprint(\"clase 1\")\nshap.plots.beeswarm(shap_values[:,:,0], max_display=X.shape[1])\n\nclase 1\n\n\n\n\n\n\n\n\n\n\nprint(\"clase 2\")\nshap.plots.beeswarm(shap_values[:,:,1], max_display=X.shape[1])\n\nclase 2\n\n\n\n\n\n\n\n\n\n\nprint(\"clase 3\")\nshap.plots.beeswarm(shap_values[:,:,2], max_display=X.shape[1])\n\nclase 3"
  },
  {
    "objectID": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html#conclusión",
    "href": "semana_3/notebooks/Nb_3b_Introduccion_explicabilidad_interpretabilidad.html#conclusión",
    "title": "Introducción a la explicabilidad e interpretabilidad en modelos",
    "section": "Conclusión:",
    "text": "Conclusión:\n\nNote que ahora podemos entender mucho mejor nuestra red, y que caracteristicas son mas importantes para determinar la clase a predecir. Por ejemplo, esto es muy útil para poder explicar nuestro modelo, para que una persona pueda entender cómo funciona. Por ejemplo, si fuese un conjunto de datos bancarios, saber por cuales caracteristicas fue rechazado un crédito y que así el cliente pueda mejorar."
  },
  {
    "objectID": "semana_3/index.html",
    "href": "semana_3/index.html",
    "title": "Semana 3: Técnicas Avanzadas y Robustez",
    "section": "",
    "text": "La última semana se centra en refinar y aplicar técnicas avanzadas para construir modelos robustos e interpretables.\nTemas Clave:\n\nPrincipios de Generalización.\nTransferencia de Aprendizaje y Fine-tuning.\nExplicabilidad e Interpretabilidad de modelos.\nOptimización de hiperparámetros.\nEntrenamiento Adversarial y Robustez.\n\nEnfoque Práctico: Desarrollarás habilidades para mejorar la eficacia de tus modelos en entornos reales. Implementarás técnicas de transferencia de aprendizaje, fine-tuning, generalización y usarás herramientas de explicabilidad para comprender y comunicar los resultados de modelos complejos.\nMateriales de la Semana:\n\nSlides de la Semana 3 (Próximamente)\nNotebook: Introducción a Transfer Learning y Finetuning\nNotebook: Introducción a Explicabilidad e Interpretabilidad"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2b_Resolviendo_desafios_con_RNNs_usando_keras.html",
    "href": "semana_2/notebooks/Nb_2b_Resolviendo_desafios_con_RNNs_usando_keras.html",
    "title": "Resolviendo diversidad de tareas con RNNs",
    "section": "",
    "text": "Open In Colab\n#@title Importar librerías\n#importar librerías necesarias\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\n#@title Funciones complementarias\n\ndef plot_graphs(history, metric):\n  plt.plot(history.history[metric])\n  plt.plot(history.history['val_'+metric], '')\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(metric)\n  plt.legend([metric, 'val_'+metric])\n\ndef plot_variables(df, date_time):\n    plot_cols = ['T (degC)', 'p (mbar)', 'rho (g/m**3)']\n    plot_features = df[plot_cols]\n    plot_features.index = date_time\n    _ = plot_features.plot(subplots=True)\n\n    plot_features = df[plot_cols][:480]\n    plot_features.index = date_time[:480]\n    _ = plot_features.plot(subplots=True)\n\n\n\ndef read_dataset_clima():\n\n    zip_path = tf.keras.utils.get_file(\n        origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n        fname='jena_climate_2009_2016.csv.zip',\n        extract=True)\n    csv_path, _ = os.path.splitext(zip_path)\n\n    df = pd.read_csv(csv_path)\n    # Slice [start:stop:step], starting from index 5 take every 6th record.\n    df = df[5::6]\n\n    date_time = pd.to_datetime(df.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')\n\n    return df, date_time\n\ndef split_data(df):\n    column_indices = {name: i for i, name in enumerate(df.columns)}\n\n    n = len(df)\n    train_df = df[0:int(n*0.7)]\n    val_df = df[int(n*0.7):int(n*0.9)]\n    test_df = df[int(n*0.9):]\n\n    num_features = df.shape[1]\n\n    return train_df, val_df, test_df, num_features, column_indices\n\ndef normalizacion_datos(train_df, val_df, test_df):\n    train_mean = train_df.mean()\n    train_std = train_df.std()\n\n    train_df = (train_df - train_mean) / train_std\n    val_df = (val_df - train_mean) / train_std\n    test_df = (test_df - train_mean) / train_std\n\n    return train_df, val_df, test_df\n\n\nclass WindowGenerator():\n  def __init__(self, input_width, label_width, shift,\n               train_df, val_df, test_df,\n               label_columns=None):\n    # Store the raw data.\n    self.train_df = train_df\n    self.val_df = val_df\n    self.test_df = test_df\n\n    # Work out the label column indices.\n    self.label_columns = label_columns\n    if label_columns is not None:\n      self.label_columns_indices = {name: i for i, name in\n                                    enumerate(label_columns)}\n    self.column_indices = {name: i for i, name in\n                           enumerate(train_df.columns)}\n\n    # Work out the window parameters.\n    self.input_width = input_width\n    self.label_width = label_width\n    self.shift = shift\n\n    self.total_window_size = input_width + shift\n\n    self.input_slice = slice(0, input_width)\n    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n\n    self.label_start = self.total_window_size - self.label_width\n    self.labels_slice = slice(self.label_start, None)\n    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n\n  def __repr__(self):\n    return '\\n'.join([\n        f'Total window size: {self.total_window_size}',\n        f'Input indices: {self.input_indices}',\n        f'Label indices: {self.label_indices}',\n        f'Label column name(s): {self.label_columns}'])\n\n\ndef split_window(self, features):\n  inputs = features[:, self.input_slice, :]\n  labels = features[:, self.labels_slice, :]\n  if self.label_columns is not None:\n    labels = tf.stack(\n        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n        axis=-1)\n\n  # Slicing doesn't preserve static shape information, so set the shapes\n  # manually. This way the `tf.data.Datasets` are easier to inspect.\n  inputs.set_shape([None, self.input_width, None])\n  labels.set_shape([None, self.label_width, None])\n\n  return inputs, labels\n\ndef plot(self, model=None, plot_col='T (degC)', max_subplots=3):\n  inputs, labels = self.example\n  plt.figure(figsize=(12, 8))\n  plot_col_index = self.column_indices[plot_col]\n  max_n = min(max_subplots, len(inputs))\n  for n in range(max_n):\n    plt.subplot(max_n, 1, n+1)\n    plt.ylabel(f'{plot_col} [normed]')\n    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n             label='Inputs', marker='.', zorder=-10)\n\n    if self.label_columns:\n      label_col_index = self.label_columns_indices.get(plot_col, None)\n    else:\n      label_col_index = plot_col_index\n\n    if label_col_index is None:\n      continue\n\n    plt.scatter(self.label_indices, labels[n, :, label_col_index],\n                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n    if model is not None:\n      predictions = model(inputs)\n      plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n                  marker='X', edgecolors='k', label='Predictions',\n                  c='#ff7f0e', s=64)\n\n    if n == 0:\n      plt.legend()\n\n  plt.xlabel('Time [h]')\n\ndef make_dataset(self, data):\n  data = np.array(data, dtype=np.float32)\n  ds = tf.keras.utils.timeseries_dataset_from_array(\n      data=data,\n      targets=None,\n      sequence_length=self.total_window_size,\n      sequence_stride=1,\n      shuffle=True,\n      batch_size=32,)\n\n  ds = ds.map(self.split_window)\n\n  return ds\n\n\n@property\ndef train(self):\n  return self.make_dataset(self.train_df)\n\n@property\ndef val(self):\n  return self.make_dataset(self.val_df)\n\n@property\ndef test(self):\n  return self.make_dataset(self.test_df)\n\n@property\ndef example(self):\n  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n  result = getattr(self, '_example', None)\n  if result is None:\n    # No example batch was found, so get one from the `.train` dataset\n    result = next(iter(self.train))\n    # And cache it for next time\n    self._example = result\n  return result\n\n\n\nWindowGenerator.plot = plot\nWindowGenerator.split_window = split_window\nWindowGenerator.make_dataset = make_dataset\nWindowGenerator.train = train\nWindowGenerator.val = val\nWindowGenerator.test = test\nWindowGenerator.example = example"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2b_Resolviendo_desafios_con_RNNs_usando_keras.html#clasificación-de-texto-usando-rnns-relación-muchas-entradas-a-una-salida",
    "href": "semana_2/notebooks/Nb_2b_Resolviendo_desafios_con_RNNs_usando_keras.html#clasificación-de-texto-usando-rnns-relación-muchas-entradas-a-una-salida",
    "title": "Resolviendo diversidad de tareas con RNNs",
    "section": "Clasificación de texto usando RNNs (Relación muchas entradas a una salida)",
    "text": "Clasificación de texto usando RNNs (Relación muchas entradas a una salida)\nEn este escenario se reciben multiples entradas pero solo se genera una salida. Aquí podemos abordar el ejemplo más común que es la clasificación de texto.\n\nDescargar el dataset de reviews de peliculas usando Tensorflow.\nEl gran conjunto de datos de críticas de películas de IMDB es un conjunto de datos de clasificación binaria: todas las críticas tienen un sentimiento positivo o negativo.\n\ndataset, info = tfds.load('imdb_reviews', with_info=True,\n                          as_supervised=True)\ntrain_dataset, test_dataset = dataset['train'], dataset['test']\n\nDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\n\n\n\n# contar cuantas muestras tiene train_dataset y test_dataset\nprint(info.splits)\n\n{Split('train'): &lt;SplitInfo num_examples=25000, num_shards=1&gt;, Split('test'): &lt;SplitInfo num_examples=25000, num_shards=1&gt;, Split('unsupervised'): &lt;SplitInfo num_examples=50000, num_shards=1&gt;}\n\n\n\n# extraer una muestra del conjunto de entrenamiento\ntrain_example, train_label = next(iter(train_dataset.batch(1)))\n\nprint('Para la etiqueta: {} se tiene el texto: {}'.format(train_label, train_example.numpy()))\n\nPara la etiqueta: [0] se tiene el texto: [b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"]\n\n\nLas siguientes lineas son para optimizar los conjuntos y que el entrenamiento sea más acelerado.\n\nBUFFER_SIZE = 10000\nBATCH_SIZE = 32\n\n# optimización para train\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\n# optimización para test\ntest_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\n\n# muestras para el conjunto de test\nfor ejemplo, label in test_dataset.take(1):\n  print('textos: ', ejemplo.numpy()[:3])\n  print()\n  print('labels: ', label.numpy()[:3])\n\ntextos:  [b\"There are films that make careers. For George Romero, it was NIGHT OF THE LIVING DEAD; for Kevin Smith, CLERKS; for Robert Rodriguez, EL MARIACHI. Add to that list Onur Tukel's absolutely amazing DING-A-LING-LESS. Flawless film-making, and as assured and as professional as any of the aforementioned movies. I haven't laughed this hard since I saw THE FULL MONTY. (And, even then, I don't think I laughed quite this hard... So to speak.) Tukel's talent is considerable: DING-A-LING-LESS is so chock full of double entendres that one would have to sit down with a copy of this script and do a line-by-line examination of it to fully appreciate the, uh, breadth and width of it. Every shot is beautifully composed (a clear sign of a sure-handed director), and the performances all around are solid (there's none of the over-the-top scenery chewing one might've expected from a film like this). DING-A-LING-LESS is a film whose time has come.\"\n b\"A blackly comic tale of a down-trodden priest, Nazarin showcases the economy that Luis Bunuel was able to achieve in being able to tell a deeply humanist fable with a minimum of fuss. As an output from his Mexican era of film making, it was an invaluable talent to possess, with little money and extremely tight schedules. Nazarin, however, surpasses many of Bunuel's previous Mexican films in terms of the acting (Francisco Rabal is excellent), narrative and theme.&lt;br /&gt;&lt;br /&gt;The theme, interestingly, is something that was explored again in Viridiana, made three years later in Spain. It concerns the individual's struggle for humanity and altruism amongst a society that rejects any notion of virtue. Father Nazarin, however, is portrayed more sympathetically than Sister Viridiana. Whereas the latter seems to choose charity because she wishes to atone for her (perceived) sins, Nazarin's whole existence and reason for being seems to be to help others, whether they (or we) like it or not. The film's last scenes, in which he casts doubt on his behaviour and, in a split second, has to choose between the life he has been leading or the conventional life that is expected of a priest, are so emotional because they concern his moral integrity and we are never quite sure whether it remains intact or not.&lt;br /&gt;&lt;br /&gt;This is a remarkable film and I would urge anyone interested in classic cinema to seek it out. It is one of Bunuel's most moving films, and encapsulates many of his obsessions: frustrated desire, mad love, religious hypocrisy etc. In my view 'Nazarin' is second only to 'The Exterminating Angel', in terms of his Mexican movies, and is certainly near the top of the list of Bunuel's total filmic output.\"\n b'Scary Movie 1-4, Epic Movie, Date Movie, Meet the Spartans, Not another Teen Movie and Another Gay Movie. Making \"Superhero Movie\" the eleventh in a series that single handily ruined the parody genre. Now I\\'ll admit it I have a soft spot for classics such as Airplane and The Naked Gun but you know you\\'ve milked a franchise so bad when you can see the gags a mile off. In fact the only thing that might really temp you into going to see this disaster is the incredibly funny but massive sell-out Leslie Neilson.&lt;br /&gt;&lt;br /&gt;You can tell he needs the money, wither that or he intends to go down with the ship like a good Capitan would. In no way is he bringing down this genre but hell he\\'s not helping it. But if I feel sorry for anybody in this film its decent actor Drake Bell who is put through an immense amount of embarrassment. The people who are put through the largest amount of torture by far however is the audience forced to sit through 90 minutes of laughless bile no funnier than herpes.&lt;br /&gt;&lt;br /&gt;After spoofing disaster films in Airplane!, police shows in The Naked Gun, and Hollywood horrors in Scary Movie 3 and 4, producer David Zucker sets his satirical sights on the superhero genre with this anarchic comedy lampooning everything from Spider-Man to X-Men and Superman Returns.&lt;br /&gt;&lt;br /&gt;Shortly after being bitten by a genetically altered dragonfly, high-school outcast Rick Riker (Drake Bell) begins to experience a startling transformation. Now Rick\\'s skin is as strong as steel, and he possesses the strength of ten men. Determined to use his newfound powers to fight crime, Rick creates a special costume and assumes the identity of The Dragonfly -- a fearless crime fighter dedicated to keeping the streets safe for law-abiding citizens.&lt;br /&gt;&lt;br /&gt;But every superhero needs a nemesis, and after Lou Landers (Christopher McDonald) is caught in the middle of an experiment gone horribly awry, he develops the power to leech the life force out of anyone he meets and becomes the villainous Hourglass. Intent on achieving immortality, the Hourglass attempts to gather as much life force as possible as the noble Dragonfly sets out to take down his archenemy and realize his destiny as a true hero. Craig Mazin writes and directs this low-flying spoof.&lt;br /&gt;&lt;br /&gt;featuring Tracy Morgan, Pamela Anderson, Leslie Nielsen, Marion Ross, Jeffrey Tambor, and Regina Hall.&lt;br /&gt;&lt;br /&gt;Hell Superhero Movie may earn some merit in the fact that it\\'s a hell of a lot better than Meet the Spartans and Epic Movie. But with great responsibility comes one of the worst outings of 2008 to date. Laughless but a little less irritating than Meet the Spartans. And in the same sense much more forgettable than meet the Spartans. But maybe that\\'s a good reason. There are still some of us trying to scrape away the stain that was Meet the Spartans from our memory.&lt;br /&gt;&lt;br /&gt;My final verdict? Avoid, unless you\\'re one of thoses people who enjoy such car crash cinema. As bad as Date Movie and Scary Movie 2 but not quite as bad as Meet the Spartans or Epic Movie. Super Villain.']\n\nlabels:  [1 1 0]\n\n\n\n\nPre-procesamiento y modelado.\n\n\n\nbidirectional.png\n\n\nExplicación del modelo:\nEste modelo se puede construir como un tf.keras.Sequential.\n\nLa primera capa (amarilla) es el codificador, que convierte el texto en una secuencia de índices de tokens.\nDespués del codificador hay una capa de embedding (verde). Una capa de embedding almacena un vector por palabra. Cuando se llama, convierte las secuencias de índices de palabras en secuencias de vectores. Estos vectores son entrenables. Después del entrenamiento (con suficientes datos), las palabras con significados similares a menudo tienen vectores similares.\nUna red neuronal recurrente (RNN) procesa la entrada secuencial iterando a través de los elementos. Las RNN pasan las salidas de un instante de tiempo a su entrada en el siguiente instante de tiempo.\nLa capa tf.keras.layers.Bidirectional también se puede usar con una capa RNN. Esto propaga la entrada hacia adelante y hacia atrás a través de la capa RNN y luego concatena la salida final.\n\nLa principal ventaja de una RNN bidireccional es que la señal desde el comienzo de la entrada no necesita ser procesada a lo largo de cada instante de tiempo para afectar la salida.\nLa principal desventaja de una RNN bidireccional es que no se pueden transmitir predicciones de manera eficiente a medida que se agregan palabras al final.\n\nDespués de que la RNN ha convertido la secuencia en un solo vector, las dos capas Dense realizan un procesamiento final y convierten esta representación vectorial en un solo valor como salida de clasificación.\n\n\nCapa Codificador (Text Vectorization)\nEl texto sin procesar cargado por tfds necesita ser procesado antes de que pueda ser utilizado en un modelo. La forma más sencilla de procesar texto para el entrenamiento es utilizando la capa TextVectorization.\nExplicación:\nLa capa TextVectorization es una herramienta en TensorFlow que transforma texto sin procesar en una representación numérica que los modelos de aprendizaje automático pueden entender. Convierte las palabras en vectores, lo que facilita el entrenamiento del modelo. Esto incluye tareas como:\n\nTokenización: Dividir el texto en palabras o subpalabras.\nNormalización: Convertir todo el texto a minúsculas, eliminar caracteres especiales, etc.\nVectorización: Asignar un índice o valor numérico a cada palabra o token, lo que permite representarlo como una matriz numérica.\n\n\nVOCAB_SIZE = 1000\nencoder = tf.keras.layers.TextVectorization(\n    max_tokens=VOCAB_SIZE)\n\n# Crea la capa y pasa el texto del conjunto de datos al método .adapt de la capa\nencoder.adapt(train_dataset.map(lambda text, label: text))\n\nEl método .adapt se utiliza para “entrenar” la capa en el conjunto de datos de texto. Esto significa que la capa analizará el texto y aprenderá el vocabulario, así como otras características (como la frecuencia de las palabras) que serán útiles para la vectorización\n\n# revisar los primeros 20 elementos del vocabulario creado\nvocab = np.array(encoder.get_vocabulary())\nvocab[:20]\n\narray(['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i',\n       'this', 'that', 'br', 'was', 'as', 'for', 'with', 'movie', 'but'],\n      dtype='&lt;U14')\n\n\nUna vez que el vocabulario está establecido, la capa puede codificar el texto en índices. Los tensores de índices se rellenan con ceros hasta la secuencia más larga en el batch (a menos que establezcas una longitud de salida fija).\n\nencoded_ejemplo = encoder(ejemplo)[:3].numpy()\nencoded_ejemplo\n\narray([[ 48,  24,  95, ...,   0,   0,   0],\n       [  4,   1, 723, ...,   0,   0,   0],\n       [633,  18,   1, ...,  18,   1,   1]])\n\n\n\n# Visualizamos algunas imágenes\nfor n in range(3):\n  print(\"Original: \", ejemplo[n].numpy())\n  print(\"Round-trip: \", \" \".join(vocab[encoded_ejemplo[n]]))\n  print()\n\nOriginal:  b\"There are films that make careers. For George Romero, it was NIGHT OF THE LIVING DEAD; for Kevin Smith, CLERKS; for Robert Rodriguez, EL MARIACHI. Add to that list Onur Tukel's absolutely amazing DING-A-LING-LESS. Flawless film-making, and as assured and as professional as any of the aforementioned movies. I haven't laughed this hard since I saw THE FULL MONTY. (And, even then, I don't think I laughed quite this hard... So to speak.) Tukel's talent is considerable: DING-A-LING-LESS is so chock full of double entendres that one would have to sit down with a copy of this script and do a line-by-line examination of it to fully appreciate the, uh, breadth and width of it. Every shot is beautifully composed (a clear sign of a sure-handed director), and the performances all around are solid (there's none of the over-the-top scenery chewing one might've expected from a film like this). DING-A-LING-LESS is a film whose time has come.\"\nRound-trip:  there are films that make [UNK] for george [UNK] it was night of the living dead for [UNK] [UNK] [UNK] for robert [UNK] [UNK] [UNK] add to that [UNK] [UNK] [UNK] absolutely amazing [UNK] [UNK] [UNK] and as [UNK] and as [UNK] as any of the [UNK] movies i havent [UNK] this hard since i saw the full [UNK] and even then i dont think i [UNK] quite this hard so to [UNK] [UNK] talent is [UNK] [UNK] is so [UNK] full of [UNK] [UNK] that one would have to sit down with a [UNK] of this script and do a [UNK] [UNK] of it to [UNK] [UNK] the [UNK] [UNK] and [UNK] of it every shot is [UNK] [UNK] a clear [UNK] of a [UNK] director and the performances all around are [UNK] theres none of the [UNK] [UNK] [UNK] one [UNK] expected from a film like this [UNK] is a film whose time has come                                                                                                                                                                                                                                                                                                                                                                                        \n\nOriginal:  b\"A blackly comic tale of a down-trodden priest, Nazarin showcases the economy that Luis Bunuel was able to achieve in being able to tell a deeply humanist fable with a minimum of fuss. As an output from his Mexican era of film making, it was an invaluable talent to possess, with little money and extremely tight schedules. Nazarin, however, surpasses many of Bunuel's previous Mexican films in terms of the acting (Francisco Rabal is excellent), narrative and theme.&lt;br /&gt;&lt;br /&gt;The theme, interestingly, is something that was explored again in Viridiana, made three years later in Spain. It concerns the individual's struggle for humanity and altruism amongst a society that rejects any notion of virtue. Father Nazarin, however, is portrayed more sympathetically than Sister Viridiana. Whereas the latter seems to choose charity because she wishes to atone for her (perceived) sins, Nazarin's whole existence and reason for being seems to be to help others, whether they (or we) like it or not. The film's last scenes, in which he casts doubt on his behaviour and, in a split second, has to choose between the life he has been leading or the conventional life that is expected of a priest, are so emotional because they concern his moral integrity and we are never quite sure whether it remains intact or not.&lt;br /&gt;&lt;br /&gt;This is a remarkable film and I would urge anyone interested in classic cinema to seek it out. It is one of Bunuel's most moving films, and encapsulates many of his obsessions: frustrated desire, mad love, religious hypocrisy etc. In my view 'Nazarin' is second only to 'The Exterminating Angel', in terms of his Mexican movies, and is certainly near the top of the list of Bunuel's total filmic output.\"\nRound-trip:  a [UNK] comic tale of a [UNK] [UNK] [UNK] [UNK] the [UNK] that [UNK] [UNK] was able to [UNK] in being able to tell a [UNK] [UNK] [UNK] with a [UNK] of [UNK] as an [UNK] from his [UNK] [UNK] of film making it was an [UNK] talent to [UNK] with little money and extremely [UNK] [UNK] [UNK] however [UNK] many of [UNK] previous [UNK] films in [UNK] of the acting [UNK] [UNK] is excellent [UNK] and [UNK] br the theme [UNK] is something that was [UNK] again in [UNK] made three years later in [UNK] it [UNK] the [UNK] [UNK] for [UNK] and [UNK] [UNK] a society that [UNK] any [UNK] of [UNK] father [UNK] however is portrayed more [UNK] than sister [UNK] [UNK] the [UNK] seems to [UNK] [UNK] because she [UNK] to [UNK] for her [UNK] [UNK] [UNK] whole [UNK] and reason for being seems to be to help others whether they or we like it or not the films last scenes in which he [UNK] doubt on his [UNK] and in a [UNK] second has to [UNK] between the life he has been leading or the [UNK] life that is expected of a [UNK] are so emotional because they [UNK] his [UNK] [UNK] and we are never quite sure whether it [UNK] [UNK] or [UNK] br this is a [UNK] film and i would [UNK] anyone interested in classic cinema to [UNK] it out it is one of [UNK] most moving films and [UNK] many of his [UNK] [UNK] [UNK] [UNK] love [UNK] [UNK] etc in my view [UNK] is second only to the [UNK] [UNK] in [UNK] of his [UNK] movies and is certainly near the top of the [UNK] of [UNK] total [UNK] [UNK]                                                                                                                                                                                                                                                   \n\nOriginal:  b'Scary Movie 1-4, Epic Movie, Date Movie, Meet the Spartans, Not another Teen Movie and Another Gay Movie. Making \"Superhero Movie\" the eleventh in a series that single handily ruined the parody genre. Now I\\'ll admit it I have a soft spot for classics such as Airplane and The Naked Gun but you know you\\'ve milked a franchise so bad when you can see the gags a mile off. In fact the only thing that might really temp you into going to see this disaster is the incredibly funny but massive sell-out Leslie Neilson.&lt;br /&gt;&lt;br /&gt;You can tell he needs the money, wither that or he intends to go down with the ship like a good Capitan would. In no way is he bringing down this genre but hell he\\'s not helping it. But if I feel sorry for anybody in this film its decent actor Drake Bell who is put through an immense amount of embarrassment. The people who are put through the largest amount of torture by far however is the audience forced to sit through 90 minutes of laughless bile no funnier than herpes.&lt;br /&gt;&lt;br /&gt;After spoofing disaster films in Airplane!, police shows in The Naked Gun, and Hollywood horrors in Scary Movie 3 and 4, producer David Zucker sets his satirical sights on the superhero genre with this anarchic comedy lampooning everything from Spider-Man to X-Men and Superman Returns.&lt;br /&gt;&lt;br /&gt;Shortly after being bitten by a genetically altered dragonfly, high-school outcast Rick Riker (Drake Bell) begins to experience a startling transformation. Now Rick\\'s skin is as strong as steel, and he possesses the strength of ten men. Determined to use his newfound powers to fight crime, Rick creates a special costume and assumes the identity of The Dragonfly -- a fearless crime fighter dedicated to keeping the streets safe for law-abiding citizens.&lt;br /&gt;&lt;br /&gt;But every superhero needs a nemesis, and after Lou Landers (Christopher McDonald) is caught in the middle of an experiment gone horribly awry, he develops the power to leech the life force out of anyone he meets and becomes the villainous Hourglass. Intent on achieving immortality, the Hourglass attempts to gather as much life force as possible as the noble Dragonfly sets out to take down his archenemy and realize his destiny as a true hero. Craig Mazin writes and directs this low-flying spoof.&lt;br /&gt;&lt;br /&gt;featuring Tracy Morgan, Pamela Anderson, Leslie Nielsen, Marion Ross, Jeffrey Tambor, and Regina Hall.&lt;br /&gt;&lt;br /&gt;Hell Superhero Movie may earn some merit in the fact that it\\'s a hell of a lot better than Meet the Spartans and Epic Movie. But with great responsibility comes one of the worst outings of 2008 to date. Laughless but a little less irritating than Meet the Spartans. And in the same sense much more forgettable than meet the Spartans. But maybe that\\'s a good reason. There are still some of us trying to scrape away the stain that was Meet the Spartans from our memory.&lt;br /&gt;&lt;br /&gt;My final verdict? Avoid, unless you\\'re one of thoses people who enjoy such car crash cinema. As bad as Date Movie and Scary Movie 2 but not quite as bad as Meet the Spartans or Epic Movie. Super Villain.'\nRound-trip:  scary movie [UNK] [UNK] movie [UNK] movie meet the [UNK] not another [UNK] movie and another [UNK] movie making [UNK] movie the [UNK] in a series that single [UNK] [UNK] the [UNK] genre now ill admit it i have a [UNK] [UNK] for [UNK] such as [UNK] and the [UNK] [UNK] but you know youve [UNK] a [UNK] so bad when you can see the [UNK] a [UNK] off in fact the only thing that might really [UNK] you into going to see this [UNK] is the incredibly funny but [UNK] [UNK] [UNK] [UNK] br you can tell he needs the money [UNK] that or he [UNK] to go down with the [UNK] like a good [UNK] would in no way is he [UNK] down this genre but hell hes not [UNK] it but if i feel sorry for [UNK] in this film its decent actor [UNK] [UNK] who is put through an [UNK] [UNK] of [UNK] the people who are put through the [UNK] [UNK] of [UNK] by far however is the audience forced to sit through [UNK] minutes of [UNK] [UNK] no [UNK] than [UNK] br after [UNK] [UNK] films in [UNK] police shows in the [UNK] [UNK] and hollywood [UNK] in scary movie 3 and 4 [UNK] david [UNK] sets his [UNK] [UNK] on the [UNK] genre with this [UNK] comedy [UNK] everything from [UNK] to [UNK] and [UNK] [UNK] br [UNK] after being [UNK] by a [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] begins to experience a [UNK] [UNK] now [UNK] [UNK] is as strong as [UNK] and he [UNK] the [UNK] of ten men [UNK] to use his [UNK] [UNK] to fight crime [UNK] [UNK] a special [UNK] and [UNK] the [UNK] of the [UNK] a [UNK] crime [UNK] [UNK] to [UNK] the [UNK] [UNK] for [UNK] [UNK] br but every [UNK] needs a [UNK] and after [UNK] [UNK] [UNK] [UNK] is [UNK] in the middle of an [UNK] gone [UNK] [UNK] he [UNK] the power to [UNK] the life [UNK] out of anyone he meets and becomes the [UNK] [UNK] [UNK] on [UNK] [UNK] the [UNK] attempts to [UNK] as much life [UNK] as possible as the [UNK] [UNK] sets out to take down his [UNK] and realize his [UNK] as a true hero [UNK] [UNK] [UNK] and [UNK] this [UNK] [UNK] br [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] and [UNK] [UNK] br hell [UNK] movie may [UNK] some [UNK] in the fact that its a hell of a lot better than meet the [UNK] and [UNK] movie but with great [UNK] comes one of the worst [UNK] of [UNK] to [UNK] [UNK] but a little less [UNK] than meet the [UNK] and in the same sense much more [UNK] than meet the [UNK] but maybe thats a good reason there are still some of us trying to [UNK] away the [UNK] that was meet the [UNK] from our [UNK] br my final [UNK] avoid unless youre one of [UNK] people who enjoy such car [UNK] cinema as bad as [UNK] movie and scary movie 2 but not quite as bad as meet the [UNK] or [UNK] movie [UNK] [UNK]\n\n\n\n\n\nCreación del modelo usando keras\n\n'''model = tf.keras.Sequential([\n    encoder,\n    tf.keras.layers.Embedding(\n        input_dim=len(encoder.get_vocabulary()),\n        output_dim=64,\n        # Se ignora los valores en 0\n        mask_zero=True),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64), merge_mode='concat'),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1)\n])'''\n\ndef rnn():\n    # Ahora utilizamos la API funcional de Keras\n    inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)  # El input será una cadena de texto\n    x = encoder(inputs)  # Aplicamos el encoder\n\n    x = tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary()), output_dim=64, mask_zero=True)(x)  # Capa de Embedding\n\n    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False), merge_mode='concat')(x)  # Capa LSTM Bidireccional\n\n    x = tf.keras.layers.Dense(64, activation='relu')(x)  # Capa densa\n    outputs = tf.keras.layers.Dense(1)(x)  # Capa de salida\n\n    # Definimos el modelo\n    model = tf.keras.Model(inputs, outputs)\n\n    return model\n\n# crear la red RNN\nmodel = rnn()\n\n# Compilamos el modelo\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              optimizer=tf.keras.optimizers.Adam(1e-4),\n              metrics=['accuracy'])\n\n# Verificamos la estructura del modelo\nmodel.summary()\n\nModel: \"functional_2\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)              ┃ Output Shape           ┃        Param # ┃ Connected to           ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_2             │ (None, 1)              │              0 │ -                      │\n│ (InputLayer)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text_vectorization        │ (None, None)           │              0 │ input_layer_2[0][0]    │\n│ (TextVectorization)       │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ embedding_4 (Embedding)   │ (None, None, 64)       │         64,000 │ text_vectorization[2]… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ not_equal_2 (NotEqual)    │ (None, None)           │              0 │ text_vectorization[2]… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ bidirectional_4           │ (None, 128)            │         66,048 │ embedding_4[0][0],     │\n│ (Bidirectional)           │                        │                │ not_equal_2[0][0]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_8 (Dense)           │ (None, 64)             │          8,256 │ bidirectional_4[0][0]  │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_9 (Dense)           │ (None, 1)              │             65 │ dense_8[0][0]          │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n\n\n\n Total params: 138,369 (540.50 KB)\n\n\n\n Trainable params: 138,369 (540.50 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\ntf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)\n\n\n\n\n\n\n\n\n\n# hacer una prueba sin usar padding\n# El texto crudo que quieres predecir\nsample_text = ('The movie was cool. The animation and the graphics were out of this world.')\n\n# No es necesario hacer la vectorización manual aquí, simplemente pasa el texto crudo al modelo\npredictions = model.predict(tf.constant([sample_text]))\n\n# Imprime la predicción\nprint(predictions[0])\n\n\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 279ms/step\n\n[-0.00120403]\n\n\n\n\n\n# prueba ahora usando padding\n# debe dar el mismo resultado\npadding = \"the \" * 2000\npredictions = model.predict(tf.constant([sample_text, padding]))\nprint(predictions[0])\n\n\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 367ms/step\n\n[-0.00120403]\n\n\n\n\n\n\nEntrenamiento y evaluación del modelo\n\nhistory = model.fit(train_dataset, epochs=10,\n                    validation_data=test_dataset,\n                    validation_steps=30)\n\n\nEpoch 1/10\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 41s 49ms/step - accuracy: 0.5446 - loss: 0.6621 - val_accuracy: 0.7406 - val_loss: 0.4787\n\nEpoch 2/10\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 38s 49ms/step - accuracy: 0.8113 - loss: 0.4087 - val_accuracy: 0.8531 - val_loss: 0.3368\n\nEpoch 3/10\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 41s 48ms/step - accuracy: 0.8475 - loss: 0.3460 - val_accuracy: 0.8583 - val_loss: 0.3476\n\nEpoch 4/10\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 41s 48ms/step - accuracy: 0.8518 - loss: 0.3369 - val_accuracy: 0.8385 - val_loss: 0.3222\n\nEpoch 5/10\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 38s 48ms/step - accuracy: 0.8585 - loss: 0.3212 - val_accuracy: 0.8448 - val_loss: 0.3103\n\nEpoch 6/10\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 42s 50ms/step - accuracy: 0.8657 - loss: 0.3102 - val_accuracy: 0.8833 - val_loss: 0.3025\n\nEpoch 7/10\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 37s 47ms/step - accuracy: 0.8650 - loss: 0.3053 - val_accuracy: 0.8417 - val_loss: 0.3308\n\nEpoch 8/10\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 38s 48ms/step - accuracy: 0.8643 - loss: 0.3065 - val_accuracy: 0.8573 - val_loss: 0.3422\n\nEpoch 9/10\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 41s 48ms/step - accuracy: 0.8723 - loss: 0.2969 - val_accuracy: 0.8604 - val_loss: 0.3092\n\nEpoch 10/10\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 39s 50ms/step - accuracy: 0.8674 - loss: 0.3016 - val_accuracy: 0.8583 - val_loss: 0.3209\n\n\n\n\n\ntest_loss, test_acc = model.evaluate(test_dataset)\n\nprint('Test Loss:', test_loss)\nprint('Test Accuracy:', test_acc)\n\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 18s 23ms/step - accuracy: 0.8611 - loss: 0.3154\n\nTest Loss: 0.3137839138507843\n\nTest Accuracy: 0.8598799705505371\n\n\n\n\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplot_graphs(history, 'accuracy')\nplt.ylim(None, 1)\nplt.subplot(1, 2, 2)\nplot_graphs(history, 'loss')\nplt.ylim(0, None)\n\n\n\n\n\n\n\n\n\n# realizar una predicción des pues de entrenar\nsample_text = ('The movie was cool. The animation and the graphics were out of this world.')\npredictions = model.predict(tf.constant([sample_text]))\nprint(predictions)\n\n\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\n\n[[1.2081792]]\n\n\n\n\nCómo podríamos agregar más capas LSTM para aumentar la complejidad del modelo?\n\n\n\nlayered_bidirectional.png\n\n\nLas capas recurrentes de Keras tienen dos modos disponibles que son controlados por el argumento del constructor return_sequences:\n\nSi es False devuelve sólo la última salida para cada secuencia de entrada (un tensor 2D de forma (batch_size, output_features)). Este es el valor por defecto, utilizado en el modelo anterior.\nSi es True se devuelven las secuencias completas de salidas sucesivas para cada paso de tiempo (un tensor 3D de forma (batch_size, timesteps, output_features)).\n\n# model = tf.keras.Sequential([\n    encoder,\n    tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1)\n])"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2b_Resolviendo_desafios_con_RNNs_usando_keras.html#leer-datos",
    "href": "semana_2/notebooks/Nb_2b_Resolviendo_desafios_con_RNNs_usando_keras.html#leer-datos",
    "title": "Resolviendo diversidad de tareas con RNNs",
    "section": "Leer datos",
    "text": "Leer datos\n\ndf, date_time = read_dataset_clima()\n\n\ndf\n\n\n    \n\n\n\n\n\n\np (mbar)\nT (degC)\nTpot (K)\nTdew (degC)\nrh (%)\nVPmax (mbar)\nVPact (mbar)\nVPdef (mbar)\nsh (g/kg)\nH2OC (mmol/mol)\nrho (g/m**3)\nwv (m/s)\nmax. wv (m/s)\nwd (deg)\n\n\n\n\n5\n996.50\n-8.05\n265.38\n-8.78\n94.40\n3.33\n3.14\n0.19\n1.96\n3.15\n1307.86\n0.21\n0.63\n192.7\n\n\n11\n996.62\n-8.88\n264.54\n-9.77\n93.20\n3.12\n2.90\n0.21\n1.81\n2.91\n1312.25\n0.25\n0.63\n190.3\n\n\n17\n996.84\n-8.81\n264.59\n-9.66\n93.50\n3.13\n2.93\n0.20\n1.83\n2.94\n1312.18\n0.18\n0.63\n167.2\n\n\n23\n996.99\n-9.05\n264.34\n-10.02\n92.60\n3.07\n2.85\n0.23\n1.78\n2.85\n1313.61\n0.10\n0.38\n240.0\n\n\n29\n997.46\n-9.63\n263.72\n-10.65\n92.20\n2.94\n2.71\n0.23\n1.69\n2.71\n1317.19\n0.40\n0.88\n157.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n420521\n1002.18\n-0.98\n272.01\n-5.36\n72.00\n5.69\n4.09\n1.59\n2.54\n4.08\n1280.70\n0.87\n1.36\n190.6\n\n\n420527\n1001.40\n-1.40\n271.66\n-6.84\n66.29\n5.51\n3.65\n1.86\n2.27\n3.65\n1281.87\n1.02\n1.92\n225.4\n\n\n420533\n1001.19\n-2.75\n270.32\n-6.90\n72.90\n4.99\n3.64\n1.35\n2.26\n3.63\n1288.02\n0.71\n1.56\n158.7\n\n\n420539\n1000.65\n-2.89\n270.22\n-7.15\n72.30\n4.93\n3.57\n1.37\n2.22\n3.57\n1288.03\n0.35\n0.68\n216.7\n\n\n420545\n1000.11\n-3.93\n269.23\n-8.09\n72.60\n4.56\n3.31\n1.25\n2.06\n3.31\n1292.41\n0.56\n1.00\n202.6\n\n\n\n\n70091 rows × 14 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\ndate_time\n\n\n\n\n\n\n\n\nDate Time\n\n\n\n\n5\n2009-01-01 01:00:00\n\n\n11\n2009-01-01 02:00:00\n\n\n17\n2009-01-01 03:00:00\n\n\n23\n2009-01-01 04:00:00\n\n\n29\n2009-01-01 05:00:00\n\n\n...\n...\n\n\n420521\n2016-12-31 19:10:00\n\n\n420527\n2016-12-31 20:10:00\n\n\n420533\n2016-12-31 21:10:00\n\n\n420539\n2016-12-31 22:10:00\n\n\n420545\n2016-12-31 23:10:00\n\n\n\n\n70091 rows × 1 columns\ndtype: datetime64[ns]\n\n\n\nplot_variables(df, date_time)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_df, val_df, test_df, num_features, column_indices = split_data(df)\ntrain_df.shape, val_df.shape, test_df.shape\n\n((49063, 14), (14018, 14), (7010, 14))\n\n\n\ntrain_df.head()\n\n\n    \n\n\n\n\n\n\np (mbar)\nT (degC)\nTpot (K)\nTdew (degC)\nrh (%)\nVPmax (mbar)\nVPact (mbar)\nVPdef (mbar)\nsh (g/kg)\nH2OC (mmol/mol)\nrho (g/m**3)\nwv (m/s)\nmax. wv (m/s)\nwd (deg)\n\n\n\n\n5\n996.50\n-8.05\n265.38\n-8.78\n94.4\n3.33\n3.14\n0.19\n1.96\n3.15\n1307.86\n0.21\n0.63\n192.7\n\n\n11\n996.62\n-8.88\n264.54\n-9.77\n93.2\n3.12\n2.90\n0.21\n1.81\n2.91\n1312.25\n0.25\n0.63\n190.3\n\n\n17\n996.84\n-8.81\n264.59\n-9.66\n93.5\n3.13\n2.93\n0.20\n1.83\n2.94\n1312.18\n0.18\n0.63\n167.2\n\n\n23\n996.99\n-9.05\n264.34\n-10.02\n92.6\n3.07\n2.85\n0.23\n1.78\n2.85\n1313.61\n0.10\n0.38\n240.0\n\n\n29\n997.46\n-9.63\n263.72\n-10.65\n92.2\n2.94\n2.71\n0.23\n1.69\n2.71\n1317.19\n0.40\n0.88\n157.0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ntrain_df, val_df, test_df = normalizacion_datos(train_df, val_df, test_df)\ntrain_df.describe()\n\n\n    \n\n\n\n\n\n\np (mbar)\nT (degC)\nTpot (K)\nTdew (degC)\nrh (%)\nVPmax (mbar)\nVPact (mbar)\nVPdef (mbar)\nsh (g/kg)\nH2OC (mmol/mol)\nrho (g/m**3)\nwv (m/s)\nmax. wv (m/s)\nwd (deg)\n\n\n\n\ncount\n4.906300e+04\n4.906300e+04\n4.906300e+04\n4.906300e+04\n4.906300e+04\n49063.000000\n4.906300e+04\n49063.000000\n4.906300e+04\n4.906300e+04\n4.906300e+04\n4.906300e+04\n4.906300e+04\n4.906300e+04\n\n\nmean\n2.027515e-16\n-1.946415e-16\n9.685730e-16\n1.853728e-17\n-6.719765e-16\n0.000000\n-2.270817e-16\n0.000000\n-1.506154e-16\n1.807385e-16\n-2.321795e-15\n1.540912e-16\n-1.616219e-16\n-2.178131e-16\n\n\nstd\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000\n1.000000e+00\n1.000000\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n\n\nmin\n-9.045695e+00\n-3.682079e+00\n-3.707266e+00\n-4.216645e+00\n-3.746587e+00\n-1.609554\n-2.030996e+00\n-0.829861\n-2.022853e+00\n-2.031986e+00\n-3.846513e+00\n-1.403684e+00\n-1.535423e+00\n-1.977937e+00\n\n\n25%\n-6.093840e-01\n-7.069026e-01\n-6.939982e-01\n-6.697392e-01\n-6.581569e-01\n-0.750526\n-7.786971e-01\n-0.657581\n-7.762466e-01\n-7.761335e-01\n-7.116941e-01\n-7.390786e-01\n-7.595612e-01\n-6.326198e-01\n\n\n50%\n5.467421e-02\n9.450477e-03\n1.318575e-02\n5.168967e-02\n1.989686e-01\n-0.222892\n-1.561120e-01\n-0.383594\n-1.548152e-01\n-1.540757e-01\n-7.847992e-02\n-2.308512e-01\n-2.250786e-01\n2.674928e-01\n\n\n75%\n6.548575e-01\n7.200265e-01\n7.123465e-01\n7.530390e-01\n8.150841e-01\n0.533469\n6.684569e-01\n0.268164\n6.650251e-01\n6.651626e-01\n6.442168e-01\n4.793641e-01\n5.206108e-01\n6.932912e-01\n\n\nmax\n2.913378e+00\n3.066661e+00\n3.041354e+00\n2.647686e+00\n1.455361e+00\n5.846190\n4.489514e+00\n7.842254\n4.550843e+00\n4.524268e+00\n4.310438e+00\n7.724863e+00\n8.593884e+00\n2.131645e+00"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2b_Resolviendo_desafios_con_RNNs_usando_keras.html#creación-de-ventanas",
    "href": "semana_2/notebooks/Nb_2b_Resolviendo_desafios_con_RNNs_usando_keras.html#creación-de-ventanas",
    "title": "Resolviendo diversidad de tareas con RNNs",
    "section": "Creación de ventanas",
    "text": "Creación de ventanas\nEjemplo para un problema que dado las últimas 6 mediciones de temperatura va a predecir la siguiente hora de temperatura.\n\n\n\nsplit_window.png\n\n\n\nEjemplo creación dataset ventanas\n\n# Crear un array de 15 minutos y 3 características (features), con valores consecutivos para facilitar el entendimiento\ndatae = np.arange(10 * 3).reshape(10, 3)\n\n# Crear un DataFrame para mostrar los datos de manera más clara\ndfe = pd.DataFrame(datae, columns=['Feature_1', 'Feature_2', 'Target'])\ndfe.index.name = 'Minuto'\n\n# Mostrar el DataFrame\ndfe\n\n\n    \n\n\n\n\n\n\nFeature_1\nFeature_2\nTarget\n\n\nMinuto\n\n\n\n\n\n\n\n0\n0\n1\n2\n\n\n1\n3\n4\n5\n\n\n2\n6\n7\n8\n\n\n3\n9\n10\n11\n\n\n4\n12\n13\n14\n\n\n5\n15\n16\n17\n\n\n6\n18\n19\n20\n\n\n7\n21\n22\n23\n\n\n8\n24\n25\n26\n\n\n9\n27\n28\n29\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n# La variable a predecir es la última columna (columna final)\ntargets = datae[2:, -1]  # Los targets son el valor de la columna final (variable a predecir) desplazados por 2 pasos\ntargets\n\narray([ 8, 11, 14, 17, 20, 23, 26, 29])\n\n\n\n# Crear el dataset de series temporales\nds = tf.keras.utils.timeseries_dataset_from_array(\n    data=datae[:-1],  # Todas las filas menos la última, porque no hay target para la última fila\n    targets=targets,  # Los valores a predecir son la columna final de la siguiente fila\n    sequence_length=2,  # Usamos secuencias de 6 minutos\n    sequence_stride=1,  # Stride de 1 para obtener todas las posibles ventanas\n    shuffle=False,  # Barajamos las secuencias\n    batch_size=2  # Agrupamos en lotes de 2 secuencias\n)\n\n\n# Mostrar los primeros 5 lotes de inputs y labels, con un formato mejorado\nfor batch_num, batch in enumerate(ds.take(5), 1):\n    inputs, labels = batch\n    print(f\"Batch {batch_num}:\")\n    print(f\"Inputs shape: {inputs.shape}\")\n    print(\"Inputs:\")\n\n    # Imprimir cada secuencia de inputs con su respectiva etiqueta al final\n    for i, (input_sequence, label) in enumerate(zip(inputs.numpy(), labels.numpy()), 1):\n        print(f\"  Sequence {i}:\")\n        print(f\" {input_sequence} -&gt; Target: {label}\")\n\n    print(\"\\n\" + \"-\"*50 + \"\\n\")\n\nBatch 1:\nInputs shape: (2, 2, 3)\nInputs:\n  Sequence 1:\n [[0 1 2]\n [3 4 5]] -&gt; Target: 8\n  Sequence 2:\n [[3 4 5]\n [6 7 8]] -&gt; Target: 11\n\n--------------------------------------------------\n\nBatch 2:\nInputs shape: (2, 2, 3)\nInputs:\n  Sequence 1:\n [[ 6  7  8]\n [ 9 10 11]] -&gt; Target: 14\n  Sequence 2:\n [[ 9 10 11]\n [12 13 14]] -&gt; Target: 17\n\n--------------------------------------------------\n\nBatch 3:\nInputs shape: (2, 2, 3)\nInputs:\n  Sequence 1:\n [[12 13 14]\n [15 16 17]] -&gt; Target: 20\n  Sequence 2:\n [[15 16 17]\n [18 19 20]] -&gt; Target: 23\n\n--------------------------------------------------\n\nBatch 4:\nInputs shape: (2, 2, 3)\nInputs:\n  Sequence 1:\n [[18 19 20]\n [21 22 23]] -&gt; Target: 26\n  Sequence 2:\n [[21 22 23]\n [24 25 26]] -&gt; Target: 29\n\n--------------------------------------------------\n\n\n\n\n\nRetomando\n\nMAX_EPOCHS = 20\n\ndef compile_and_fit(model, window, patience=2):\n  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                                    patience=patience,\n                                                    mode='min')\n\n  model.compile(loss=tf.keras.losses.MeanSquaredError(),\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[tf.keras.metrics.MeanAbsoluteError()])\n\n  history = model.fit(window.train, epochs=MAX_EPOCHS,\n                      validation_data=window.val,\n                      callbacks=[early_stopping])\n  return history\n\n\nWIDTH_TEMP = 6\nwindow = WindowGenerator(\n    input_width=WIDTH_TEMP,\n    label_width=1,\n    shift=1,\n    train_df = train_df,\n    val_df = val_df,\n    test_df=test_df,\n    label_columns=['T (degC)'])\n\nwindow\n\nTotal window size: 7\nInput indices: [0 1 2 3 4 5]\nLabel indices: [6]\nLabel column name(s): ['T (degC)']\n\n\n\n# Stack three slices, the length of the total window.\nexample_window = tf.stack([np.array(train_df[:window.total_window_size]),\n                           np.array(train_df[100:100+window.total_window_size]),\n                           np.array(train_df[200:200+window.total_window_size])])\n\nexample_inputs, example_labels = window.split_window(example_window)\n\nprint('All shapes are: (batch, time, features)')\nprint(f'Window shape: {example_window.shape}')\nprint(f'Inputs shape: {example_inputs.shape}')\nprint(f'Labels shape: {example_labels.shape}')\n\nAll shapes are: (batch, time, features)\nWindow shape: (3, 7, 14)\nInputs shape: (3, 6, 14)\nLabels shape: (3, 1, 1)\n\n\n\n# graficar los datos en las ventanas\nwindow.plot()\n\n\n\n\n\n\n\n\n\nlstm_model = tf.keras.models.Sequential([\n    # Shape [batch, time, features] =&gt; [batch, time, lstm_units]\n    tf.keras.layers.LSTM(32, return_sequences=True),\n    # Shape =&gt; [batch, time, features]\n    tf.keras.layers.Dense(units=1)\n])\n\n\nprint('Input shape:', window.example[0].shape)\nprint('Output shape:', lstm_model(window.example[0]).shape)\n\nInput shape: (32, 6, 14)\nOutput shape: (32, 6, 1)\n\n\n\nhistory = compile_and_fit(lstm_model, window)\n\nval_performance = {}\nperformance = {}\nval_performance['LSTM'] = lstm_model.evaluate(window.val, return_dict=True)\nperformance['LSTM'] = lstm_model.evaluate(window.test, verbose=0, return_dict=True)\n\n\nEpoch 1/20\n\n1534/1534 ━━━━━━━━━━━━━━━━━━━━ 13s 8ms/step - loss: 0.1702 - mean_absolute_error: 0.2857 - val_loss: 0.0806 - val_mean_absolute_error: 0.2000\n\nEpoch 2/20\n\n1534/1534 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - loss: 0.0796 - mean_absolute_error: 0.1978 - val_loss: 0.0805 - val_mean_absolute_error: 0.2008\n\nEpoch 3/20\n\n1534/1534 ━━━━━━━━━━━━━━━━━━━━ 10s 6ms/step - loss: 0.0759 - mean_absolute_error: 0.1912 - val_loss: 0.0750 - val_mean_absolute_error: 0.1902\n\nEpoch 4/20\n\n1534/1534 ━━━━━━━━━━━━━━━━━━━━ 13s 9ms/step - loss: 0.0727 - mean_absolute_error: 0.1861 - val_loss: 0.0742 - val_mean_absolute_error: 0.1871\n\nEpoch 5/20\n\n1534/1534 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - loss: 0.0720 - mean_absolute_error: 0.1845 - val_loss: 0.0732 - val_mean_absolute_error: 0.1869\n\nEpoch 6/20\n\n1534/1534 ━━━━━━━━━━━━━━━━━━━━ 12s 8ms/step - loss: 0.0705 - mean_absolute_error: 0.1822 - val_loss: 0.0728 - val_mean_absolute_error: 0.1857\n\nEpoch 7/20\n\n1534/1534 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - loss: 0.0698 - mean_absolute_error: 0.1810 - val_loss: 0.0732 - val_mean_absolute_error: 0.1864\n\nEpoch 8/20\n\n1534/1534 ━━━━━━━━━━━━━━━━━━━━ 11s 7ms/step - loss: 0.0693 - mean_absolute_error: 0.1804 - val_loss: 0.0721 - val_mean_absolute_error: 0.1836\n\nEpoch 9/20\n\n1534/1534 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - loss: 0.0690 - mean_absolute_error: 0.1797 - val_loss: 0.0725 - val_mean_absolute_error: 0.1850\n\nEpoch 10/20\n\n1534/1534 ━━━━━━━━━━━━━━━━━━━━ 19s 6ms/step - loss: 0.0687 - mean_absolute_error: 0.1794 - val_loss: 0.0728 - val_mean_absolute_error: 0.1851\n\n438/438 ━━━━━━━━━━━━━━━━━━━━ 2s 4ms/step - loss: 0.0729 - mean_absolute_error: 0.1852\n\n\n\n\n\nperformance\n\n{'LSTM': {'loss': 0.0691012442111969,\n  'mean_absolute_error': 0.18326018750667572}}\n\n\n\nval_performance\n\n{'LSTM': {'loss': 0.07278452813625336,\n  'mean_absolute_error': 0.1851218044757843}}"
  },
  {
    "objectID": "semana_2/index.html",
    "href": "semana_2/index.html",
    "title": "Semana 2: Arquitecturas de Redes Neuronales",
    "section": "",
    "text": "Expandimos nuestro conocimiento explorando diversas arquitecturas de redes neuronales y sus aplicaciones.\nTemas Clave:\n\nDiversas arquitecturas de redes neuronales (DNN, CNN, RNN).\nAplicaciones y ventajas de cada arquitectura.\nIntroducción a arquitecturas avanzadas (Transformers, GANs, Redes de Grafos).\n\nEnfoque Práctico: Aprenderás a identificar y seleccionar la arquitectura más adecuada para diferentes tipos de problemas. Implementarás modelos DNN, CNN y RNN en proyectos prácticos para experimentar con su desempeño.\nMateriales de la Semana:\n\nSlides de la Semana 2 (Próximamente)\nNotebook: Implementando una CNN usando Keras\nNotebook: Resolviendo desafíos con RNNs usando Keras"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html",
    "href": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html",
    "title": "Implementado un Perceptrón multi-capa usando frameworks y MLFlow",
    "section": "",
    "text": "Open In Colab\n#@title Importar librerías\n#importar librerías necesarias\nimport json\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Input\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.datasets import mnist\n\n!pip install mlflow --quiet\n\nimport mlflow\nimport mlflow.sklearn\nimport mlflow.tensorflow\nfrom mlflow.tracking import MlflowClient\n\n\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.7/26.7 MB 19.7 MB/s eta 0:00:00\n\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.7/5.7 MB 32.5 MB/s eta 0:00:00\n\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 233.5/233.5 kB 8.5 MB/s eta 0:00:00\n\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.8/147.8 kB 5.6 MB/s eta 0:00:00\n\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.7/114.7 kB 3.8 MB/s eta 0:00:00\n\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.0/85.0 kB 2.3 MB/s eta 0:00:00\n\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 569.1/569.1 kB 10.8 MB/s eta 0:00:00\n\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 203.2/203.2 kB 3.6 MB/s eta 0:00:00\n\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.6/78.6 kB 4.4 MB/s eta 0:00:00\n#@title Configurando MLFlow\n# configurar que el servidor de trackin sea localhost con una BD sqlite como el almacenamiento para almacenar el proceso de tracking\n\nlocal_registry = \"sqlite:///mlruns.db\"\nprint(f\"Ejecutando registro en modo local={local_registry}\")\nmlflow.set_tracking_uri(local_registry)\n\nEjecutando registro en modo local=sqlite:///mlruns.db\n#@title Funciones complementarias\ndef plot_samples_dataset(X, y):\n    # Convertir las etiquetas a enteros\n    y = y.astype(int)\n\n    # Crear la grilla de 4x4\n    fig, axes = plt.subplots(4, 4, figsize=(6, 6))\n    fig.suptitle('Grilla de imágenes del dataset MNIST')\n\n    # Iterar para mostrar las primeras 16 imágenes con sus etiquetas\n    for i, ax in enumerate(axes.flat):\n        img = X.iloc[i].values.reshape(28, 28)\n        label = y[i]\n        ax.imshow(img, cmap='gray')\n        ax.set_title(f'Label: {label}')\n        ax.axis('off')\n\n    plt.show()\n\ndef plot_curva_aprendizaje(mlp):\n    plt.figure(figsize=(8, 5))\n    plt.plot(mlp.loss_curve_, marker='o')\n    plt.title('Pérdida durante el entrenamiento del MLP por iteración')\n    plt.xlabel('Iteración')\n    plt.ylabel('Pérdida (Loss)')\n    plt.grid()\n    plt.show()\n\ndef plot_matriz_confusion(cm, nombre):\n    # Visualizar la matriz de confusión usando Seaborn\n    plt.figure(figsize=(10, 7))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n    plt.xlabel('Etiqueta predicha')\n    plt.ylabel('Etiqueta real')\n    plt.title('Matriz de Confusión para el MLP en el dataset MNIST')\n    plt.show()\n    plt.savefig(f\"{nombre}.png\")\n\ndef encontrar_dim_imagen(n_neurons):\n    \"\"\"\n    Encuentra la mejor forma cuadrada (filas, columnas) para una cantidad dada de neuronas.\n    \"\"\"\n    side_length = int(np.sqrt(n_neurons))  # Calcular la raíz cuadrada del número de neuronas\n    if side_length * side_length == n_neurons:\n        return (side_length, side_length)  # Si es un cuadrado perfecto\n    else:\n        # Si no es un cuadrado perfecto, buscamos la mejor aproximación (filas, columnas)\n        for i in range(side_length, 0, -1):\n            if n_neurons % i == 0:\n                return (i, n_neurons // i)  # Devolver filas y columnas\n    return (n_neurons, 1)  # Si no encuentra, retornar en forma de vector (n_neurons, 1)\n\ndef visualizacion_pesos_mlp(mlp):\n    # Definir la figura con 3 filas y 5 columnas\n    fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n\n    # Asignar las dimensiones para visualizar cada capa\n    layer_shapes = [encontrar_dim_imagen(layer.shape[0]) for layer in mlp.coefs_]\n\n    # Recorrer cada capa de coeficientes del MLP\n    for layer_index, (layer_coefs, ax_row) in enumerate(zip(mlp.coefs_, axes)):\n        # Seleccionar aleatoriamente 5 neuronas de la capa actual\n        num_neurons = layer_coefs.shape[1]\n        random_neurons = random.sample(range(num_neurons), 5)\n\n        # Obtener la forma de visualización para esta capa\n        layer_shape = layer_shapes[layer_index]\n\n        vmin, vmax = layer_coefs.min(), layer_coefs.max()\n\n        # Visualizar las neuronas seleccionadas\n        for neuron_index, ax in zip(random_neurons, ax_row):\n            # Seleccionar los pesos de la neurona específica y reestructurarlos en una matriz 2D\n            neuron_weights = layer_coefs[:, neuron_index].reshape(layer_shape)\n            # Dibujar la imagen de los pesos de la neurona\n            ax.matshow(neuron_weights, cmap=plt.cm.gray, vmin=0.5 * vmin, vmax=0.5 * vmax)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.set_title(f'Capa {layer_index+1}, Neurona {neuron_index}')\n\n    plt.suptitle('Visualización de Pesos de las Neuronas en las Capas Ocultas')\n    plt.tight_layout()\n    plt.show()\n\ndef plot_loss_historia_keras(history):\n    # Graficar el histórico de pérdida durante el entrenamiento\n    plt.plot(history.history['loss'], label='Pérdida de Entrenamiento')\n    plt.plot(history.history['val_loss'], label='Pérdida de Validación')\n    plt.title('Pérdida durante el Entrenamiento')\n    plt.xlabel('Época')\n    plt.ylabel('Pérdida')\n    plt.legend()\n    plt.show()\n\ndef plot_acc_historia_keras(history):\n    # Graficar la precisión durante el entrenamiento\n    plt.plot(history.history['accuracy'], label='Precisión de Entrenamiento')\n    plt.plot(history.history['val_accuracy'], label='Precisión de Validación')\n    plt.title('Precisión durante el Entrenamiento')\n    plt.xlabel('Época')\n    plt.ylabel('Precisión')\n    plt.legend()\n    plt.show()\n\ndef visualizacion_pesos_mlp_keras(model):\n    # Obtener los pesos del modelo (par de listas [pesos, biases] para cada capa)\n    weights = model.get_weights()\n\n    # Extraer solo los pesos de cada capa oculta, ignorando los bias\n    layer_weights = [weights[i] for i in range(0, len(weights), 2)]  # Solo los pesos, no los sesgos\n\n    # Definir la figura con 3 filas (una por cada capa) y 5 columnas (5 neuronas al azar)\n    fig, axes = plt.subplots(len(layer_weights), 5, figsize=(15, 9))\n\n    # Calcular las formas de cada capa de manera dinámica\n    layer_shapes = [encontrar_dim_imagen(layer.shape[0]) for layer in layer_weights]\n\n    # Recorrer cada capa y sus pesos\n    for layer_index, (layer_coefs, ax_row) in enumerate(zip(layer_weights, axes)):\n        # Seleccionar aleatoriamente 5 neuronas de la capa actual\n        num_neurons = layer_coefs.shape[1]\n        random_neurons = random.sample(range(num_neurons), 5)\n\n        # Obtener la forma de visualización para esta capa\n        layer_shape = layer_shapes[layer_index]\n        vmin, vmax = layer_coefs.min(), layer_coefs.max()\n\n\n        # Visualizar las neuronas seleccionadas\n        for neuron_index, ax in zip(random_neurons, ax_row):\n            # Seleccionar los pesos de la neurona específica y reestructurarlos en una matriz 2D\n            neuron_weights = layer_coefs[:, neuron_index].reshape(layer_shape)\n            # Dibujar la imagen de los pesos de la neurona\n            ax.matshow(neuron_weights, cmap=plt.cm.gray, vmin=0.5 * vmin, vmax=0.5 * vmax)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.set_title(f'Capa {layer_index+1}, Neurona {neuron_index}')\n\n    plt.suptitle('Visualización de Pesos de las Neuronas en las Capas Ocultas de Keras')\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html#evaluación-completa",
    "href": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html#evaluación-completa",
    "title": "Implementado un Perceptrón multi-capa usando frameworks y MLFlow",
    "section": "Evaluación completa",
    "text": "Evaluación completa\nRealizaremos una evaluación completa revisando el rendimiento en ambos conjuntos, seguidamente generaremos el reporte de clasificación y la matriz de confusión.\n\nprint(f\"Training set score: {mlp.score(X_train, y_train):.3f}\")\nprint(f\"Test set score: {mlp.score(X_test, y_test):.3f}\")\n\nTraining set score: 1.000\nTest set score: 0.982\n\n\n\n# re abrir un run anterior para registrar más datos\nmlflow.start_run(run_id=run_id)\n\n# Realizar predicciones\ny_pred = mlp.predict(X_test)\n\n# Imprimir el reporte de métricas\nprint(\"Reporte de Clasificación del MLP en MNIST:\\n\")\nreport = classification_report(y_test, y_pred)\nprint(report)\n\n# registrar artefacto\nwith open(\"classification_report.json\", \"w\") as f:\n    json.dump(report, f)\nmlflow.log_artifact(\"classification_report.json\")\n\n# Generar la matriz de confusión\ncm = confusion_matrix(y_test, y_pred)\n\n# visualizar la matriz de confusión\nplot_matriz_confusion(cm, nombre='confusion_matrix')\n# Guardar la visualización como imagen y registrarla en MLflow\nmlflow.log_artifact(\"confusion_matrix.png\")\n\n# Finalizar el `run` si ya no se va a registrar nada más\nmlflow.end_run()\n\nReporte de Clasificación del MLP en MNIST:\n\n              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99      2058\n           1       0.99      0.99      0.99      2364\n           2       0.98      0.98      0.98      2133\n           3       0.98      0.98      0.98      2176\n           4       0.98      0.98      0.98      1936\n           5       0.98      0.98      0.98      1915\n           6       0.98      0.99      0.99      2088\n           7       0.98      0.98      0.98      2248\n           8       0.98      0.97      0.97      1992\n           9       0.98      0.97      0.98      2090\n\n    accuracy                           0.98     21000\n   macro avg       0.98      0.98      0.98     21000\nweighted avg       0.98      0.98      0.98     21000\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\nEjecutar MLFlow UI\n\n# Ejecutar tracking UI en background\nget_ipython().system_raw(\"mlflow ui --backend-store-uri sqlite:///mlruns.db --port 5000 &\")\n\n\n# Crear un tunel remoto usando ngrok.com\n!pip install -U pyngrok --quiet\nfrom pyngrok import ngrok\n\n# Terminate open tunnels if exist\nngrok.kill()\n\n# Colocar el token\n# Coloque su authtoken obtenido de https://dashboard.ngrok.com/auth\nNGROK_AUTH_TOKEN = \"2oR7ctXTRQ7kJl8csoQXXoxsb6V_72fsChgD1kFQN5MkVr7U3\"\nngrok.set_auth_token(NGROK_AUTH_TOKEN)\n\n# Abrir el tunel http en el puerto 5000 para http://localhost:5000\npublic_url = ngrok.connect(\"http://localhost:5000\", proto='http')\nprint(\"MLflow Tracking UI:\", public_url)\n\nMLflow Tracking UI: NgrokTunnel: \"https://3ec3-34-125-33-68.ngrok-free.app\" -&gt; \"http://localhost:5000\""
  },
  {
    "objectID": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html#visualización-de-pesos",
    "href": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html#visualización-de-pesos",
    "title": "Implementado un Perceptrón multi-capa usando frameworks y MLFlow",
    "section": "Visualización de pesos",
    "text": "Visualización de pesos\n\nvisualizacion_pesos_mlp(mlp)\n\n\n\n\n\n\n\n\n\nTutoriales relacionados\n\nAnálisis de la variación del parametro de regularización alpha\nComparación de las diferentes estrategias de aprendizaje"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html#evaluación-completa-1",
    "href": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html#evaluación-completa-1",
    "title": "Implementado un Perceptrón multi-capa usando frameworks y MLFlow",
    "section": "Evaluación completa",
    "text": "Evaluación completa\n\ny_test_categorical = to_categorical(y_test)\n\nscore = mlp_keras.evaluate(X_test.values.astype(float), y_test_categorical, batch_size=128)\n\nscore\n\n\n165/165 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - accuracy: 0.9481 - loss: 0.1761\n\n\n\n\n[0.17650671303272247, 0.9479047656059265]\n\n\n\n# Realizar predicciones en el conjunto de prueba\ny_pred = mlp_keras.predict(X_test.values.astype(float))\n\n# Convertir las predicciones en etiquetas (la clase con mayor probabilidad)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true = y_test.values.astype(int)  # Las etiquetas reales del conjunto de prueba\n\n\n657/657 ━━━━━━━━━━━━━━━━━━━━ 4s 6ms/step\n\n\n\n\n\n# Generar el reporte de clasificación\nprint(\"Reporte de Clasificación para el MLP en MNIST:\\n\")\nprint(classification_report(y_true, y_pred_classes))\n\n# Crear la matriz de confusión\ncm = confusion_matrix(y_true, y_pred_classes)\n\n# Visualizar la matriz de confusión usando Seaborn\nplot_matriz_confusion(cm)\n\nReporte de Clasificación para el MLP en MNIST:\n\n              precision    recall  f1-score   support\n\n           0       0.96      0.97      0.97      2058\n           1       0.96      0.98      0.97      2364\n           2       0.96      0.93      0.95      2133\n           3       0.93      0.93      0.93      2176\n           4       0.94      0.96      0.95      1936\n           5       0.95      0.93      0.94      1915\n           6       0.96      0.97      0.96      2088\n           7       0.95      0.95      0.95      2248\n           8       0.95      0.92      0.93      1992\n           9       0.93      0.93      0.93      2090\n\n    accuracy                           0.95     21000\n   macro avg       0.95      0.95      0.95     21000\nweighted avg       0.95      0.95      0.95     21000"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html#visualización-de-pesos-1",
    "href": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html#visualización-de-pesos-1",
    "title": "Implementado un Perceptrón multi-capa usando frameworks y MLFlow",
    "section": "Visualización de pesos",
    "text": "Visualización de pesos\n\nvisualizacion_pesos_mlp_keras(mlp_keras)"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html#actividad-extra-clase",
    "href": "semana_1/notebooks/Nb_1c_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks_MLFlow.html#actividad-extra-clase",
    "title": "Implementado un Perceptrón multi-capa usando frameworks y MLFlow",
    "section": "Actividad extra clase",
    "text": "Actividad extra clase\nUsando la siguiente documentación: https://mlflow.org/docs/latest/deep-learning/tensorflow/guide/index.html realizar un seguimiento y registro del MLP construido en keras/tensorflow."
  },
  {
    "objectID": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html",
    "href": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html",
    "title": "Implementando un Perceptrón Multi-capa (MLP) desde CERO usando python",
    "section": "",
    "text": "Open In Colab\n#@title Importar librerías\n#importar librerías necesarias\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_moons\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\n#@title Funciones complementarias\ndef plot_dataset(X_train, y_train, X_test, y_test):\n    # Tamaño de paso en la grilla de valores\n    # (para la visualización del espacio de características)\n    h = 0.02\n\n    # Definir los límites del gráfico en el eje x e y basados\n    # en los datos de entrenamiento\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n\n    # Crear una malla de puntos para cubrir el espacio de características\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n    # Creación del lienzo para visualizar los datos\n    fig, ax = plt.subplots(1,1, figsize=(8, 5))\n\n    # Agregar titulo a la grafica\n    ax.set_title(\"Dataset linealmente no separable\")\n\n    # Agregar nombres a cada eje de caracteristica\n    ax.set_xlabel(\"Característica x_1\")\n    ax.set_ylabel(\"Característica x_2\")\n\n    # Puntos de entrenamiento\n    ax.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1],\n               c=\"#FF0000\", edgecolors=\"k\", label='Clase de entrenamiento 1')\n    ax.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1],\n               c=\"#0000FF\", edgecolors=\"k\", label='Clase de entrenamiento 2')\n\n    # Puntos de prueba\n    ax.scatter(X_test[y_test == 0, 0], X_test[y_test == 0, 1],\n               c=\"#FF0000\", edgecolors=\"k\", alpha=0.6, label='Clase de prueba 1')\n    ax.scatter(X_test[y_test == 1, 0], X_test[y_test == 1, 1],\n               c=\"#0000FF\", edgecolors=\"k\", alpha=0.6, label='Clase de prueba 2')\n\n    # Establecer los límites del gráfico para asegurar que todos los puntos sean visibles\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n\n    # Eliminar las marcas en los ejes x e y para un gráfico más limpio\n    ax.set_xticks(())\n    ax.set_yticks(())\n\n    # Añadir una leyenda para identificar las clases de los\n    # puntos de entrenamiento y prueba\n    ax.legend()\n\n    # mostrar el grafico\n    plt.show()\n\ndef plot_decision_boundary(mlp, X, y, h=0.02):\n    # Crear una malla de puntos para el espacio de características\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # por cada punto de la grilla, hacer una predicción del MLP\n    Z = np.array([mlp.prediccion([np.array([xx.ravel()[i], yy.ravel()[i]])])\n                  for i in range(len(xx.ravel()))])\n\n    # redimensionar para que tenga el mismo shape de la grilla\n    Z = Z.reshape(xx.shape)\n\n    # crear una figura de dos subplots\n    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n\n    # Graficar los puntos originales\n    ax[0].set_title('Puntos originales')\n    ax[0].scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu,\n                  edgecolors='k', alpha=0.6)\n\n    # Graficar los puntos de entrenamiento\n    ax[1].set_title('Frontera de decisión generada por el MLP')\n    ax[1].scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.RdBu)\n    # Graficar la frontera de decisión con un contorno\n    ax[1].contourf(xx, yy, Z, alpha=0.6, cmap=plt.cm.RdBu)\n\n    # mejorar la visualización\n    for i in range(2):\n        ax[i].set_xlim(xx.min(), xx.max())\n        ax[i].set_ylim(yy.min(), yy.max())\n        ax[i].set_xticks(())\n        ax[i].set_yticks(())\n        ax[i].set_xlabel(\"Característica x_1\")\n        ax[i].set_ylabel(\"Característica x_2\")\n\n    plt.show()\n\ndef plot_cost_history(costo_historia):\n    # Grafica el cambio del costo en el entrenamiento\n    plt.figure(figsize=(8, 4))\n    plt.plot(costo_historia, label='Costo')\n    plt.title('Historia del Costo')\n    plt.xlabel('Épocas')\n    plt.ylabel('Costo')\n    plt.legend()\n    plt.grid(True)\n    plt.show()"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#inicialización",
    "href": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#inicialización",
    "title": "Implementando un Perceptrón Multi-capa (MLP) desde CERO usando python",
    "section": "1. Inicialización",
    "text": "1. Inicialización\nAsignación de atributos e inicialización de pesos y biases\n\nclass PerceptronMulticapa():\n    def __init__(self, params=None):\n        # Asignación de hiperparámetros\n        self.capa_entrada = params['capa_entrada']\n        self.capa_oculta = params['capa_oculta']\n        self.capa_salida = params['capa_salida']\n        self.epochs = params['epochs']\n        self.lr = params['lr']\n        self.relu = (lambda x: x*(x &gt; 0))\n        self.derivada_relu = (lambda x: 1 * (x&gt;0))\n        self.sigmoide = (lambda x: 1/(1 + np.exp(-x)))\n        self.derivada_sigmoide = (lambda x: x*(1-x))\n\n        # inicialización de pesos y bias\n        self.inicializacion()\n\n    def inicializacion(self):\n        # inicialización de pesos y bias aleatoria\n        np.random.seed(42) # fijar una semilla para reproducir resultados\n\n        # Capa Oculta\n        self.pesos_capa_oculta = np.random.rand(self.capa_oculta, self.capa_entrada)\n        self.bias_capa_oculta = np.ones((self.capa_oculta, 1))\n\n        # Capa de salida\n        self.pesos_capa_salida = np.random.rand(self.capa_salida, self.capa_oculta)\n        self.bias_capa_salida = np.ones((self.capa_salida, 1))\n\n\n# Instanciamos nuestro perceptrón multicapa\nmlp =  PerceptronMulticapa(params)\n\n\nprint('Dimensión pesos capa oculta: {}'.format(mlp.pesos_capa_oculta.shape))\nprint('Dimensión biases capa oculta: {}'.format(mlp.bias_capa_oculta.shape))\nprint('Dimensión pesos capa salida: {}'.format(mlp.pesos_capa_salida.shape))\nprint('Dimensión bias capa salida: {}'.format(mlp.bias_capa_salida.shape))\n\nDimensión pesos capa oculta: (50, 2)\nDimensión biases capa oculta: (50, 1)\nDimensión pesos capa salida: (1, 50)\nDimensión bias capa salida: (1, 1)\n\n\n\n# ejemplo pesos capa salida\nmlp.pesos_capa_salida\n\narray([[0.03142919, 0.63641041, 0.31435598, 0.50857069, 0.90756647,\n        0.24929223, 0.41038292, 0.75555114, 0.22879817, 0.07697991,\n        0.28975145, 0.16122129, 0.92969765, 0.80812038, 0.63340376,\n        0.87146059, 0.80367208, 0.18657006, 0.892559  , 0.53934224,\n        0.80744016, 0.8960913 , 0.31800347, 0.11005192, 0.22793516,\n        0.42710779, 0.81801477, 0.86073058, 0.00695213, 0.5107473 ,\n        0.417411  , 0.22210781, 0.11986537, 0.33761517, 0.9429097 ,\n        0.32320293, 0.51879062, 0.70301896, 0.3636296 , 0.97178208,\n        0.96244729, 0.2517823 , 0.49724851, 0.30087831, 0.28484049,\n        0.03688695, 0.60956433, 0.50267902, 0.05147875, 0.27864646]])"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#propagación-hacia-adelante-forward",
    "href": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#propagación-hacia-adelante-forward",
    "title": "Implementando un Perceptrón Multi-capa (MLP) desde CERO usando python",
    "section": "2. Propagación hacia adelante (forward)",
    "text": "2. Propagación hacia adelante (forward)\nMétodo de propagación hacia adelante\n\ndef forward_pass(self, x):\n    # Realizar la operacion Wx + b de la capa oculta, x = x_0\n    z = np.matmul(self.pesos_capa_oculta, x) + self.bias_capa_oculta\n    # Aplicar función de activación\n    h = self.relu(z) # z = x_1, h = x_2\n\n    # Aplicar la operación Wh + b para generar la salida, y = x_3\n    y = np.matmul(self.pesos_capa_salida, h) + self.bias_capa_salida\n    # Aplicar función de activación softmax para la clasificación\n    y_pred = self.sigmoide(y) # y = x_4\n\n    return z, h, y_pred\n\n\n# Añadimos nuestro nuevo método\nsetattr(PerceptronMulticapa, 'forward_pass', forward_pass)\n\n\n# seleccionamos una muestra del dataset\n# por ser solo uno se redimensiona para que tenga la estructura de entrada propia\nx_i = X_train[0,:].reshape((-1, 1))\nz, h, y_pred = mlp.forward_pass(x_i)\n\n\nprint('Dimensión biases capa oculta: {}'.format(z.shape))\nprint('Dimensión de la capa oculta: {}'.format(h.shape))\nprint('Predicción: {}'.format(y_pred))\nprint('Capa oculta: {}'.format(h))\n\nDimensión biases capa oculta: (50, 1)\nDimensión de la capa oculta: (50, 1)\nPredicción: [[1.]]\nCapa oculta: [[0.72633077]\n [1.01761811]\n [0.99129875]\n [0.64226347]\n [0.91951467]\n [0.58240149]\n [1.22554416]\n [0.98915428]\n [0.88627666]\n [1.03760999]\n [1.17304912]\n [0.95112959]\n [0.83016191]\n [0.85085938]\n [1.20642355]\n [1.1577875 ]\n [0.60864823]\n [1.01505623]\n [1.07377182]\n [1.06886654]\n [0.82949748]\n [0.61426555]\n [0.80843687]\n [0.89119274]\n [1.12821114]\n [1.03116189]\n [0.96713639]\n [0.82449353]\n [0.94790496]\n [0.87459926]\n [1.02976633]\n [1.1607742 ]\n [0.86948377]\n [0.70204454]\n [0.59561443]\n [1.20847428]\n [0.64438828]\n [0.95081357]\n [1.26279179]\n [1.08640592]\n [1.05700326]\n [1.09879949]\n [0.97640557]\n [0.99963981]\n [1.13251031]\n [0.73290002]\n [1.0450389 ]\n [1.0785398 ]\n [1.0125701 ]\n [0.96240177]]"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#función-para-calcular-el-error",
    "href": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#función-para-calcular-el-error",
    "title": "Implementando un Perceptrón Multi-capa (MLP) desde CERO usando python",
    "section": "3. Función para calcular el error",
    "text": "3. Función para calcular el error\nMétodo que permite conocer el error de una predicción con respecto a la etiqueta real.\n\ndef calcular_perdida_entropia_cruzada(self, y_real, y_pred):\n    epsilon = 1e-12\n    # asegura que los valores de las predicciones esten en un rango\n    # seguro para evitar logaritmos de 0 y 1\n    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n    # calculo de la perdida\n    perdida = -(((1 - y_real) * np.log(1 - y_pred + epsilon)) + (y_real * np.log(y_pred + epsilon)))\n\n    return perdida\n\n\n# Añadimos nuestro nuevo método\nsetattr(PerceptronMulticapa, 'calcular_perdida_entropia_cruzada', calcular_perdida_entropia_cruzada)\n\n\n# Probamos nuestra función de error\ny_real = y_train[0]\nerror = mlp.calcular_perdida_entropia_cruzada(y_real, y_pred)\nprint(f'Error: {error}')\n\nError: [[6.67165212e-11]]"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#propagación-hacía-atrás-backward",
    "href": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#propagación-hacía-atrás-backward",
    "title": "Implementando un Perceptrón Multi-capa (MLP) desde CERO usando python",
    "section": "4. Propagación hacía atrás (backward)",
    "text": "4. Propagación hacía atrás (backward)\nMétodo para propagar los errores hacía atrás.\n\ndef backward_pass(self, x, z, y_real, h, y_pred):\n    # Propagación de error en la capa de salida\n    # Calculo de error en la capa de salida g_out\n    #error_salida =  (y_pred - y_real) * self.derivada_sigmoide(y_pred)\n    error_salida =  y_pred - y_real\n\n    # gradiente de los pesos respecto a la capa de salida\n    # X_in * g_out = error_salida * h.T\n    # X_in = h es la entrada a la capa de salida\n    self.gradiente_pesos_capa_salida = np.matmul(error_salida, h.T)\n    # gradiente de los bias respecto a la capa de salida\n    self.gradiente_bias_capa_salida = error_salida\n\n    # Propagación de error en la capa oculta\n    # gradiente respecto a la capa oculta\n    # (g_out * W) * relu'(X_in)\n    # X_in en esta capa es la salida de aplicar la primera transformación\n    error_oculta = np.matmul(self.pesos_capa_salida.T, error_salida) * self.derivada_relu(z)\n    # gradientes con respecto a la capa oculta, de nuevo g_out * X_in\n    self.gradiente_pesos_capa_oculta = np.matmul(error_oculta, x.T)\n    self.gradiente_bias_capa_oculta = error_oculta\n\n\n# Añadimos nuestro nuevo método\nsetattr(PerceptronMulticapa, 'backward_pass', backward_pass)\n\n\n# calcular propagación de errores\nmlp.backward_pass(x_i, z, y_real, h, y_pred)\n\n\nprint('Dimensión gradientes capa oculta: {}'.format(mlp.gradiente_pesos_capa_oculta.shape))\nprint('Gradientes capa oculta: {}'.format(mlp.gradiente_pesos_capa_oculta))\n\nDimensión gradientes capa oculta: (50, 2)\nGradientes capa oculta: [[-8.14789923e-13  9.33629314e-13]\n [-1.64987027e-11  1.89050846e-11]\n [-8.14956162e-12  9.33819800e-12]\n [-1.31845056e-11  1.51075026e-11]\n [-2.35283225e-11  2.69599942e-11]\n [-6.46280811e-12  7.40542677e-12]\n [-1.06390243e-11  1.21907558e-11]\n [-1.95873816e-11  2.24442561e-11]\n [-5.93150715e-12  6.79663407e-12]\n [-1.99567547e-12  2.28675032e-12]\n [-7.51169840e-12  8.60730064e-12]\n [-4.17960177e-12  4.78920839e-12]\n [-2.41020651e-11  2.76174187e-11]\n [-2.09502196e-11  2.40058678e-11]\n [-1.64207563e-11  1.88157695e-11]\n [-2.25922909e-11  2.58874398e-11]\n [-2.08348990e-11  2.38737273e-11]\n [-4.83675923e-12  5.54221410e-12]\n [-2.31392593e-11  2.65141851e-11]\n [-1.39822466e-11  1.60215964e-11]\n [-2.09325850e-11  2.39856612e-11]\n [-2.32308329e-11  2.66191149e-11]\n [-8.24412154e-12  9.44654975e-12]\n [-2.85305512e-12  3.26918119e-12]\n [-5.90913412e-12  6.77099787e-12]\n [-1.10726102e-11  1.26875814e-11]\n [-2.12067279e-11  2.42997885e-11]\n [-2.23141195e-11  2.55686963e-11]\n [-1.80231392e-13  2.06518646e-13]\n [-1.32409334e-11  1.51721606e-11]\n [-1.08212247e-11  1.23995306e-11]\n [-5.75806219e-12  6.59789168e-12]\n [-3.10746496e-12  3.56069743e-12]\n [-8.75254746e-12  1.00291310e-11]\n [-2.44445826e-11  2.80098934e-11]\n [-8.37891553e-12  9.60100382e-12]\n [-1.34494535e-11  1.54110939e-11]\n [-1.82255044e-11  2.08837453e-11]\n [-9.42696189e-12  1.08019107e-11]\n [-2.51930882e-11  2.88675707e-11]\n [-2.49510873e-11  2.85902733e-11]\n [-6.52736216e-12  7.47939620e-12]\n [-1.28909822e-11  1.47711680e-11]\n [-7.80015802e-12  8.93783290e-12]\n [-7.38438363e-12  8.46141666e-12]\n [-9.56280359e-13  1.09575653e-12]\n [-1.58027281e-11  1.81076003e-11]\n [-1.30317663e-11  1.49324859e-11]\n [-1.33456744e-12  1.52921783e-12]\n [-7.22380571e-12  8.27741800e-12]]"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#entrenamiento",
    "href": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#entrenamiento",
    "title": "Implementando un Perceptrón Multi-capa (MLP) desde CERO usando python",
    "section": "5. Entrenamiento",
    "text": "5. Entrenamiento\nMétodo para iterar sobre todo el conjunto de datos y entrenar la red. Antes se deberá generar otro método que haga la respectiva actualización de pesos, una vez ya los gradientes son calculados.\n\ndef actualizar_pesos(self):\n    # actualizar pesos aplicando gradiente descendiente\n    self.pesos_capa_salida -= self.lr * self.gradiente_pesos_capa_salida\n    self.bias_capa_salida -= self.lr * self.gradiente_bias_capa_salida\n    self.pesos_capa_oculta -= self.lr * self.gradiente_pesos_capa_oculta\n    self.bias_capa_oculta -= self.lr * self.gradiente_bias_capa_oculta\n\ndef entrenar(self, X, y):\n    # almacenar el costo de cada iteración\n    self.costo_historia = []\n\n    # iterar sobre el número de épocas\n    for iteracion in tqdm(range(self.epochs), desc='Iteraciones'):\n        # iterar sobre los datos de entrenamiento\n        error_total = 0 # error para la iteración i\n        # iterar sobre todo el conjunto de datos\n        for i, (x_i, y_i) in tqdm(enumerate(zip(X,y)), desc='Datos', leave=False):\n            # asegurar de que la entrada y la salida solo tenga una columna\n            x_i = x_i.reshape(-1, 1)\n            y_i = y_i.reshape(-1, 1)\n            # aplicar propagación hacia adelante\n            z, h, y_pred = self.forward_pass(x_i)\n            # calcular la perdida de entropia cruzada\n            perdida = self.calcular_perdida_entropia_cruzada(y_i, y_pred)\n            error_total += perdida\n            # aplicar propagación hacia atras\n            self.backward_pass(x_i, z,  y_i, h, y_pred)\n            # actualizar pesos y bias usando gradiente descendiente\n            self.actualizar_pesos()\n\n        # almacenar los costos de cada iteración\n        self.costo = error_total / len(X)\n        self.costo_historia.append(self.costo)\n\n    #print(f'costo: {self.costo[0][0]} en la iteración: {iteracion}')\n\n\n# Añadimos nuestros nuevos métodos\nsetattr(PerceptronMulticapa, 'actualizar_pesos', actualizar_pesos)\nsetattr(PerceptronMulticapa, 'entrenar', entrenar)\n\n\n# realizamos el respectivo entrenamiento\nmlp.entrenar(X_train, y_train)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint(f'Costo final: {mlp.costo}')\n\nCosto final: [[0.44965122]]\n\n\n\n# Visualizar el cambio del costo durante el entrenamiento\nplot_cost_history(np.array(mlp.costo_historia).ravel())"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#evaluación",
    "href": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#evaluación",
    "title": "Implementando un Perceptrón Multi-capa (MLP) desde CERO usando python",
    "section": "Evaluación",
    "text": "Evaluación\nPara evaluar nuestro modelo con la información de test, creamos primero la función de predicción y seguidamente evaluamos el rendimiento en test.\n\ndef prediccion(self, X):\n    predicciones = []\n    for x_i in X:\n        x_i = x_i.reshape(-1, 1)  # Asegurar que x_i sea una columna\n        z, _, y_pred = self.forward_pass(x_i)\n        predicciones.append(y_pred)\n    return np.array(predicciones).flatten()\n\ndef evaluar(self, X, y, umbral):\n    # generar predicciones\n    y_pred = self.prediccion(X)\n    # convertir a 0 y 1 bajo un umbral\n    y_pred = np.where(y_pred &gt;= umbral, 1, 0)\n    # generar reporte de clasificación y matriz de confusión\n    print(classification_report(y, y_pred))\n    cm = confusion_matrix(y, y_pred)\n\n    # visualizacion de la matriz de confusion\n    ConfusionMatrixDisplay(cm).plot()\n    plt.show()\n\n    return y_pred, cm\n\n\n# Añadimos nuestros nuevos métodos\nsetattr(PerceptronMulticapa, 'prediccion', prediccion)\nsetattr(PerceptronMulticapa, 'evaluar', evaluar)\n\n\n# evaluar y visualizar la matriz de confusion usando sklearn\ny_pred, cm = mlp.evaluar(X_test, y_test, umbral=0.5)\n\n              precision    recall  f1-score   support\n\n           0       0.81      0.81      0.81        26\n           1       0.74      0.74      0.74        19\n\n    accuracy                           0.78        45\n   macro avg       0.77      0.77      0.77        45\nweighted avg       0.78      0.78      0.78        45"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#interpretabilidad",
    "href": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#interpretabilidad",
    "title": "Implementando un Perceptrón Multi-capa (MLP) desde CERO usando python",
    "section": "7. Interpretabilidad",
    "text": "7. Interpretabilidad\nGráficamos algunos aspectos de interpretabilidad como la frontera de decisión generada.\n\nplot_decision_boundary(mlp, X, y, h=0.02)"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#clase-python-perceptronmulticapa-completo",
    "href": "semana_1/notebooks/Nb_1a_Implementando_un_Perceptron_Multicapa_MLP_desde_CERO.html#clase-python-perceptronmulticapa-completo",
    "title": "Implementando un Perceptrón Multi-capa (MLP) desde CERO usando python",
    "section": "Clase python PerceptronMulticapa Completo",
    "text": "Clase python PerceptronMulticapa Completo\n\nclass PerceptronMulticapa():\n    def __init__(self, params=None):\n        # Asignación de hiperparámetros\n        self.capa_entrada = params['capa_entrada']\n        self.capa_oculta = params['capa_oculta']\n        self.capa_salida = params['capa_salida']\n        self.epochs = params['epochs']\n        self.lr = params['lr']\n        self.relu = (lambda x: x*(x &gt; 0))\n        self.derivada_relu = (lambda x: 1 * (x&gt;0))\n        self.sigmoide = (lambda x: 1/(1 + np.exp(-x)))\n        self.derivada_sigmoide = (lambda x: x*(1-x))\n\n        # inicialización de pesos y bias\n        self.inicializacion()\n\n    def inicializacion(self):\n        # inicialización de pesos y bias aleatoria\n        np.random.seed(42) # fijar una semilla para reproducir resultados\n\n        # Capa Oculta\n        self.pesos_capa_oculta = np.random.rand(self.capa_oculta, self.capa_entrada)\n        self.bias_capa_oculta = np.ones((self.capa_oculta, 1))\n\n        # Capa de salida\n        self.pesos_capa_salida = np.random.rand(self.capa_salida, self.capa_oculta)\n        self.bias_capa_salida = np.ones((self.capa_salida, 1))\n\n    def forward_pass(self, x):\n        # Realizar la operacion Wx + b de la capa oculta, x = x_0\n        z = np.matmul(self.pesos_capa_oculta, x) + self.bias_capa_oculta\n        # Aplicar función de activación\n        h = self.relu(z) # z = x_1, h = x_2\n\n        # Aplicar la operación Wh + b para generar la salida, y = x_3\n        y = np.matmul(self.pesos_capa_salida, h) + self.bias_capa_salida\n        # Aplicar función de activación softmax para la clasificación\n        y_pred = self.sigmoide(y) # y = x_4\n\n        return z, h, y_pred\n\n    def actualizar_pesos(self):\n        # actualizar pesos aplicando gradiente descendiente\n        self.pesos_capa_salida -= self.lr * self.gradiente_pesos_capa_salida\n        self.bias_capa_salida -= self.lr * self.gradiente_bias_capa_salida\n        self.pesos_capa_oculta -= self.lr * self.gradiente_pesos_capa_oculta\n        self.bias_capa_oculta -= self.lr * self.gradiente_bias_capa_oculta\n\n    def backward_pass(self, x, z, y_real, h, y_pred):\n        # Propagación de error en la capa de salida\n        # Calculo de error en la capa de salida g_out\n        #error_salida =  (y_pred - y_real) * self.derivada_sigmoide(y_pred)\n        error_salida =  y_pred - y_real\n\n        # gradiente de los pesos respecto a la capa de salida\n        # X_in * g_out = error_salida * h.T\n        # X_in = h es la entrada a la capa de salida\n        self.gradiente_pesos_capa_salida = np.matmul(error_salida, h.T)\n        # gradiente de los bias respecto a la capa de salida\n        self.gradiente_bias_capa_salida = error_salida\n\n        # Propagación de error en la capa oculta\n        # gradiente respecto a la capa oculta\n        # (g_out * W) * relu'(X_in)\n        # X_in en esta capa es la salida de aplicar la primera transformación\n        error_oculta = np.matmul(self.pesos_capa_salida.T, error_salida) * self.derivada_relu(z)\n        # gradientes con respecto a la capa oculta, de nuevo g_out * X_in\n        self.gradiente_pesos_capa_oculta = np.matmul(error_oculta, x.T)\n        self.gradiente_bias_capa_oculta = error_oculta\n\n    def entrenar(self, X, y):\n        # almacenar el costo de cada iteración\n        self.costo_historia = []\n\n        # iterar sobre el número de épocas\n        for iteracion in tqdm(range(self.epochs), desc='Iteraciones'):\n            # iterar sobre los datos de entrenamiento\n            error_total = 0 # error para la iteración i\n            # iterar sobre todo el conjunto de datos\n            for i, (x_i, y_i) in tqdm(enumerate(zip(X,y)), desc='Datos', leave=False):\n                # asegurar de que la entrada y la salida solo tenga una columna\n                x_i = x_i.reshape(-1, 1)\n                y_i = y_i.reshape(-1, 1)\n                # aplicar propagación hacia adelante\n                z, h, y_pred = self.forward_pass(x_i)\n                # calcular la perdida de entropia cruzada\n                perdida = self.calcular_perdida_entropia_cruzada(y_i, y_pred)\n                error_total += perdida\n                # aplicar propagación hacia atras\n                self.backward_pass(x_i, z,  y_i, h, y_pred)\n                # actualizar pesos y bias usando gradiente descendiente\n                self.actualizar_pesos()\n\n            # almacenar los costos de cada iteración\n            self.costo = error_total / len(X)\n            self.costo_historia.append(self.costo)\n\n        #print(f'costo: {self.costo[0][0]} en la iteración: {iteracion}')\n\n    def calcular_perdida_entropia_cruzada(self, y_real, y_pred):\n        epsilon = 1e-12\n        # asegura que los valores de las predicciones esten en un rango\n        # seguro para evitar logaritmos de 0 y 1\n        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n        # calculo de la perdida\n        perdida = -(((1 - y_real) * np.log(1 - y_pred + epsilon)) + (y_real * np.log(y_pred + epsilon)))\n\n        return perdida\n\n    def prediccion(self, X):\n        predicciones = []\n        for x_i in X:\n            x_i = x_i.reshape(-1, 1)  # Asegurar que x_i sea una columna\n            z, _, y_pred = self.forward_pass(x_i)\n            predicciones.append(y_pred)\n        return np.array(predicciones).flatten()\n\n    def evaluar(self, X, y, umbral):\n        # generar predicciones\n        y_pred = self.prediccion(X)\n        # convertir a 0 y 1 bajo un umbral\n        y_pred = np.where(y_pred &gt;= umbral, 1, 0)\n        # generar reporte de clasificación y matriz de confusión\n        print(classification_report(y, y_pred))\n        cm = confusion_matrix(y, y_pred)\n\n        # visualizacion de la matriz de confusion\n        ConfusionMatrixDisplay(cm).plot()\n        plt.show()\n\n        return y_pred, cm"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Curso Práctico: Neural Networks y Deep Learning",
    "section": "",
    "text": "¡Bienvenido al sitio web del curso práctico intensivo de Neural Networks y Deep Learning!\nEste curso está diseñado para proporcionarte una comprensión sólida y aplicada de los conceptos fundamentales de las redes neuronales y su aplicación en el campo de la inteligencia artificial, con un enfoque marcado en la implementación práctica y la resolución de problemas reales.\nA lo largo de tres semanas intensivas, explorarás desde los principios básicos hasta arquitecturas avanzadas y técnicas de vanguardia, preparándote para enfrentar desafíos complejos en el mundo del Deep Learning."
  },
  {
    "objectID": "index.html#qué-exploraremos",
    "href": "index.html#qué-exploraremos",
    "title": "Curso Práctico: Neural Networks y Deep Learning",
    "section": "¿Qué Exploraremos?",
    "text": "¿Qué Exploraremos?\nEl curso se estructura en tres unidades clave:\n\nFundamentos de Redes Neuronales: Introduce los conceptos esenciales, la estructura y el entrenamiento de modelos, abordando el perceptrón, la retropropagación y métodos de optimización como el gradiente descendente. Sentarás las bases para entender cómo aprenden las redes mientras aplicas de manera práctica los conceptos.\nArquitecturas de Redes Neuronales: Explorarás diversas arquitecturas, incluyendo Redes Neuronales Densas (DNN), Convolucionales (CNN) y Recurrentes (RNN). Ampliarás tu visión con una introducción a arquitecturas avanzadas como transformers y desarrollarás la habilidad para seleccionar la más adecuada según el problema.\nTécnicas Avanzadas y Robustez: Te centrarás en la construcción de modelos avanzados, cubriendo la generalización, la transferencia de aprendizaje, optimización de hiperparámetros y la explicabilidad. Se introducirá el entrenamiento adversarial y técnicas para crear modelos robustos y transferibles que funcionen bien en entornos de datos diversos."
  },
  {
    "objectID": "index.html#enfoque-práctico",
    "href": "index.html#enfoque-práctico",
    "title": "Curso Práctico: Neural Networks y Deep Learning",
    "section": "Enfoque Práctico",
    "text": "Enfoque Práctico\nEste curso es eminentemente práctico. A través de Implementaciones en Python y trabajo directo con notebooks, aplicarás los conceptos teóricos de inmediato. El objetivo es que no solo comprendas cómo funcionan las redes neuronales, sino que también ganes experiencia práctica en cómo construirlas y utilizarlas para resolver problemas reales.\nCada semana incluye componentes prácticos diseñados para consolidar tu aprendizaje y permitirte experimentar con diferentes arquitecturas y técnicas."
  },
  {
    "objectID": "index.html#estructura-del-sitio",
    "href": "index.html#estructura-del-sitio",
    "title": "Curso Práctico: Neural Networks y Deep Learning",
    "section": "Estructura del Sitio",
    "text": "Estructura del Sitio\n\nSemana 1: Fundamentos y MLP\nSemana 2: Arquitecturas neuronales profundas\nSemana 3: Técnicas Avanzadas y Robustez\n\nExplora cada sección para encontrar los materiales de la semana, los notebooks (renderizados para visualización web) y las slides correspondientes."
  },
  {
    "objectID": "semana_1/index.html",
    "href": "semana_1/index.html",
    "title": "Semana 1: Fundamentos de Redes Neuronales",
    "section": "",
    "text": "Esta semana sentamos las bases del Deep Learning. Cubriremos los conceptos esenciales, la estructura y el entrenamiento de las redes neuronales.\nTemas Clave:\n\nEstructura y entrenamiento de modelos en Deep Learning\nEl Perceptrón\nProceso de Retropropagación\nMétodos de optimización (Gradiente Descendente)\n\nEnfoque Práctico: Comprenderás el funcionamiento interno de una red y aplicarás técnicas de entrenamiento en modelos simples. Realizarás implementaciones prácticas para observar cómo aprenden los modelos.\nMateriales de la Semana:\n\nSlides de la Semana 1 (Próximamente)\nNotebook: Implementando Perceptrón Multicapa desde CERO\nNotebook: MLP usando Frameworks"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1b_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks.html",
    "href": "semana_1/notebooks/Nb_1b_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks.html",
    "title": "Implementado un Perceptrón multi-capa usando frameworks",
    "section": "",
    "text": "Open In Colab\n#@title Importar librerías\n#importar librerías necesarias\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn import set_config\nset_config(display='diagram')\nimport seaborn as sns\nimport random\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Input\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.utils import plot_model\n#@title Funciones complementarias\ndef plot_samples_dataset(X, y):\n    # Convertir las etiquetas a enteros\n    y = y.astype(int)\n\n    # Crear la grilla de 4x4\n    fig, axes = plt.subplots(4, 4, figsize=(6, 6))\n    fig.suptitle('Grilla de imágenes del dataset MNIST')\n\n    # Iterar para mostrar las primeras 16 imágenes con sus etiquetas\n    for i, ax in enumerate(axes.flat):\n        img = X.iloc[i].values.reshape(28, 28)\n        label = y[i]\n        ax.imshow(img, cmap='gray')\n        ax.set_title(f'Label: {label}')\n        ax.axis('off')\n\n    plt.show()\n\ndef plot_curva_aprendizaje(mlp):\n    plt.figure(figsize=(8, 5))\n    plt.plot(mlp.loss_curve_, marker='o')\n    plt.title('Pérdida durante el entrenamiento del MLP por iteración')\n    plt.xlabel('Iteración')\n    plt.ylabel('Pérdida (Loss)')\n    plt.grid()\n    plt.show()\n\ndef plot_matriz_confusion(cm):\n    # Visualizar la matriz de confusión usando Seaborn\n    plt.figure(figsize=(10, 7))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n    plt.xlabel('Etiqueta predicha')\n    plt.ylabel('Etiqueta real')\n    plt.title('Matriz de Confusión para el MLP en el dataset MNIST')\n    plt.show()\n\ndef encontrar_dim_imagen(n_neurons):\n    \"\"\"\n    Encuentra la mejor forma cuadrada (filas, columnas) para una cantidad dada de neuronas.\n    \"\"\"\n    side_length = int(np.sqrt(n_neurons))  # Calcular la raíz cuadrada del número de neuronas\n    if side_length * side_length == n_neurons:\n        return (side_length, side_length)  # Si es un cuadrado perfecto\n    else:\n        # Si no es un cuadrado perfecto, buscamos la mejor aproximación (filas, columnas)\n        for i in range(side_length, 0, -1):\n            if n_neurons % i == 0:\n                return (i, n_neurons // i)  # Devolver filas y columnas\n    return (n_neurons, 1)  # Si no encuentra, retornar en forma de vector (n_neurons, 1)\n\ndef visualizacion_pesos_mlp(mlp):\n    # Definir la figura con 3 filas y 5 columnas\n    fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n\n    # Asignar las dimensiones para visualizar cada capa\n    layer_shapes = [encontrar_dim_imagen(layer.shape[0]) for layer in mlp.coefs_]\n\n    # Recorrer cada capa de coeficientes del MLP\n    for layer_index, (layer_coefs, ax_row) in enumerate(zip(mlp.coefs_, axes)):\n        # Seleccionar aleatoriamente 5 neuronas de la capa actual\n        num_neurons = layer_coefs.shape[1]\n        random_neurons = random.sample(range(num_neurons), 5)\n\n        # Obtener la forma de visualización para esta capa\n        layer_shape = layer_shapes[layer_index]\n\n        vmin, vmax = layer_coefs.min(), layer_coefs.max()\n\n        # Visualizar las neuronas seleccionadas\n        for neuron_index, ax in zip(random_neurons, ax_row):\n            # Seleccionar los pesos de la neurona específica y reestructurarlos en una matriz 2D\n            neuron_weights = layer_coefs[:, neuron_index].reshape(layer_shape)\n            # Dibujar la imagen de los pesos de la neurona\n            ax.matshow(neuron_weights, cmap=plt.cm.gray, vmin=0.5 * vmin, vmax=0.5 * vmax)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.set_title(f'Capa {layer_index+1}, Neurona {neuron_index}')\n\n    plt.suptitle('Visualización de Pesos de las Neuronas en las Capas Ocultas')\n    plt.tight_layout()\n    plt.show()\n\ndef plot_loss_historia_keras(history):\n    # Graficar el histórico de pérdida durante el entrenamiento\n    plt.plot(history.history['loss'], label='Pérdida de Entrenamiento')\n    plt.plot(history.history['val_loss'], label='Pérdida de Validación')\n    plt.title('Pérdida durante el Entrenamiento')\n    plt.xlabel('Época')\n    plt.ylabel('Pérdida')\n    plt.legend()\n    plt.show()\n\ndef plot_acc_historia_keras(history):\n    # Graficar la precisión durante el entrenamiento\n    plt.plot(history.history['accuracy'], label='Precisión de Entrenamiento')\n    plt.plot(history.history['val_accuracy'], label='Precisión de Validación')\n    plt.title('Precisión durante el Entrenamiento')\n    plt.xlabel('Época')\n    plt.ylabel('Precisión')\n    plt.legend()\n    plt.show()\n\ndef visualizacion_pesos_mlp_keras(model):\n    # Obtener los pesos del modelo (par de listas [pesos, biases] para cada capa)\n    weights = model.get_weights()\n\n    # Extraer solo los pesos de cada capa oculta, ignorando los bias\n    layer_weights = [weights[i] for i in range(0, len(weights), 2)]  # Solo los pesos, no los sesgos\n\n    # Definir la figura con 3 filas (una por cada capa) y 5 columnas (5 neuronas al azar)\n    fig, axes = plt.subplots(len(layer_weights), 5, figsize=(15, 9))\n\n    # Calcular las formas de cada capa de manera dinámica\n    layer_shapes = [encontrar_dim_imagen(layer.shape[0]) for layer in layer_weights]\n\n    # Recorrer cada capa y sus pesos\n    for layer_index, (layer_coefs, ax_row) in enumerate(zip(layer_weights, axes)):\n        # Seleccionar aleatoriamente 5 neuronas de la capa actual\n        num_neurons = layer_coefs.shape[1]\n        random_neurons = random.sample(range(num_neurons), 5)\n\n        # Obtener la forma de visualización para esta capa\n        layer_shape = layer_shapes[layer_index]\n        vmin, vmax = layer_coefs.min(), layer_coefs.max()\n\n\n        # Visualizar las neuronas seleccionadas\n        for neuron_index, ax in zip(random_neurons, ax_row):\n            # Seleccionar los pesos de la neurona específica y reestructurarlos en una matriz 2D\n            neuron_weights = layer_coefs[:, neuron_index].reshape(layer_shape)\n            # Dibujar la imagen de los pesos de la neurona\n            ax.matshow(neuron_weights, cmap=plt.cm.gray, vmin=0.5 * vmin, vmax=0.5 * vmax)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.set_title(f'Capa {layer_index+1}, Neurona {neuron_index}')\n\n    plt.suptitle('Visualización de Pesos de las Neuronas en las Capas Ocultas de Keras')\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1b_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks.html#evaluación-completa",
    "href": "semana_1/notebooks/Nb_1b_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks.html#evaluación-completa",
    "title": "Implementado un Perceptrón multi-capa usando frameworks",
    "section": "Evaluación completa",
    "text": "Evaluación completa\nRealizaremos una evaluación completa revisando el rendimiento en ambos conjuntos, seguidamente generaremos el reporte de clasificación y la matriz de confusión.\n\nprint(f\"Training set score: {mlp.score(X_train, y_train):.3f}\")\nprint(f\"Test set score: {mlp.score(X_test, y_test):.3f}\")\n\nTraining set score: 1.000\nTest set score: 0.982\n\n\n\n# Realizar predicciones\ny_pred = mlp.predict(X_test)\n\n# Imprimir el reporte de métricas\nprint(\"Reporte de Clasificación del MLP en MNIST:\\n\")\nprint(classification_report(y_test, y_pred))\n\n# Generar la matriz de confusión\ncm = confusion_matrix(y_test, y_pred)\n\n# visualizar la matriz de confusión\nplot_matriz_confusion(cm)\n\nReporte de Clasificación del MLP en MNIST:\n\n              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99      2058\n           1       0.99      0.99      0.99      2364\n           2       0.98      0.98      0.98      2133\n           3       0.98      0.98      0.98      2176\n           4       0.98      0.98      0.98      1936\n           5       0.98      0.98      0.98      1915\n           6       0.98      0.99      0.99      2088\n           7       0.98      0.98      0.98      2248\n           8       0.98      0.97      0.97      1992\n           9       0.98      0.97      0.98      2090\n\n    accuracy                           0.98     21000\n   macro avg       0.98      0.98      0.98     21000\nweighted avg       0.98      0.98      0.98     21000"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1b_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks.html#visualización-de-pesos",
    "href": "semana_1/notebooks/Nb_1b_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks.html#visualización-de-pesos",
    "title": "Implementado un Perceptrón multi-capa usando frameworks",
    "section": "Visualización de pesos",
    "text": "Visualización de pesos\n\n# pesos de la primera capa oculta. Todas las neuronas conectadas con cada pixel\nprint('Dimensión de la primera capa oculta: {}'.format(mlp.coefs_[0].shape))\nprint('Dimensión de la segunda capa oculta: {}'.format(mlp.coefs_[1].shape))\nprint('Dimensión de la tercera capa oculta: {}'.format(mlp.coefs_[2].shape))\n\nDimensión de la primera capa oculta: (784, 225)\nDimensión de la segunda capa oculta: (225, 100)\nDimensión de la tercera capa oculta: (100, 10)\n\n\n\nvisualizacion_pesos_mlp(mlp)\n\n\n\n\n\n\n\n\n\nTutoriales relacionados\n\nAnálisis de la variación del parametro de regularización alpha\nComparación de las diferentes estrategias de aprendizaje\nAccelaración de Sklearn (GPU) usando la extensión sklearnex\nAccelaración de Sklearn (GPU) usando CuML"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1b_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks.html#evaluación-completa-1",
    "href": "semana_1/notebooks/Nb_1b_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks.html#evaluación-completa-1",
    "title": "Implementado un Perceptrón multi-capa usando frameworks",
    "section": "Evaluación completa",
    "text": "Evaluación completa\n\ny_test_categorical = to_categorical(y_test)\n\nscore = mlp_keras.evaluate(X_test.values.astype(float), y_test_categorical, batch_size=128)\n\nscore\n\n\n165/165 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.9484 - loss: 0.1750\n\n\n\n\n[0.175654336810112, 0.9481428861618042]\n\n\n\n# Realizar predicciones en el conjunto de prueba\ny_pred = mlp_keras.predict(X_test.values.astype(float))\n\n# Convertir las predicciones en etiquetas (la clase con mayor probabilidad)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true = y_test.values.astype(int)  # Las etiquetas reales del conjunto de prueba\n\n\n657/657 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step\n\n\n\n\n\n# Generar el reporte de clasificación\nprint(\"Reporte de Clasificación para el MLP en MNIST:\\n\")\nprint(classification_report(y_true, y_pred_classes))\n\n# Crear la matriz de confusión\ncm = confusion_matrix(y_true, y_pred_classes)\n\n# Visualizar la matriz de confusión usando Seaborn\nplot_matriz_confusion(cm)\n\nReporte de Clasificación para el MLP en MNIST:\n\n              precision    recall  f1-score   support\n\n           0       0.97      0.98      0.97      2058\n           1       0.96      0.98      0.97      2364\n           2       0.95      0.94      0.94      2133\n           3       0.93      0.94      0.93      2176\n           4       0.94      0.95      0.94      1936\n           5       0.95      0.93      0.94      1915\n           6       0.96      0.97      0.97      2088\n           7       0.95      0.96      0.95      2248\n           8       0.95      0.92      0.93      1992\n           9       0.93      0.92      0.93      2090\n\n    accuracy                           0.95     21000\n   macro avg       0.95      0.95      0.95     21000\nweighted avg       0.95      0.95      0.95     21000"
  },
  {
    "objectID": "semana_1/notebooks/Nb_1b_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks.html#visualización-de-pesos-1",
    "href": "semana_1/notebooks/Nb_1b_Implementando_un_Percentron_Multiplicapa_MLP_usando_frameworks.html#visualización-de-pesos-1",
    "title": "Implementado un Perceptrón multi-capa usando frameworks",
    "section": "Visualización de pesos",
    "text": "Visualización de pesos\n\nvisualizacion_pesos_mlp_keras(mlp_keras)\n\n\n\n\n\n\n\n\n\n✅ Cómo transformar tus datos para usar un MLP estándar (Keras o scikit-learn)\n\n\n\n\n\n\n\n\nTipo de dato o problema\n¿Qué debes hacer para usar un MLP?\nEjemplo sencillo\n\n\n\n\n📝 Texto\nConvertir el texto a vectores. Usa técnicas como Bag of Words (BoW), TF-IDF.\nClasificación de sentimientos: convertir cada comentario en un vector TF-IDF\n\n\n⏱ Series temporales\nDividir en ventanas de tiempo fijas y calcular características estadísticas (media, std, min, max, energía, etc.) por ventana.\nPredicción de fallas: usar estadísticas de 10s de datos de sensores como entrada al MLP\n\n\n❗ Anomalías\nEtiquetar datos anómalos (si puedes).\nDetección de fraude: marcar transacciones normales y anómalas y entrenar un clasificador\n\n\n🖼 Imágenes\nExtraer características manuales (como color, textura, tamaño, etc.) o redimensionar las imágenes y vectorizarlas.\nClasificación de imágenes de zapatos: usar un modelo CNN preentrenado para extraer features\n\n\n🔊 Audio\nExtraer features de audio como MFCCs, espectrogramas, energía, pitch, etc., y construir vectores con esas estadísticas.\nDetección de emociones en voz: usar MFCCs y energía para representar cada audio\n\n\n\n\n\n🧭 Guía paso a paso para construir tu baseline con un MLP\nSigue estos pasos para convertir tu idea o proyecto en un experimento funcional con un Perceptrón Multicapa (MLP), usando scikit-learn o Keras.\n\n\n1️⃣ Define el objetivo de predicción\n\n¿Tu problema es de clasificación o regresión?\n¿Cuál es la variable que quieres predecir?\nEjemplos:\n\nClasificación: ¿Este artículo es de biología o matemáticas?\nRegresión: ¿Cuál será el consumo energético el próximo mes?\n\n\n\n\n\n2️⃣ Identifica tu tipo de datos\n\n¿Qué tipo de datos tienes?\n\nTexto\nSeries temporales\nDatos tabulares\nImágenes\nAudio\n\n\n🔍 Revisa la tabla anterior para ver cómo transformar tus datos para usarlos con un MLP.\n\n\n\n3️⃣ Preprocesa y vectoriza tus datos\n\nNormaliza tus valores si son numéricos (por ejemplo, entre 0 y 1).\nSi tienes texto, usa TF-IDF o BoW.\nSi tienes imágenes, vectorízalas (flatten).\nSi tienes secuencias, divide en ventanas y calcula estadísticas (media, std, etc.).\n\n📌 Asegúrate de que cada fila sea un ejemplo y cada columna una característica.\n\n\n\n4️⃣ Define el tipo de salida y la función de pérdida\n\nClasificación binaria → sigmoid + binary_crossentropy\nClasificación multiclase → softmax + categorical_crossentropy\nRegresión → linear + mean_squared_error o mean_absolute_error\n\n⚠️ Si usas Keras, recuerda convertir las etiquetas con to_categorical() si usas softmax.\n\n\n\n5️⃣ Construye tu MLP\n\nDecide cuántas capas ocultas y neuronas usar (ej. 2 capas de 128 y 64).\nUsa relu como activación oculta y softmax o sigmoid según el caso.\nAñade regularización (Dropout, L2) si hay riesgo de sobreajuste."
  },
  {
    "objectID": "semana_1/slides/slides_semana_1.html#perceptrón-como-clasificador-básico",
    "href": "semana_1/slides/slides_semana_1.html#perceptrón-como-clasificador-básico",
    "title": "Semana 1: Fundamentos de Redes Neuronales",
    "section": "Perceptrón como Clasificador básico",
    "text": "Perceptrón como Clasificador básico"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2a_Implementando_una_CNN_usando_Keras.html",
    "href": "semana_2/notebooks/Nb_2a_Implementando_una_CNN_usando_Keras.html",
    "title": "Implementado una Red Neuronal Convolucional",
    "section": "",
    "text": "Open In Colab\n#@title Importar librerías\n#importar librerías necesarias\nimport random\nfrom random import randint\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport random\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Input\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.datasets import mnist\n#@title Funciones complementarias\ndef plot_samples_dataset(X, y):\n    # Convertir las etiquetas a enteros\n    y = y.astype(int)\n\n    # Crear la grilla de 4x4\n    fig, axes = plt.subplots(4, 4, figsize=(6, 6))\n    fig.suptitle('Grilla de imágenes del dataset MNIST')\n\n    # Iterar para mostrar las primeras 16 imágenes con sus etiquetas\n    for i, ax in enumerate(axes.flat):\n        img = X.iloc[i].values.reshape(28, 28)\n        label = y[i]\n        ax.imshow(img, cmap='gray')\n        ax.set_title(f'Label: {label}')\n        ax.axis('off')\n\n    plt.show()\n\ndef plot_curva_aprendizaje(mlp):\n    plt.figure(figsize=(8, 5))\n    plt.plot(mlp.loss_curve_, marker='o')\n    plt.title('Pérdida durante el entrenamiento del MLP por iteración')\n    plt.xlabel('Iteración')\n    plt.ylabel('Pérdida (Loss)')\n    plt.grid()\n    plt.show()\n\ndef plot_matriz_confusion(cm):\n    # Visualizar la matriz de confusión usando Seaborn\n    plt.figure(figsize=(10, 7))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n    plt.xlabel('Etiqueta predicha')\n    plt.ylabel('Etiqueta real')\n    plt.title('Matriz de Confusión para el MLP en el dataset MNIST')\n    plt.show()\n\ndef encontrar_dim_imagen(n_neurons):\n    \"\"\"\n    Encuentra la mejor forma cuadrada (filas, columnas) para una cantidad dada de neuronas.\n    \"\"\"\n    side_length = int(np.sqrt(n_neurons))  # Calcular la raíz cuadrada del número de neuronas\n    if side_length * side_length == n_neurons:\n        return (side_length, side_length)  # Si es un cuadrado perfecto\n    else:\n        # Si no es un cuadrado perfecto, buscamos la mejor aproximación (filas, columnas)\n        for i in range(side_length, 0, -1):\n            if n_neurons % i == 0:\n                return (i, n_neurons // i)  # Devolver filas y columnas\n    return (n_neurons, 1)  # Si no encuentra, retornar en forma de vector (n_neurons, 1)\n\ndef plot_loss_historia_keras(history):\n    # Graficar el histórico de pérdida durante el entrenamiento\n    plt.plot(history.history['loss'], label='Pérdida de Entrenamiento')\n    plt.plot(history.history['val_loss'], label='Pérdida de Validación')\n    plt.title('Pérdida durante el Entrenamiento')\n    plt.xlabel('Época')\n    plt.ylabel('Pérdida')\n    plt.legend()\n    plt.show()\n\ndef plot_acc_historia_keras(history):\n    # Graficar la precisión durante el entrenamiento\n    plt.plot(history.history['accuracy'], label='Precisión de Entrenamiento')\n    plt.plot(history.history['val_accuracy'], label='Precisión de Validación')\n    plt.title('Precisión durante el Entrenamiento')\n    plt.xlabel('Época')\n    plt.ylabel('Precisión')\n    plt.legend()\n    plt.show()\n\ndef visualizacion_filtros_cnn_keras(model):\n    # Obtener las capas convolucionales del modelo\n    conv_layers = [layer for layer in model.layers if 'conv' in layer.name]\n\n    # Definir la figura con una fila por capa convolucional y varias columnas (5 filtros al azar por capa)\n    fig, axes = plt.subplots(len(conv_layers), 5, figsize=(10, len(conv_layers) * 3))\n\n    # Recorrer cada capa convolucional y sus pesos\n    for layer_index, (layer, ax_row) in enumerate(zip(conv_layers, axes)):\n        # Obtener los pesos de la capa (solo el primer tensor, ignorar bias)\n        layer_weights = layer.get_weights()[0]  # shape: (filter_height, filter_width, input_channels, num_filters)\n\n        # Seleccionar 5 filtros de la capa actual\n        num_filters = layer_weights.shape[-1]\n        random_filters = random.sample(range(num_filters), 5)\n\n        # Obtener los límites para la normalización de las imágenes\n        vmin, vmax = layer_weights.min(), layer_weights.max()\n\n        # Dibujar cada filtro seleccionado\n        for filter_index, ax in zip(random_filters, ax_row):\n            # Extraer el filtro correspondiente (shape: filter_height, filter_width, input_channels)\n            filter_weights = layer_weights[..., filter_index]\n\n            # Promediar los canales para visualizar como imagen en escala de grises\n            if filter_weights.shape[-1] &gt; 1:\n                filter_weights = np.mean(filter_weights, axis=-1)  # Promedio sobre los canales de entrada\n\n            # Dibujar la imagen del filtro\n            ax.matshow(filter_weights, cmap=plt.cm.gray, vmin=0.5 * vmin, vmax=0.5 * vmax)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.set_title(f'Capa {layer_index + 1}, Filtro {filter_index}')\n\n    plt.suptitle('Visualización de Filtros de las Capas Convolucionales de Keras')\n    plt.tight_layout()\n    plt.show()\n\ndef visualizacion_feature_maps(model, image):\n    # Crear un nuevo modelo que toma la misma entrada pero cuya salida son los mapas de características de cada capa convolucional\n    layer_outputs = [layer.output for layer in model.layers if 'conv' in layer.name]\n    feature_map_model = tf.keras.Model(inputs=model.input, outputs=layer_outputs)\n\n    # Obtener los mapas de características al pasar la imagen a través del modelo\n    feature_maps = feature_map_model.predict(np.expand_dims(image, axis=0))  # Añadir batch dimension\n\n    # Recorrer cada capa convolucional y sus feature maps correspondientes\n    for layer_index, feature_map in enumerate(feature_maps):\n        # Número de filtros en la capa actual\n        num_filters = feature_map.shape[-1]\n\n        # Definir la figura con una fila por cada filtro (limitado a 6 para evitar gráficos muy grandes)\n        fig, axes = plt.subplots(1, min(6, num_filters), figsize=(20, 5))\n\n        # Mostrar cada filtro como imagen en escala de grises\n        for i in range(min(6, num_filters)):  # Mostrar un máximo de 6 filtros\n            ax = axes[i]\n            # Extraer el feature map del filtro `i` y mostrarlo\n            ax.matshow(feature_map[0, :, :, i], cmap='viridis')\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.set_title(f'Filtro {i+1}')\n\n        # Título de la capa y ajuste de la visualización\n        plt.suptitle(f'Visualización de Feature Maps - Capa {layer_index+1}')\n        plt.tight_layout()\n        plt.show()"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2a_Implementando_una_CNN_usando_Keras.html#componentes-base-complementarios",
    "href": "semana_2/notebooks/Nb_2a_Implementando_una_CNN_usando_Keras.html#componentes-base-complementarios",
    "title": "Implementado una Red Neuronal Convolucional",
    "section": "Componentes base complementarios",
    "text": "Componentes base complementarios\n\nCapas de Pooling\nCapas de normalización\n\n\n1. Capas de Pooling\n\n\n\n1_vKYHxr5oI9cBw_hGhjQCrA.webp\n\n\n\n\n2. Capas de normalización\nEn nuestro ejemplo, no es necesario normalizar ya que nuestras entradas estan entre 0 y 1 y no tienen un alta complejidad. Pero en otros escenarios, con imágenes más complejas y en formato RGB la normalización se hace más común.\n\n\n\n1_dsl93qeGPteT3Zt7mBy1dQ-1.webp\n\n\n\n# Asumiendo que X_train y y_train ya están definidos como en el ejemplo anterior\n# Preprocesar las etiquetas para que sean categóricas (one-hot encoding)\ny_train_categorical = to_categorical(y_train)\ny_train_categorical\n\narray([[1., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 1., 0., ..., 0., 0., 0.],\n       [1., 0., 0., ..., 0., 0., 0.],\n       [1., 0., 0., ..., 0., 0., 0.]])\n\n\n\n# crear modelo usando el API funcional\ndef cnn_model(input_shape, num_classes):\n    # Definir la entrada\n    inputs = tf.keras.Input(shape=input_shape)\n\n    # Primera capa convolucional y de pooling\n    x = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\",\n                               strides=(1, 1),\n                               padding=\"valid\")(inputs)\n    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n\n    # Segunda capa convolucional y de pooling\n    x = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\",\n                               strides=(1, 1),\n                               padding=\"valid\")(x)\n    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n\n    # Aplanar y añadir Dropout\n    x = tf.keras.layers.Flatten()(x)\n    x = tf.keras.layers.Dropout(0.5)(x)\n\n    # Capa de salida\n    outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n\n    # Crear el modelo usando la API funcional\n    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='cnn_keras')\n\n    return model\n\n\n# Ahora se definen los input shape y el numero de clases\ninput_shape = (dim_imagen[0], dim_imagen[1], n_canales)\nnum_classes = 10\n\n# Crear el modelo cnn\ncnn_keras = cnn_model(input_shape, num_classes)\n\n# Visualizar el resumen del modelo\ncnn_keras.summary()\n\nModel: \"cnn_keras\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer_4 (InputLayer)           │ (None, 28, 28, 1)           │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_8 (Conv2D)                    │ (None, 26, 26, 32)          │             320 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_8 (MaxPooling2D)       │ (None, 13, 13, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_9 (Conv2D)                    │ (None, 11, 11, 64)          │          18,496 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_9 (MaxPooling2D)       │ (None, 5, 5, 64)            │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_4 (Flatten)                  │ (None, 1600)                │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_4 (Dropout)                  │ (None, 1600)                │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_4 (Dense)                      │ (None, 10)                  │          16,010 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 34,826 (136.04 KB)\n\n\n\n Trainable params: 34,826 (136.04 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# tambien es posible visualizar nuestra red en formato de grafo\ntf.keras.utils.plot_model(cnn_keras, rankdir='LR',show_dtype=True)\n\n\n\n\n\n\n\n\n\n# Compilar el modelo\ncnn_keras.compile(loss='categorical_crossentropy',\n            optimizer=SGD(),\n            metrics=['accuracy'])\n\n# Entrenar el modelo\nhistory = cnn_keras.fit(X_train, y_train_categorical,\n                    epochs=30,\n                    batch_size=128,\n                    validation_split=0.2,\n                    verbose=1)\n\n\nEpoch 1/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 37s 114ms/step - accuracy: 0.2201 - loss: 2.2057 - val_accuracy: 0.7818 - val_loss: 1.0697\n\nEpoch 2/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 41s 114ms/step - accuracy: 0.6966 - loss: 0.9674 - val_accuracy: 0.8851 - val_loss: 0.4270\n\nEpoch 3/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 42s 117ms/step - accuracy: 0.8352 - loss: 0.5283 - val_accuracy: 0.9119 - val_loss: 0.3104\n\nEpoch 4/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 38s 107ms/step - accuracy: 0.8783 - loss: 0.3987 - val_accuracy: 0.9287 - val_loss: 0.2499\n\nEpoch 5/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 35s 113ms/step - accuracy: 0.8973 - loss: 0.3304 - val_accuracy: 0.9379 - val_loss: 0.2164\n\nEpoch 6/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 40s 110ms/step - accuracy: 0.9098 - loss: 0.2894 - val_accuracy: 0.9452 - val_loss: 0.1902\n\nEpoch 7/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 33s 108ms/step - accuracy: 0.9211 - loss: 0.2539 - val_accuracy: 0.9516 - val_loss: 0.1763\n\nEpoch 8/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 33s 107ms/step - accuracy: 0.9284 - loss: 0.2354 - val_accuracy: 0.9534 - val_loss: 0.1646\n\nEpoch 9/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 41s 106ms/step - accuracy: 0.9343 - loss: 0.2212 - val_accuracy: 0.9570 - val_loss: 0.1519\n\nEpoch 10/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 41s 105ms/step - accuracy: 0.9395 - loss: 0.2012 - val_accuracy: 0.9598 - val_loss: 0.1423\n\nEpoch 11/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 36s 117ms/step - accuracy: 0.9442 - loss: 0.1924 - val_accuracy: 0.9606 - val_loss: 0.1350\n\nEpoch 12/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 33s 107ms/step - accuracy: 0.9440 - loss: 0.1829 - val_accuracy: 0.9617 - val_loss: 0.1289\n\nEpoch 13/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 41s 107ms/step - accuracy: 0.9456 - loss: 0.1772 - val_accuracy: 0.9637 - val_loss: 0.1241\n\nEpoch 14/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 41s 106ms/step - accuracy: 0.9478 - loss: 0.1702 - val_accuracy: 0.9646 - val_loss: 0.1184\n\nEpoch 15/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 33s 108ms/step - accuracy: 0.9521 - loss: 0.1600 - val_accuracy: 0.9662 - val_loss: 0.1143\n\nEpoch 16/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 33s 109ms/step - accuracy: 0.9530 - loss: 0.1579 - val_accuracy: 0.9678 - val_loss: 0.1113\n\nEpoch 17/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 42s 113ms/step - accuracy: 0.9535 - loss: 0.1531 - val_accuracy: 0.9677 - val_loss: 0.1076\n\nEpoch 18/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 40s 112ms/step - accuracy: 0.9540 - loss: 0.1493 - val_accuracy: 0.9691 - val_loss: 0.1052\n\nEpoch 19/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 40s 108ms/step - accuracy: 0.9572 - loss: 0.1434 - val_accuracy: 0.9689 - val_loss: 0.1014\n\nEpoch 20/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 39s 128ms/step - accuracy: 0.9575 - loss: 0.1400 - val_accuracy: 0.9708 - val_loss: 0.0986\n\nEpoch 21/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 37s 116ms/step - accuracy: 0.9596 - loss: 0.1369 - val_accuracy: 0.9717 - val_loss: 0.0965\n\nEpoch 22/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 32s 105ms/step - accuracy: 0.9596 - loss: 0.1311 - val_accuracy: 0.9715 - val_loss: 0.0939\n\nEpoch 23/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 41s 106ms/step - accuracy: 0.9606 - loss: 0.1319 - val_accuracy: 0.9721 - val_loss: 0.0920\n\nEpoch 24/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 41s 105ms/step - accuracy: 0.9616 - loss: 0.1259 - val_accuracy: 0.9730 - val_loss: 0.0897\n\nEpoch 25/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 41s 106ms/step - accuracy: 0.9622 - loss: 0.1273 - val_accuracy: 0.9731 - val_loss: 0.0885\n\nEpoch 26/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 42s 109ms/step - accuracy: 0.9611 - loss: 0.1232 - val_accuracy: 0.9732 - val_loss: 0.0875\n\nEpoch 27/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 35s 113ms/step - accuracy: 0.9635 - loss: 0.1189 - val_accuracy: 0.9745 - val_loss: 0.0853\n\nEpoch 28/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 40s 109ms/step - accuracy: 0.9661 - loss: 0.1172 - val_accuracy: 0.9750 - val_loss: 0.0833\n\nEpoch 29/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 41s 108ms/step - accuracy: 0.9644 - loss: 0.1202 - val_accuracy: 0.9752 - val_loss: 0.0817\n\nEpoch 30/30\n\n307/307 ━━━━━━━━━━━━━━━━━━━━ 34s 110ms/step - accuracy: 0.9656 - loss: 0.1126 - val_accuracy: 0.9753 - val_loss: 0.0807\n\n\n\n\n\nplot_loss_historia_keras(history)\n\n\n\n\n\n\n\n\n\nplot_acc_historia_keras(history)"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2a_Implementando_una_CNN_usando_Keras.html#evaluación-completa",
    "href": "semana_2/notebooks/Nb_2a_Implementando_una_CNN_usando_Keras.html#evaluación-completa",
    "title": "Implementado una Red Neuronal Convolucional",
    "section": "Evaluación completa",
    "text": "Evaluación completa\n\ny_test_categorical = to_categorical(y_test)\n\nscore = cnn_keras.evaluate(X_test, y_test_categorical, batch_size=128)\n\nscore\n\n\n165/165 ━━━━━━━━━━━━━━━━━━━━ 4s 27ms/step - accuracy: 0.9733 - loss: 0.0893\n\n\n\n\n[0.08807186037302017, 0.9740952253341675]\n\n\n\n# Realizar predicciones en el conjunto de prueba\ny_pred = cnn_keras.predict(X_test)\n\n# Convertir las predicciones en etiquetas (la clase con mayor probabilidad)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true = y_test.values.astype(int)  # Las etiquetas reales del conjunto de prueba\n\n\n657/657 ━━━━━━━━━━━━━━━━━━━━ 6s 9ms/step\n\n\n\n\n\n# Generar el reporte de clasificación\nprint(\"Reporte de Clasificación para la red CNN en MNIST:\\n\")\nprint(classification_report(y_true, y_pred_classes))\n\n# Crear la matriz de confusión\ncm = confusion_matrix(y_true, y_pred_classes)\n\n# Visualizar la matriz de confusión usando Seaborn\nplot_matriz_confusion(cm)\n\nReporte de Clasificación para la red CNN en MNIST:\n\n              precision    recall  f1-score   support\n\n           0       0.98      0.99      0.98      2058\n           1       0.97      0.99      0.98      2364\n           2       0.96      0.97      0.97      2133\n           3       0.97      0.97      0.97      2176\n           4       0.97      0.98      0.98      1936\n           5       0.99      0.97      0.98      1915\n           6       0.99      0.98      0.99      2088\n           7       0.97      0.97      0.97      2248\n           8       0.97      0.95      0.96      1992\n           9       0.96      0.96      0.96      2090\n\n    accuracy                           0.97     21000\n   macro avg       0.97      0.97      0.97     21000\nweighted avg       0.97      0.97      0.97     21000"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2a_Implementando_una_CNN_usando_Keras.html#visualización-de-filtros-convolucionales",
    "href": "semana_2/notebooks/Nb_2a_Implementando_una_CNN_usando_Keras.html#visualización-de-filtros-convolucionales",
    "title": "Implementado una Red Neuronal Convolucional",
    "section": "Visualización de filtros convolucionales",
    "text": "Visualización de filtros convolucionales\n\nvisualizacion_filtros_cnn_keras(cnn_keras)"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2a_Implementando_una_CNN_usando_Keras.html#visualización-de-los-features-maps",
    "href": "semana_2/notebooks/Nb_2a_Implementando_una_CNN_usando_Keras.html#visualización-de-los-features-maps",
    "title": "Implementado una Red Neuronal Convolucional",
    "section": "Visualización de los features maps",
    "text": "Visualización de los features maps\n\nvisualizacion_feature_maps(cnn_keras, image=X_test[randint(0, 100),:,:,:])\n\n\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html",
    "href": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html",
    "title": "Explicando la Mejora de los Transformers sobre las RNNs",
    "section": "",
    "text": "Última actualización 07/05/2025\n#@title Importar librerías\n#importar librerías necesarias\nimport os\nimport logging\nimport time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\n\nimport tensorflow_text\n\nimport warnings\nwarnings.filterwarnings('ignore')\n#@title Funciones complementarias\ndef plot_similaridad_positional_encodings(pos_encoding):\n  # normalización de los vectores a 1\n  pos_encoding/=tf.norm(pos_encoding, axis=1, keepdims=True)\n  # seleccionamos el vector de la posición 1000\n  p = pos_encoding[1000]\n  # cálculo de la similitud del producto punto\n  dots = tf.einsum('pd,d -&gt; p', pos_encoding, p)\n\n  # visualización de la relación de los vectores con sus palabras\n  # vecinas, por definición tendran mucha similaridad.\n  plt.subplot(2,1,1)\n  plt.plot(dots)\n  plt.ylim([0,1])\n  plt.plot([950, 950, float('nan'), 1050, 1050],\n          [0,1,float('nan'),0,1], color='k', label='Zoom')\n  plt.legend()\n  plt.subplot(2,1,2)\n  plt.plot(dots)\n  plt.xlim([950, 1050])\n  plt.ylim([0,1])\n\ndef plot_distribucion_longitudes_tokens(all_lengths):\n  plt.hist(all_lengths, np.linspace(0, 500, 101))\n  plt.ylim(plt.ylim())\n  max_length = max(all_lengths)\n  plt.plot([max_length, max_length], plt.ylim())\n  plt.title(f'Número máximo de tokens por muestra: {max_length}');\n\ndef plot_positional_encodings(pos_encoding):\n  # Gráficar las dimensiones\n  plt.pcolormesh(pos_encoding.numpy().T, cmap='RdBu')\n  plt.ylabel('Profundidad')\n  plt.xlabel('Posición')\n  plt.colorbar()\n  plt.show()\n\ndef plot_lr_planificador(learning_rate):\n  plt.plot(learning_rate(tf.range(40000, dtype=tf.float32)))\n  plt.ylabel('Learning Rate');\n  plt.xlabel('Paso de entrenamiento');\n\ndef plot_attention_head(in_tokens, translated_tokens, attention):\n  # Saltar el token start.\n  translated_tokens = translated_tokens[1:]\n\n  ax = plt.gca()\n  ax.matshow(attention)\n  ax.set_xticks(range(len(in_tokens)))\n  ax.set_yticks(range(len(translated_tokens)))\n\n  labels = [label.decode('utf-8') for label in in_tokens.numpy()]\n  ax.set_xticklabels(\n      labels, rotation=90)\n\n  labels = [label.decode('utf-8') for label in translated_tokens.numpy()]\n  ax.set_yticklabels(labels)\n\ndef plot_attention_weights(sentence, translated_tokens, attention_heads):\n  in_tokens = tf.convert_to_tensor([sentence])\n  in_tokens = tokenizers.pt.tokenize(in_tokens).to_tensor()\n  in_tokens = tokenizers.pt.lookup(in_tokens)[0]\n\n  fig = plt.figure(figsize=(12, 15))\n\n  for h, head in enumerate(attention_heads):\n    ax = fig.add_subplot(2, 4, h+1)\n\n    plot_attention_head(in_tokens, translated_tokens, head)\n\n    ax.set_xlabel(f'Head {h+1}')\n\n  plt.tight_layout()\n  plt.show()\n\ndef plot_traduccion(sentence, tokens, ground_truth):\n  print(f'{\"Input:\":15s}: {sentence}')\n  print(f'{\"Predicción\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n  print(f'{\"Ground truth\":15s}: {ground_truth}')\nLos Transformers: Una Nueva Era en el Procesamiento de Secuencias\nLas Redes Neuronales Recurrentes (RNNs) han sido una herramienta fundamental para el procesamiento de datos secuenciales, como el lenguaje natural. Sin embargo, los Transformers, propuestos en el artículo “Attention is all you need”, han revolucionado este campo, ofreciendo ventajas significativas sobre las RNNs. Los Transformers son redes neuronales profundas que reemplazan las CNNs y las RNNs. Estos introducen la auto-atención (self-attention) permite a los Transformers transmitir información fácilmente a través de las secuencias de entrada.\n¿Por qué los Transformers son importantes?\nEn este notebook, exploraremos en profundidad los componentes clave de los Transformers y compararemos su funcionamiento con el de las RNNs, destacando las razones de su superioridad."
  },
  {
    "objectID": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#esquema-de-trabajo",
    "href": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#esquema-de-trabajo",
    "title": "Explicando la Mejora de los Transformers sobre las RNNs",
    "section": "Esquema de trabajo",
    "text": "Esquema de trabajo\nCon lo anterior en mente, y una vez visto un poco las diferencias entre RNNs y Transformers, vamos a abordar los siguientes contenidos de manera detallada:\n\nPrepararás los datos.\nImplementarás los componentes necesarios:\n\nEmbeddings posicionales.\nCapas de atención.\nEl codificador y el decodificador.\n\nConstruirás y entrenarás el Transformer.\nGenerarás traducciones.\nExportarás el modelo.\n\n\nCarga y Preprocesamiento de Datos\nCargaremos un conjunto de datos de traducción Portugués-Inglés y utilizaremos un tokenizador para preparar el texto para el modelo. Este conjunto de datos contiene aproximadamente 52.000 ejemplos de entrenamiento, 1.200 de validación y 1.800 de prueba.\n\nexamples, metadata = tfds.load('ted_hrlr_translate/pt_to_en',\n                               with_info=True,\n                               as_supervised=True)\n\ntrain_examples, val_examples = examples['train'], examples['validation']\n\n\n# Mostrar algunos ejemplos de parejas de oraciones\nfor pt_examples, en_examples in train_examples.batch(3).take(1):\n  print('-&gt; Ejemplos en portugués:')\n  for pt in pt_examples.numpy():\n    print(pt.decode('utf-8'))\n  print()\n\n  print('-&gt; Traducción al inglés:')\n  for en in en_examples.numpy():\n    print(en.decode('utf-8'))\n\n-&gt; Ejemplos en portugués:\ne quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\nmas e se estes fatores fossem ativos ?\nmas eles não tinham a curiosidade de me testar .\n\n-&gt; Traducción al inglés:\nand when you improve searchability , you actually take away the one advantage of print , which is serendipity .\nbut what if it were active ?\nbut they did n't test for curiosity .\n\n\n\nTokenizadores\nEn nuestro ejemplo vamos a usar los tokenizadores construidos en el tutorial subword tokenizer. Ese tutorial optimiza dos objetos text.BertTokenizer (uno para inglés, otro para portugués) para este conjunto de datos y los exporta en formato saved_model de TensorFlow.\n\nmodel_name = 'ted_hrlr_translate_pt_en_converter'\ntf.keras.utils.get_file(\n    f'{model_name}.zip',\n    f'https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip',\n    cache_dir='.', cache_subdir='', extract=True\n)\n\n'./ted_hrlr_translate_pt_en_converter_extracted'\n\n\n\n# cargar los tokenizadores\ntokenizers = tf.saved_model.load(f'{model_name}_extracted/{model_name}')\n\n\n# ambos tokenizadores tienen los mismo métodos\n[item for item in dir(tokenizers.en) if not item.startswith('_')]\n\n['detokenize',\n 'get_reserved_tokens',\n 'get_vocab_path',\n 'get_vocab_size',\n 'lookup',\n 'tokenize',\n 'tokenizer',\n 'vocab']\n\n\n\n# por ejemplo revisemos los vocab\ntokenizers.en.vocab.shape, tokenizers.pt.vocab.shape\n\n(TensorShape([7010]), TensorShape([7765]))\n\n\nEl método tokenize convierte un grupo de oraciones en un grupo de identificadores de tokens de una misma longitud (padding). Este método separa los signos de puntuación, las minúsculas y normaliza el texto de entrada antes de la tokenización. Esta normalización no es visible aquí porque los datos de entrada ya están normalizados.\n\nprint('-&gt; Esto es un grupo de cadenas:')\nfor en in en_examples.numpy():\n  print(en.decode('utf-8'))\n\n-&gt; Esto es un grupo de cadenas:\nand when you improve searchability , you actually take away the one advantage of print , which is serendipity .\nbut what if it were active ?\nbut they did n't test for curiosity .\n\n\n\nencoded = tokenizers.en.tokenize(en_examples)\n\nprint('-&gt; Este es el grupo de cedena de ID de tokens (con padding)')\nfor row in encoded.to_list():\n  print(row)\n\n-&gt; Este es el grupo de cedena de ID de tokens (con padding)\n[2, 72, 117, 79, 1259, 1491, 2362, 13, 79, 150, 184, 311, 71, 103, 2308, 74, 2679, 13, 148, 80, 55, 4840, 1434, 2423, 540, 15, 3]\n[2, 87, 90, 107, 76, 129, 1852, 30, 3]\n[2, 87, 83, 149, 50, 9, 56, 664, 85, 2512, 15, 3]\n\n\nEl método detokenize convierte los ID de token de nuevo en texto legible normal:\n\nround_trip = tokenizers.en.detokenize(encoded)\n\nprint('-&gt; Correspondiente texto:')\nfor line in round_trip.numpy():\n  print(line.decode('utf-8'))\n\n-&gt; Correspondiente texto:\nand when you improve searchability , you actually take away the one advantage of print , which is serendipity .\nbut what if it were active ?\nbut they did n ' t test for curiosity .\n\n\nEl método lookup convierte de token-IDs a token-texto:\n\nprint('-&gt; Este es el texto dividido en tokens:')\ntokens = tokenizers.en.lookup(encoded)\ntokens\n\n-&gt; Este es el texto dividido en tokens:\n\n\n&lt;tf.RaggedTensor [[b'[START]', b'and', b'when', b'you', b'improve', b'search', b'##ability',\n  b',', b'you', b'actually', b'take', b'away', b'the', b'one', b'advantage',\n  b'of', b'print', b',', b'which', b'is', b's', b'##ere', b'##nd', b'##ip',\n  b'##ity', b'.', b'[END]']                                                 ,\n [b'[START]', b'but', b'what', b'if', b'it', b'were', b'active', b'?',\n  b'[END]']                                                           ,\n [b'[START]', b'but', b'they', b'did', b'n', b\"'\", b't', b'test', b'for',\n  b'curiosity', b'.', b'[END]']                                          ]&gt;\n\n\nLa salida demuestra el aspecto de \"subword\" de la tokenización de subpalabras.\nPor ejemplo, la palabra 'searchability' se descompone en 'search' y '##ability', y la palabra 'serendipity' en 's', '##ere', '##nd', '##ip' e '##ity'.\nTen en cuenta que el texto tokenizado incluye los tokens '[START]' y '[END]'.\nLa distribución de tokens por ejemplo en el conjunto de datos es la siguiente:\n\nlengths = []\n\nfor pt_examples, en_examples in train_examples.batch(1024):\n  pt_tokens = tokenizers.pt.tokenize(pt_examples)\n  lengths.append(pt_tokens.row_lengths())\n\n  en_tokens = tokenizers.en.tokenize(en_examples)\n  lengths.append(en_tokens.row_lengths())\n  print('.', end='', flush=True)\n\n...................................................\n\n\n\nall_lengths = np.concatenate(lengths)\n\nplot_distribucion_longitudes_tokens(all_lengths)\n\n\n\n\n\n\n\n\n\n\nConfiguración del pipeline de datos usando tf.data\nLa siguiente función toma batches de texto como entrada y los convierte a un formato adecuado para el entrenamiento.\n\nLos tokeniza en lotes de diferentes dimensiones (ragged).\nRecorta cada uno para que no tenga más de MAX_TOKENS.\nDivide los tokens objetivo (inglés) en entradas y etiquetas. Estos se desplazan un paso de modo que en cada ubicación de entrada, la etiqueta es el ID del siguiente token.\nConvierte los RaggedTensor en Tensor densos con padding.\nDevuelve un par (entradas, etiquetas).\n\n\nMAX_TOKENS=64\ndef prepare_batch(pt, en):\n    pt = tokenizers.pt.tokenize(pt) # la sálida tiene diferentes longitudes\n    pt = pt[:, :MAX_TOKENS]    # Truncar al max # de tokens\n    pt = pt.to_tensor()  # Convertir a un tensor denso sin padding\n\n    en = tokenizers.en.tokenize(en)\n    en = en[:, :(MAX_TOKENS+1)] # para poder hacer el shift\n    en_inputs = en[:, :-1].to_tensor()  # Elimina los tokens [END]\n    en_labels = en[:, 1:].to_tensor()   # Eliminar los tokens [START]\n\n    return (pt, en_inputs), en_labels\n\nLa siguiente función convierte un conjunto de datos de ejemplos de texto en datos de lotes para el entrenamiento.\n\nTokeniza el texto y filtra las secuencias que son demasiado largas.\nEl método cache asegura que ese trabajo solo se ejecute una vez.\nLuego, shuffle y preparar el batch.\nFinalmente, prefetch ejecuta el conjunto de datos en paralelo con el modelo para asegurar que los datos estén disponibles cuando se necesiten. Consulta Mejorar rendimiento con tf.data para más detalles.\n\n\nBUFFER_SIZE = 10000\nBATCH_SIZE = 64\n\n\ndef make_batches(ds):\n  return (\n      ds\n      .shuffle(BUFFER_SIZE)\n      .batch(BATCH_SIZE)\n      .map(prepare_batch, tf.data.AUTOTUNE)\n      .prefetch(buffer_size=tf.data.AUTOTUNE))\n\n\n\nProbar nuestro pipeline de datos\n\n# Crear los conjuntos de entrenamiento y validación\ntrain_batches = make_batches(train_examples)\nval_batches = make_batches(val_examples)\n\nComo se verían las entradas y sálidas en nuestro pipeline de datos?\n\n\n\nInputs en la parte inferior, labels en la parte superior.\n\n\n\n\n\n\n\n\nEsta configuración se llama “teacher forcing” porque, independientemente de la salida del modelo en cada paso de tiempo, recibe el valor verdadero como entrada para el siguiente paso de tiempo. Esta es una forma simple y eficiente de entrenar un modelo de generación de texto. Es eficiente porque no necesitas ejecutar el modelo secuencialmente; las salidas en las diferentes ubicaciones de la secuencia se pueden calcular en paralelo.\n\n# visualizar un ejemplo de nuestros datos para entrenar\nfor (pt, en), en_labels in train_batches.take(1):\n  break\n\nprint(pt.shape)\nprint(en.shape)\nprint(en_labels.shape)\n\n(64, 64)\n(64, 64)\n(64, 64)\n\n\nLas etiquetas en y en_labels son las mismas, sólo que desplazadas en 1:\n\nprint(en[0][:10])\nprint(en_labels[0][:10])\n\ntf.Tensor([ 2 90 80 81 85 30  0  0  0  0], shape=(10,), dtype=int64)\ntf.Tensor([90 80 81 85 30  3  0  0  0  0], shape=(10,), dtype=int64)\n\n\n\n\n\nDefinir los componentes del Transformer\nDentro de un Transformer pasan muchas cosas. Las cosas importantes que hay que recordar son:\n\nSigue el mismo patrón general que un modelo estándar secuencia-a-secuencia con un codificador y un decodificador.\nSi trabajas paso a paso, todo tendrá sentido.\n\n\n\n\nDiagrama original del Transformer\n\n\nRepresentación de un Transformer de 4 capas\n\n\n\n\n\n\n\n\n\n\n\n\nLa capa para la codificación de la posición\nLas entradas tanto del codificador como del descodificador utilizan la misma lógica de incrustación y codificación posicional\n\n\n\nThe embedding and positional encoding layer\n\n\n\n\n\n\n\n\nDada una secuencia de tokens, tanto los tokens de entrada (portugués) como los tokens objetivo (inglés) deben convertirse en vectores utilizando una capa tf.keras.layers.Embedding.\nLas capas de atención utilizadas en todo el modelo ven su entrada como un conjunto de vectores, sin ningún orden. Dado que el modelo no contiene ninguna capa recurrente o convolucional, necesita alguna forma de identificar el orden de las palabras; de lo contrario, vería la secuencia de entrada como una instancia de bolsa de palabras, cómo estás, cómo tú estás, tú cómo estás, y así sucesivamente, son indistinguibles.\nPor lo tanto, el Transformer agrega una “Codificación Posicional” a los vectores de embedding. Utiliza un conjunto de senos y cosenos en diferentes frecuencias (a lo largo de la secuencia). Por definición, los elementos cercanos tendrán codificaciones posicionales similares.\nEl paper original usa la siguiente formúla para la codificación posicional:\n\\[\\Large{PE_{(pos, 2i)} = \\sin(pos / 10000^{2i / d_{model}})} \\] \\[\\Large{PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i / d_{model}})} \\]\nNota: El código a continuación lo implementa, pero en lugar de intercalar los senos y cosenos, los vectores de senos y cosenos simplemente se concatenan. Permutar los canales de esta manera es funcionalmente equivalente, y un poco más fácil de implementar y mostrar en las gráficas siguientes.\nExplicación Paso a Paso: Imaginemos que tenemos una frase: “El gato está en la alfombra”.\nY supongamos que d_model = 4. Esto significa que cada posición (cada palabra) se representará con un vector de 4 números.\n\nPosiciones:\n\n\n“El” está en la posición 0.\n“gato” está en la posición 1.\n“está” está en la posición 2.\n“en” está en la posición 3.\n“la” está en la posición 4.\n“alfombra” está en la posición 5.\n\n\nDimensiones:\n\nComo d_model = 4, tendremos dimensiones 0, 1, 2, y 3 en nuestro vector de codificación posicional.\n\nAplicando la Fórmula (Ejemplo: la palabra “gato” en la posición 1):\n\n\nPara la dimensión 0 (2i = 0, entonces i = 0):\n\nPE(1, 0) = sin(1 / 10000^(2*0 / 4)) = sin(1 / 10000^0) = sin(1 / 1) = sin(1) ≈ 0.841\n\nPara la dimensión 1 (2i + 1 = 1, entonces i = 0):\n\nPE(1, 1) = cos(1 / 10000^(2*0 / 4)) = cos(1 / 10000^0) = cos(1 / 1) = cos(1) ≈ 0.540\n\nPara la dimensión 2 (2i = 2, entonces i = 1):\n\nPE(1, 2) = sin(1 / 10000^(2*1 / 4)) = sin(1 / 10000^(1/2)) = sin(1 / 100) = sin(0.01) ≈ 0.01\n\nPara la dimensión 3 (2i + 1 = 3, entonces i = 1):\n\nPE(1, 3) = cos(1 / 10000^(2*1 / 4)) = cos(1 / 10000^(1/2)) = cos(1 / 100) = cos(0.01) ≈ 0.99995\nEntonces, la codificación posicional para la palabra “gato” (posición 1) sería aproximadamente el vector: [0.841, 0.540, 0.01, 0.99995].\n\ndef positional_encoding(length, depth):\n  depth = depth/2 # mitad de las dimensiones para cada función\n\n  positions = np.arange(length)[:, np.newaxis]     # shape = (seq, 1)\n  depths = np.arange(depth)[np.newaxis, :]/depth   # shape = (1, depth)\n\n  angle_rates = 1 / (10000**depths)         # (1, depth)\n  angle_rads = positions * angle_rates      # (pos, depth)\n\n  pos_encoding = np.concatenate(\n      [np.sin(angle_rads), np.cos(angle_rads)],\n      axis=-1)\n\n  return tf.cast(pos_encoding, dtype=tf.float32)\n\nLa función de codificación de posición es una pila de senos y cosenos que vibran a distintas frecuencias según su ubicación a lo largo de la profundidad del vector de incrustación. Vibran a través del eje de posición.\n\n# para el ejemplo de nuestra frase: El gato esta en la alfombra\n# en este caso se concatenaron las funciones\npositional_encoding(length=6, depth=4)\n\n&lt;tf.Tensor: shape=(6, 4), dtype=float32, numpy=\narray([[ 0.        ,  0.        ,  1.        ,  1.        ],\n       [ 0.84147096,  0.00999983,  0.5403023 ,  0.99995   ],\n       [ 0.9092974 ,  0.01999867, -0.41614684,  0.9998    ],\n       [ 0.14112   ,  0.0299955 , -0.9899925 ,  0.99955004],\n       [-0.7568025 ,  0.03998933, -0.6536436 ,  0.9992001 ],\n       [-0.9589243 ,  0.04997917,  0.2836622 ,  0.99875027]],\n      dtype=float32)&gt;\n\n\n\npos_encoding = positional_encoding(length=2048, depth=512)\n\n# Revisar la dimensión\nprint(pos_encoding.shape)\n\nplot_positional_encodings(pos_encoding)\n\n(2048, 512)\n\n\n\n\n\n\n\n\n\nPor definición, estos vectores se alinean bien con los vectores cercanos a lo largo del eje de posición. A continuación, los vectores de codificación de posición se normalizan y el vector de la posición 1000 se compara, por producto punto, con todos los demás:\n\nplot_similaridad_positional_encodings(pos_encoding)\n\n\n\n\n\n\n\n\nAhora creemos la capa: PositionEmbedding\n\nclass PositionalEmbedding(tf.keras.layers.Layer):\n  def __init__(self, vocab_size, d_model):\n    super().__init__()\n    self.d_model = d_model\n    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n\n  def compute_mask(self, *args, **kwargs):\n    return self.embedding.compute_mask(*args, **kwargs)\n\n  def call(self, x):\n    length = tf.shape(x)[1]\n    x = self.embedding(x)\n    # Este factor establece la escala relativa de la incrustación y la codificación_positonal.\n    # Es decir asegurar que tengan escalas similares\n    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n    # Se suma las posiciones a los embeddings de los tokens\n    x = x + self.pos_encoding[tf.newaxis, :length, :]\n    return x\n\n\nembed_pt = PositionalEmbedding(vocab_size=tokenizers.pt.get_vocab_size().numpy(), d_model=512)\nembed_en = PositionalEmbedding(vocab_size=tokenizers.en.get_vocab_size().numpy(), d_model=512)\n\npt_emb = embed_pt(pt)\nen_emb = embed_en(en)\n\n\n# la máscara de cada oración, recordar que las oraciones no tiene la misma\n# longitud, asi que se aplica la máscara\nen_emb._keras_mask\n\n&lt;tf.Tensor: shape=(64, 64), dtype=bool, numpy=\narray([[ True,  True,  True, ..., False, False, False],\n       [ True,  True,  True, ..., False, False, False],\n       [ True,  True,  True, ..., False, False, False],\n       ...,\n       [ True,  True,  True, ..., False, False, False],\n       [ True,  True,  True, ..., False, False, False],\n       [ True,  True,  True, ..., False, False, False]])&gt;\n\n\n\npt_emb\n\n&lt;tf.Tensor: shape=(64, 64, 512), dtype=float32, numpy=\narray([[[ 0.81105447, -0.7717942 ,  0.5229695 , ...,  0.02146667,\n          0.24027777,  0.26449662],\n        [ 1.8670975 ,  1.6279981 ,  0.97901094, ...,  1.3392618 ,\n          0.8188071 ,  0.21744752],\n        [-0.11729413,  0.5008898 ,  0.32620972, ...,  0.27159345,\n          1.1697826 ,  1.4658518 ],\n        ...,\n        [-1.3773707 ,  0.633837  ,  1.1719698 , ...,  1.6278234 ,\n          0.05931401,  0.90110606],\n        [-1.1504335 , -0.23321328,  1.8688756 , ...,  1.6278226 ,\n          0.0593133 ,  0.90110534],\n        [-0.24389721, -0.9982976 ,  1.8318357 , ...,  1.6278219 ,\n          0.05931258,  0.9011047 ]],\n\n       [[ 0.81105447, -0.7717942 ,  0.5229695 , ...,  0.02146667,\n          0.24027777,  0.26449662],\n        [-0.03075731,  0.79514253,  1.7891788 , ...,  0.0534128 ,\n          0.01318026,  0.86922586],\n        [ 1.0960544 ,  0.75714344,  0.7072108 , ...,  0.3786983 ,\n          1.0271425 ,  1.3102653 ],\n        ...,\n        [-1.3773707 ,  0.633837  ,  1.1719698 , ...,  1.6278234 ,\n          0.05931401,  0.90110606],\n        [-1.1504335 , -0.23321328,  1.8688756 , ...,  1.6278226 ,\n          0.0593133 ,  0.90110534],\n        [-0.24389721, -0.9982976 ,  1.8318357 , ...,  1.6278219 ,\n          0.05931258,  0.9011047 ]],\n\n       [[ 0.81105447, -0.7717942 ,  0.5229695 , ...,  0.02146667,\n          0.24027777,  0.26449662],\n        [ 1.7584805 ,  1.4909694 , -0.1760881 , ...,  2.1180818 ,\n          0.4953122 ,  0.23008263],\n        [ 0.55767405,  1.4934946 ,  0.02770001, ...,  0.84077555,\n          0.9217277 ,  1.9566159 ],\n        ...,\n        [-1.3773707 ,  0.633837  ,  1.1719698 , ...,  1.6278234 ,\n          0.05931401,  0.90110606],\n        [-1.1504335 , -0.23321328,  1.8688756 , ...,  1.6278226 ,\n          0.0593133 ,  0.90110534],\n        [-0.24389721, -0.9982976 ,  1.8318357 , ...,  1.6278219 ,\n          0.05931258,  0.9011047 ]],\n\n       ...,\n\n       [[ 0.81105447, -0.7717942 ,  0.5229695 , ...,  0.02146667,\n          0.24027777,  0.26449662],\n        [ 0.7150625 ,  1.6491615 ,  1.213108  , ...,  1.0735046 ,\n          0.3283956 ,  0.01745564],\n        [ 1.1696244 ,  1.2925339 ,  1.6480486 , ...,  0.24500364,\n          1.4749924 ,  1.9941585 ],\n        ...,\n        [-1.3773707 ,  0.633837  ,  1.1719698 , ...,  1.6278234 ,\n          0.05931401,  0.90110606],\n        [-1.1504335 , -0.23321328,  1.8688756 , ...,  1.6278226 ,\n          0.0593133 ,  0.90110534],\n        [-0.24389721, -0.9982976 ,  1.8318357 , ...,  1.6278219 ,\n          0.05931258,  0.9011047 ]],\n\n       [[ 0.81105447, -0.7717942 ,  0.5229695 , ...,  0.02146667,\n          0.24027777,  0.26449662],\n        [-0.23110986,  0.5054298 , -0.25015187, ...,  1.1486163 ,\n          1.1958522 ,  1.1460018 ],\n        [ 1.4417007 , -0.06407106,  0.7872464 , ...,  0.72261333,\n          0.1937148 ,  2.0049632 ],\n        ...,\n        [-1.3773707 ,  0.633837  ,  1.1719698 , ...,  1.6278234 ,\n          0.05931401,  0.90110606],\n        [-1.1504335 , -0.23321328,  1.8688756 , ...,  1.6278226 ,\n          0.0593133 ,  0.90110534],\n        [-0.24389721, -0.9982976 ,  1.8318357 , ...,  1.6278219 ,\n          0.05931258,  0.9011047 ]],\n\n       [[ 0.81105447, -0.7717942 ,  0.5229695 , ...,  0.02146667,\n          0.24027777,  0.26449662],\n        [ 1.7306108 ,  1.4167533 ,  1.2637417 , ..., -0.04352736,\n          0.04924804,  0.62587   ],\n        [ 0.03706914,  0.90970105,  1.9453614 , ...,  0.0534128 ,\n          0.01318026,  0.86922586],\n        ...,\n        [-1.3773707 ,  0.633837  ,  1.1719698 , ...,  1.6278234 ,\n          0.05931401,  0.90110606],\n        [-1.1504335 , -0.23321328,  1.8688756 , ...,  1.6278226 ,\n          0.0593133 ,  0.90110534],\n        [-0.24389721, -0.9982976 ,  1.8318357 , ...,  1.6278219 ,\n          0.05931258,  0.9011047 ]]], dtype=float32)&gt;\n\n\n\n\nCapas de Adición y normalización\n\n\n\nAdd y normalize\n\n\n\n\n\n\n\n\nEstos bloques de “Add & Norm” se encuentran distribuidos a lo largo de todo el modelo Transformer. Cada uno combina una conexión residual y pasa el resultado a través de una capa de LayerNormalization.\nLa manera más sencilla de organizar el código es estructurándolo alrededor de estos bloques residuales. En las siguientes secciones, definiremos clases de capas personalizadas para cada uno de ellos.\nLos bloques residuales “Add & Norm” se incluyen para que el entrenamiento sea eficiente. La conexión residual proporciona una ruta directa para el gradiente (y asegura que los vectores sean actualizados por las capas de atención en lugar de ser reemplazados), mientras que la normalización mantiene una escala razonable para las salidas.\nNota: Las implementaciones que se muestran a continuación utilizan la capa Add para asegurar que las máscaras de Keras se propaguen correctamente (el operador + no lo hace).\n\n\nBases de la capa de atención\nLas capas de atención se utilizan a lo largo de todo el modelo Transformer. Todas ellas son idénticas, excepto por cómo se configura la atención. Cada una contiene una capa layers.MultiHeadAttention, una capa layers.LayerNormalization y una capa layers.Add.\n\n\n\nCapa de atención básica\n\n\n\n\n\n\n\n\nPara implementar estas capas de atención, comenzaremos con una clase base simple que sólo contenga definidos los componentes. Cada caso de uso se implementará como una subclase (framework).\n\nclass BaseAttention(tf.keras.layers.Layer):\n  def __init__(self, **kwargs):\n    super().__init__()\n    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n    self.layernorm = tf.keras.layers.LayerNormalization()\n    self.add = tf.keras.layers.Add()\n\n\nAtención, cómo funciona?\nAntes de entrar en los detalles de cada uso, aquí tienes un breve repaso de cómo funciona la atención:\n\n\n\nCapa de atención básica\n\n\n\n\n\n\n\n\nHay dos entradas: 1. La secuencia de consulta (query sequence): la secuencia que se está procesando; la secuencia que “atiende” (abajo). 2. La secuencia de contexto (context sequence): la secuencia a la que se está “atendiendo” (izquierda).\nLa salida tiene la misma forma que la secuencia de consulta.\nUna analogía común es que esta operación se asemeja a una búsqueda en un diccionario. Una búsqueda en un diccionario difuso, diferenciable y vectorizado.\nAquí tienes un diccionario regular de Python, con 3 claves y 3 valores al que se le pasa una única consulta.\nd = {'color': 'azul', 'edad': 22, 'tipo': 'pickup'}\nresult = d['color']\n\nEl query es lo que se esta tratando de encontrar.\nThe key es el tipo de información que tiene el diccionario\nThe value es la información\n\nCuando buscas una query (consulta) en un diccionario normal, el diccionario encuentra la key (clave) coincidente y devuelve su value (valor) asociado. La query tiene una key coincidente o no la tiene.\nPuedes imaginar un diccionario difuso donde las claves no tienen que coincidir perfectamente. Si buscaras d[\"especie\"] en el diccionario de arriba, quizás querrías que devolviera \"pickup\" ya que es la mejor coincidencia para la consulta.\nUna capa de atención realiza una búsqueda difusa como esta, pero no solo busca la mejor clave. Combina los values basándose en qué tan bien la query coincide con cada key.\n¿Cómo funciona esto? En una capa de atención, la query, la key y el value son cada uno vectores. En lugar de realizar una búsqueda de asignación, la capa de atención combina los vectores de la query y la key para determinar qué tan bien coinciden, obteniendo una “puntuación de atención” (attention score). La capa devuelve el promedio de todos los values, ponderado por las “puntuaciones de atención”.\nCada posición en la secuencia de consulta (query sequence) proporciona un vector de query. La secuencia de contexto (context sequence) actúa como el diccionario. Cada posición en la secuencia de contexto proporciona un vector de key y un vector de value.\nLos vectores de entrada no se utilizan directamente; la capa layers.MultiHeadAttention incluye capas layers.Dense para proyectar los vectores de entrada antes de utilizarlos.\n\n\nCapa de atención cruzada\nEn el centro literal del Transformer está la capa de atención cruzada. Esta capa conecta el codificador y el decodificador. Esta capa es el uso más directo de la atención en el modelo, realiza la misma tarea que el bloque de atención en el Tutorial NMT con atención.\n\n\n\nAtención cruzada\n\n\n\n\n\n\n\n\nPara implementar esto, pasas la secuencia objetivo x como la query (consulta) y la secuencia de context (contexto) como la key/value (clave/valor) al llamar a la capa mha:\n\nclass CrossAttention(BaseAttention):\n  def call(self, x, context):\n    attn_output, attn_scores = self.mha(\n        query=x,\n        key=context,\n        value=context,\n        return_attention_scores=True)\n\n    # Almacenar los scores de atención para\n    # visualizarlos más adelante\n    self.last_attn_scores = attn_scores\n\n    x = self.add([x, attn_output])\n    x = self.layernorm(x)\n\n    return x\n\nLa siguiente caricatura muestra cómo fluye la información a través de esta capa. Las columnas representan la suma ponderada de la secuencia contextual.\nPara simplificar, no se muestran las conexiones residuales.\n\n\n\nAtención cruzada\n\n\n\n\n\n\n\n\nLa longitud de la salida es la longitud de la secuencia de query (consulta), y no la longitud de la secuencia de key/value (clave/valor) del contexto.\nEl diagrama se simplifica aún más a continuación. No es necesario dibujar la matriz completa de “pesos de atención”.\nEl punto clave es que cada ubicación en la query puede “ver” todos los pares de key/value en el contexto, pero no se intercambia información entre las diferentes consultas.\n\n\n\nCada elemento del query puede ver todo el contexto.\n\n\n\n\n\n\n\n\nEjemplo:\n\nsample_ca = CrossAttention(num_heads=2, key_dim=512)\n\nprint(pt_emb.shape)\nprint(en_emb.shape)\n# cada token de la frase en inglés\n# será operados con todos los tokens de\n# la oración en portugués, esto pasa así\n# solo en entrenamiento\nprint(sample_ca(en_emb, pt_emb).shape)\n\n(64, 64, 512)\n(64, 64, 512)\n(64, 64, 512)\n\n\n\n\nCapa global de auto-atención\nEsta capa se encarga de procesar la secuencia contextual y de propagar la información a lo largo de la misma:\n\n\n\nLa capa global de auto-atención\n\n\n\n\n\n\n\n\nDado que la secuencia contextual es fija mientras se genera la traducción (entrenamiento), se permite que la información fluya en ambas direcciones.\nAntes de los Transformers y la autoatención, los modelos solían utilizar RNNs o CNNs para realizar esta tarea (bidireccionalidad):\n\n\n\nRNNs y CNNs bidireccionales\n\n\n\n\n\n\n\n\n\n\n\n\n\nRNNs and CNNs have their limitations.\nLas RNNs y las CNNs tienen sus limitaciones:\n\nLa RNN permite que la información fluya a lo largo de toda la secuencia, pero atraviesa muchos pasos de procesamiento para llegar allí (limitando el flujo del gradiente). Estos pasos de la RNN deben ejecutarse secuencialmente, por lo que la RNN es menos capaz de aprovechar los dispositivos paralelos modernos.\nEn la CNN, cada ubicación puede procesarse en paralelo, pero solo proporciona un campo receptivo limitado. El campo receptivo solo crece linealmente con el número de capas CNN. Se necesita apilar varias capas convolucionales para transmitir información a través de la secuencia.\n\nPor otro lado, la capa de auto-atención global permite que cada elemento de la secuencia acceda directamente a todos los demás elementos de la secuencia, con solo unas pocas operaciones, y todas las salidas se pueden calcular en paralelo.\nPara implementar esta capa, solo necesitas pasar la secuencia objetivo, x, como los argumentos query (consulta) y value (valor) a la capa mha:\n\nclass GlobalSelfAttention(BaseAttention):\n  def call(self, x):\n    attn_output = self.mha(\n        query=x,\n        value=x,\n        key=x)\n    x = self.add([x, attn_output])\n    x = self.layernorm(x)\n    return x\n\n\nsample_gsa = GlobalSelfAttention(num_heads=2, key_dim=512)\n\nprint(pt_emb.shape)\nprint(sample_gsa(pt_emb).shape)\n\n(64, 64, 512)\n(64, 64, 512)\n\n\nSiguiendo con el mismo estilo de antes, podriamos dibujarlo así:\n\n\n\nLa capa global de auto-atención\n\n\n\n\n\n\n\n\nDe nuevo, las conexiones residuales se omiten para mayor claridad.\nEs más compacto e igual de preciso dibujarlo así:\n\n\n\nLa capa de auto-atención global\n\n\n\n\n\n\n\n\n\n\nCapa de auto-atención causal\nEsta capa realiza un trabajo similar al de la capa de autoatención global, para la secuencia de salida:\n\n\n\nCapa causal de auto-atención\n\n\n\n\n\n\n\n\nThis needs to be handled differently from the encoder’s global self-attention layer.\nLike the text generation tutorial, and the NMT with attention tutorial, Transformers are an “autoregressive” model: They generate the text one token at a time and feed that output back to the input. To make this efficient, these models ensure that the output for each sequence element only depends on the previous sequence elements; the models are “causal”.\nUna RNN unidireccional es causal por definición. Para hacer una convolución causal, solo necesitas rellenar (pad) la entrada y desplazar la salida para que se alinee correctamente (puedes usar layers.Conv1D(padding='causal')).\n\n\n\nCausalidad en RNNs y CNNs\n\n\n\n\n\n\n\n\n\n\n\n\n\nUn modelo causal es eficiente de dos maneras:\n\nEn el entrenamiento, te permite calcular la pérdida para cada posición en la secuencia de salida ejecutando el modelo una sola vez.\nDurante la inferencia, para cada nuevo token generado, solo necesitas calcular sus salidas; las salidas de los elementos de la secuencia anterior se pueden reutilizar.\n\nPara una RNN, solo necesitas el estado de la RNN para tener en cuenta los cálculos previos (pasa return_state=True al constructor de la capa RNN).\nPara una CNN, necesitarías seguir el enfoque de Fast Wavenet.\n\n\nPara construir una capa de auto-atención causal, necesitas usar una máscara apropiada al calcular las puntuaciones de atención y sumar los values de atención.\nEsto se gestiona automáticamente si pasas use_causal_mask = True a la capa MultiHeadAttention cuando la llamas:\n\nclass CausalSelfAttention(BaseAttention):\n  def call(self, x):\n    attn_output = self.mha(\n        query=x,\n        value=x,\n        key=x,\n        use_causal_mask = True)\n    x = self.add([x, attn_output])\n    x = self.layernorm(x)\n    return x\n\nLa máscara causal garantiza que cada lugar sólo tenga acceso a los lugares que le preceden:\n\n\n\nCapa de auto-atención causal.\n\n\n\n\n\n\n\n\nDe nuevo, las conexiones residuales se omiten por simplicidad.\nLa representación más compacta de esta capa sería:\n\n\n\nCapa de auto-atención causal.\n\n\n\n\n\n\n\n\nEjemplo:\n\nsample_csa = CausalSelfAttention(num_heads=2, key_dim=512)\n\nprint(en_emb.shape)\nprint(sample_csa(en_emb).shape)\n\n(64, 64, 512)\n(64, 64, 512)\n\n\nLa salida de los primeros elementos de la secuencia no depende de los elementos posteriores, por lo que no debería importar si recorta los elementos antes o después de aplicar la capa:\n\n# ejemplo donde la sálida debe ser cercana a cero\n# ya que no deberia influir información posterior\nout1 = sample_csa(embed_en(en[:, :3]))\nout2 = sample_csa(embed_en(en))[:, :3]\n\ntf.reduce_max(abs(out1 - out2)).numpy()\n# nota omitir el warning por ahora\n\nnp.float32(7.1525574e-07)\n\n\nNota: Cuando se utilizan máscaras Keras, los valores de salida en lugares no válidos no están bien definidos. Por lo tanto, lo anterior puede no ser válido para las regiones enmascaradas.\n\n\n\nLa red feed forward (MLP)\nEl Transformer también incluye esta red neuronal feed-forward point-wise tanto en el codificador como en el decodificador:\n\n\n\nLa red feed forward (MLP)\n\n\n\n\n\n\n\n\nLa red consiste en dos capas lineales (tf.keras.layers.Dense) con una función de activación ReLU entre ellas, y una capa de dropout. Al igual que con las capas de atención, el código aquí también incluye la conexión residual y la normalización:\n\nclass FeedForward(tf.keras.layers.Layer):\n  def __init__(self, d_model, dff, dropout_rate=0.1):\n    super().__init__()\n    self.seq = tf.keras.Sequential([\n      tf.keras.layers.Dense(dff, activation='relu'),\n      tf.keras.layers.Dense(d_model),\n      tf.keras.layers.Dropout(dropout_rate)\n    ])\n    self.add = tf.keras.layers.Add()\n    self.layer_norm = tf.keras.layers.LayerNormalization()\n\n  def call(self, x):\n    x = self.add([x, self.seq(x)])\n    x = self.layer_norm(x)\n    return x\n\nPrueba:\n\nsample_ffn = FeedForward(512, 2048)\n\nprint(en_emb.shape)\nprint(sample_ffn(en_emb).shape)\n\n(64, 64, 512)\n(64, 64, 512)\n\n\n\n\nLa capa encoder\nEl codificador contiene una pila de N capas de codificador. Donde cada EncoderLayer contiene una capa GlobalSelfAttention y una capa FeedForward:\n\n\n\nLa capa encoder\n\n\n\n\n\n\n\n\nAquí la estructura de la capa EncoderLayer:\n\nclass EncoderLayer(tf.keras.layers.Layer):\n  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n    super().__init__()\n\n    self.self_attention = GlobalSelfAttention(\n        num_heads=num_heads,\n        key_dim=d_model,\n        dropout=dropout_rate)\n\n    self.ffn = FeedForward(d_model, dff)\n\n  def call(self, x):\n    x = self.self_attention(x)\n    x = self.ffn(x)\n    return x\n\nPrueba:\n\nsample_encoder_layer = EncoderLayer(d_model=512, num_heads=8, dff=2048)\n\nprint(pt_emb.shape)\nprint(sample_encoder_layer(pt_emb).shape)\n\n(64, 64, 512)\n(64, 64, 512)\n\n\n\n\nEl módulo Encoder\nConstruyamos el encoder, agregandole la parte de embedings y la codificación posicional.\n\n\n\nEl encoder\n\n\n\n\n\n\n\n\nEl codificador consiste en:\n\nUna capa PositionalEmbedding en la entrada.\nUna pila de capas EncoderLayer.\n\n\nclass Encoder(tf.keras.layers.Layer):\n  def __init__(self, *, num_layers, d_model, num_heads,\n               dff, vocab_size, dropout_rate=0.1):\n    super().__init__()\n\n    self.d_model = d_model\n    self.num_layers = num_layers\n\n    self.pos_embedding = PositionalEmbedding(\n        vocab_size=vocab_size, d_model=d_model)\n\n    self.enc_layers = [\n        EncoderLayer(d_model=d_model,\n                     num_heads=num_heads,\n                     dff=dff,\n                     dropout_rate=dropout_rate)\n        for _ in range(num_layers)]\n    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n\n  def call(self, x):\n    # `x` es token-IDs shape: (batch, seq_len)\n    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n\n    # añadir dropout.\n    x = self.dropout(x)\n\n    for i in range(self.num_layers):\n      x = self.enc_layers[i](x)\n\n    return x  # Shape `(batch_size, seq_len, d_model)`.\n\nProbar:\n\n# Instanciar el Encoder.\nsample_encoder = Encoder(num_layers=4,\n                         d_model=512,\n                         num_heads=8,\n                         dff=2048,\n                         vocab_size=8500)\n# Fijar training en false\nsample_encoder_output = sample_encoder(pt, training=False)\n\n# Dimensiones\nprint(pt.shape)\nprint(sample_encoder_output.shape)  # Shape `(batch_size, input_seq_len, d_model)`.\n# ignorar los warnings\n\n(64, 64)\n(64, 64, 512)\n\n\n\n\nLa capa decoder\nEl decodificador es ligeramente más compleja, con cada DecoderLayer conteniendo una capa CausalSelfAttention, una capa CrossAttention y una capa FeedForward:\n\n\n\nLa capa decoder\n\n\n\n\n\n\n\n\n\nclass DecoderLayer(tf.keras.layers.Layer):\n  def __init__(self,\n               *,\n               d_model,\n               num_heads,\n               dff,\n               dropout_rate=0.1):\n    super(DecoderLayer, self).__init__()\n\n    self.causal_self_attention = CausalSelfAttention(\n        num_heads=num_heads,\n        key_dim=d_model,\n        dropout=dropout_rate)\n\n    self.cross_attention = CrossAttention(\n        num_heads=num_heads,\n        key_dim=d_model,\n        dropout=dropout_rate)\n\n    self.ffn = FeedForward(d_model, dff)\n\n  def call(self, x, context):\n    x = self.causal_self_attention(x=x)\n    x = self.cross_attention(x=x, context=context)\n\n    # Solo para efectos de visualización posterior\n    self.last_attn_scores = self.cross_attention.last_attn_scores\n\n    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n    return x\n\nProbar la capa del decoder:\n\nsample_decoder_layer = DecoderLayer(d_model=512, num_heads=8, dff=2048)\n\nsample_decoder_layer_output = sample_decoder_layer(\n    x=en_emb, context=pt_emb)\n\nprint(en_emb.shape)\nprint(pt_emb.shape)\nprint(sample_decoder_layer_output.shape)  # `(batch_size, seq_len, d_model)`\n\n(64, 64, 512)\n(64, 64, 512)\n(64, 64, 512)\n\n\n\n\nEl módulo decoder\nDe forma similar al Codificador, el Decodificador consiste en un PositionalEmbedding y una pila de DecoderLayer’s:\n\n\n\nCapa Decoder + Embedding + PE\n\n\n\n\n\n\n\n\nDefinimos el Decoder extendiendo tf.keras.layers.Layer:\n\nclass Decoder(tf.keras.layers.Layer):\n  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n               dropout_rate=0.1):\n    super(Decoder, self).__init__()\n\n    self.d_model = d_model\n    self.num_layers = num_layers\n\n    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n                                             d_model=d_model)\n    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n    self.dec_layers = [\n        DecoderLayer(d_model=d_model, num_heads=num_heads,\n                     dff=dff, dropout_rate=dropout_rate)\n        for _ in range(num_layers)]\n\n    self.last_attn_scores = None\n\n  def call(self, x, context):\n    # `x` es token-IDs shape (batch, target_seq_len)\n    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n\n    x = self.dropout(x)\n\n    for i in range(self.num_layers):\n      x  = self.dec_layers[i](x, context)\n\n    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n\n    # EL shape de x es (batch_size, target_seq_len, d_model).\n    return x\n\nProbar:\n\n# Instanciar el decoder\nsample_decoder = Decoder(num_layers=4,\n                         d_model=512,\n                         num_heads=8,\n                         dff=2048,\n                         vocab_size=8000)\n\noutput = sample_decoder(\n    x=en,\n    context=pt_emb)\n\n# Shapes.\nprint(en.shape)\nprint(pt_emb.shape)\nprint(output.shape)\n\n(64, 64)\n(64, 64, 512)\n(64, 64, 512)\n\n\n\nsample_decoder.last_attn_scores.shape  # (batch, heads, target_seq, input_seq)\n\nTensorShape([64, 8, 64, 64])\n\n\nUna vez creados el codificador y el decodificador Transformer, es hora de construir el modelo Transformer y entrenarlo."
  },
  {
    "objectID": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#el-transformer",
    "href": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#el-transformer",
    "title": "Explicando la Mejora de los Transformers sobre las RNNs",
    "section": "El Transformer",
    "text": "El Transformer\nYou now have Encoder and Decoder. To complete the Transformer model, you need to put them together and add a final linear (Dense) layer which converts the resulting vector at each location into output token probabilities.\nThe output of the decoder is the input to this final linear layer.\n\n\n\nEl transformer\n\n\n\n\n\n\n\n\nUn Transformer con una capa tanto en el Codificador como en el Decodificador se parece casi exactamente al modelo del tutorial de RNN+atención. Un Transformer multi-capa tiene más capas, pero fundamentalmente está haciendo lo mismo.\n\n\n\nTransformer de una capa\n\n\nTransformer de 4 capas\n\n\n\n\n\n\n\n\n\n\n\n\nRNN + Modelo de atención\n\n\n\n\n\n\n\n\nCrea el Transformer extendiendo tf.keras.Model:\n\nNota: El artículo original, sección 3.4, comparte la matriz de pesos entre la capa de embedding y la capa lineal final. Para mantener las cosas simples, este tutorial utiliza dos matrices de pesos separadas.\n\n\nclass Transformer(tf.keras.Model):\n  def __init__(self, *, num_layers, d_model, num_heads, dff,\n               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n    super().__init__()\n    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n                           num_heads=num_heads, dff=dff,\n                           vocab_size=input_vocab_size,\n                           dropout_rate=dropout_rate)\n\n    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n                           num_heads=num_heads, dff=dff,\n                           vocab_size=target_vocab_size,\n                           dropout_rate=dropout_rate)\n\n    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n\n  def call(self, inputs):\n    # Para usar el `.fit` del modelo keras usted debe pasar\n    # todas las inputs como el primer argumento\n    context, x  = inputs\n\n    context = self.encoder(context)  # (batch_size, context_len, d_model)\n\n    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n\n    # Capa final densa lineal\n    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n\n    try:\n      # Eliminar las máscaras para que no afecten el cálculo del loss y métricas\n      # b/250038731 --&gt; relacionado con este bug que fue reportado\n      del logits._keras_mask\n    except AttributeError:\n      pass\n\n    # retornar la salida final\n    return logits\n\n\nHiperparámetros\nPara mantener este ejemplo pequeño y relativamente rápido, el número de capas (num_layers), la dimensionalidad de los embeddings (d_model) y la dimensionalidad interna de la capa FeedForward (dff) se han reducido.\nEl modelo base descrito en el artículo original del Transformer utilizaba num_layers=6, d_model=512 y dff=2048.\nEl número de cabezas de auto-atención será (num_heads=4).\n\nnum_layers = 2\nd_model = 128\ndff = 256\nnum_heads = 4\ndropout_rate = 0.1\n\n\n\nProbemos el transformer\nInstanciar el modelo Transformer:\n\ntransformer = Transformer(\n    num_layers=num_layers,\n    d_model=d_model,\n    num_heads=num_heads,\n    dff=dff,\n    input_vocab_size=tokenizers.pt.get_vocab_size().numpy(),\n    target_vocab_size=tokenizers.en.get_vocab_size().numpy(),\n    dropout_rate=dropout_rate)\n\nProbar\n\noutput = transformer((pt, en))\n\nprint(en.shape)\nprint(pt.shape)\nprint(output.shape)\n\n(64, 64)\n(64, 64)\n(64, 64, 7010)\n\n\n\n# acceder a los scores de atención\nattn_scores = transformer.decoder.dec_layers[-1].last_attn_scores\nprint(attn_scores.shape)  # (batch, heads, target_seq, input_seq)\n\n(64, 4, 64, 64)"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#training",
    "href": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#training",
    "title": "Explicando la Mejora de los Transformers sobre las RNNs",
    "section": "Training",
    "text": "Training\nTiempo de entrenar!!\n\nConfigurar el optimizador\nUtilizar el optimizador Adam con un planificador personalizado para la tasa de aprendizaje según la fórmula del Transformer original. paper.\n\\[\\Large{lrate = d_{model}^{-0.5} * \\min(step{\\_}num^{-0.5}, step{\\_}num \\cdot warmup{\\_}steps^{-1.5})}\\]\n\n\nExplicación esquema de Learning Rate del Transformer\nParámetros:\n\nd_model (Dimensionalidad del Modelo):\n\nIntuición: “Ancho” del modelo. Mayor d_model = representaciones más ricas, más capacidad.\nEfecto: Actúa como un factor de escala base para el learning rate.\n\nSi d_model es grande, este factor será pequeño, lo que tiende a reducir el learning rate general. La intuición es que modelos más grandes pueden necesitar pasos de aprendizaje más pequeños para evitar inestabilidad debido a su mayor número de parámetros.\nSi d_model es pequeño, este factor será más grande, lo que tiende a aumentar el learning rate general.\n\n\nstep_num (Número de Paso de Entrenamiento):\n\nIntuición: Progreso del entrenamiento (iteración actual).\nEfecto: Determina la fase del learning rate:\n\nFase de Decaimiento (\\({step\\\\_num}^{-0.5}\\)): Este término hace que el learning rate disminuya a medida que avanza el entrenamiento. La intuición es que al principio queremos dar pasos de aprendizaje más grandes para explorar el espacio de parámetros rápidamente. A medida que nos acercamos a un buen conjunto de pesos, queremos dar pasos más pequeños para “afinar” los valores y evitar oscilar alrededor del mínimo óptimo. La disminución es más pronunciada al principio y se ralentiza con el tiempo.\nFase de Calentamiento (\\({step\\\\_num * warmup\\\\_steps}^{-1.5}\\)): Este término está activo principalmente durante la fase de “calentamiento”. Hace que el learning rate aumente linealmente con el número de paso hasta que step_num se acerca a warmup_steps.\n\n\nwarmup_steps (Pasos de Calentamiento):\n\nIntuición: Duración de la fase inicial de aumento gradual del learning rate.\nEfecto: Controla cuántos pasos se tarda en alcanzar el learning rate “base”. Ayuda a evitar inestabilidad al inicio.\n\n\nEn resumen:\nEl learning rate:\n\nAumenta gradualmente durante los primeros warmup_steps para estabilizar el entrenamiento inicial.\nDisminuye gradualmente después de warmup_steps para permitir una convergencia fina.\nSu magnitud general está influenciada por la dimensionalidad del modelo (d_model).\n\n\nclass CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n  def __init__(self, d_model, warmup_steps=4000):\n    super().__init__()\n\n    self.d_model = d_model\n    self.d_model = tf.cast(self.d_model, tf.float32)\n\n    self.warmup_steps = warmup_steps\n\n  def __call__(self, step):\n    step = tf.cast(step, dtype=tf.float32)\n    arg1 = tf.math.rsqrt(step)\n    arg2 = step * (self.warmup_steps ** -1.5)\n\n    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n\nInstanciar el optimizador (tf.keras.optimizers.Adam):\n\nlearning_rate = CustomSchedule(d_model)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n                                     epsilon=1e-9)\n\nProbar el scheduler para el learning rate:\n\nplot_lr_planificador(learning_rate)\n\n\n\n\n\n\n\n\n\n\nAjustar función de pérdida y métricas\nDado que las secuencias objetivo están rellenadas (padded), es importante aplicar una máscara de padding al calcular la pérdida. Utiliza la función de pérdida de entropía cruzada (tf.keras.losses.SparseCategoricalCrossentropy):\n\ndef masked_loss(label, pred):\n  mask = label != 0 # Indica padding\n  # from_logits=True indica que las predicciones no han pasado por softmax.\n  # reduction='none' hace que se devuelva la pérdida por cada ejemplo.\n  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n      from_logits=True, reduction='none')\n\n  loss = loss_object(label, pred)\n  mask = tf.cast(mask, dtype=loss.dtype)\n  # Aplica la máscara a la pérdida, multiplicando las pérdidas de las posiciones\n  # de padding por cero, lo que las elimina del cálculo total.\n  loss *= mask\n\n  # Calcula la pérdida promedio solo sobre las posiciones no enmascaradas.\n  # Suma todas las pérdidas y divide por el número de posiciones no enmascaradas.\n  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n  return loss\n\n\ndef masked_accuracy(label, pred):\n  # Obtiene la clase predicha con la probabilidad más alta (el índice máximo)\n  # a lo largo del eje de vocabulario (axis=2).\n  pred = tf.argmax(pred, axis=2)\n  # Convierte las etiquetas al mismo tipo de dato que las predicciones para comparar.\n  label = tf.cast(label, pred.dtype)\n  # Crea un tensor booleano donde True indica que la predicción coincide con la etiqueta.\n  match = label == pred\n\n  # Crea una máscara donde True indica que la etiqueta no es padding (no es 0).\n  mask = label != 0\n\n  # Combina la máscara con las coincidencias. Solo consideramos como \"match\"\n  # las predicciones correctas en las posiciones que no son padding.\n  match = match & mask\n\n  # Convierte los booleanos de 'match' y 'mask' a float para poder calcular la media.\n  match = tf.cast(match, dtype=tf.float32)\n  mask = tf.cast(mask, dtype=tf.float32)\n  # Calcula la precisión promedio solo sobre las posiciones no enmascaradas.\n  # Suma las coincidencias (1 para cada predicción correcta no enmascarada)\n  # y divide por el número total de posiciones no enmascaradas.\n  return tf.reduce_sum(match)/tf.reduce_sum(mask)\n\n\n\nEntrenar el modelo\nCon todo listo, vamos a compilar usando model.compile, y luego entrenar con model.fit:\n\ntransformer.compile(\n    loss=masked_loss,\n    optimizer=optimizer,\n    metrics=[masked_accuracy])\n\n\ntransformer.fit(train_batches,\n                epochs=3,\n                validation_data=val_batches)\n\n\nEpoch 1/3\n\n810/810 ━━━━━━━━━━━━━━━━━━━━ 1037s 1s/step - loss: 7.7200 - masked_accuracy: 0.0833 - val_loss: 4.9830 - val_masked_accuracy: 0.2540\n\nEpoch 2/3\n\n810/810 ━━━━━━━━━━━━━━━━━━━━ 531s 633ms/step - loss: 4.6781 - masked_accuracy: 0.2912 - val_loss: 3.9608 - val_masked_accuracy: 0.3668\n\nEpoch 3/3\n\n810/810 ━━━━━━━━━━━━━━━━━━━━ 424s 463ms/step - loss: 3.6996 - masked_accuracy: 0.4002 - val_loss: 3.4028 - val_masked_accuracy: 0.4305\n\n\n\n\n&lt;keras.src.callbacks.history.History at 0x7bffd0607690&gt;"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#ejecutar-inferencia",
    "href": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#ejecutar-inferencia",
    "title": "Explicando la Mejora de los Transformers sobre las RNNs",
    "section": "Ejecutar inferencia",
    "text": "Ejecutar inferencia\nAhora puedes probar el modelo realizando una traducción. Los siguientes pasos se utilizan para la inferencia:\n\nCodifica la frase de entrada utilizando el tokenizador portugués (tokenizers.pt). Esta es la entrada del codificador.\nLa entrada del decodificador se inicializa con el token [START].\nCalcula las máscaras de padding y las máscaras causales (para la auto-atención causal).\nEl decodificador luego genera las predicciones observando la salida del codificador y su propia salida (auto-atención).\nConcatena el token predicho a la entrada del decodificador y lo pasa de nuevo al decodificador.\nEn este enfoque, el decodificador predice el siguiente token basándose en los tokens que predijo previamente.\n\nNota: El modelo está optimizado para un entrenamiento eficiente y realiza una predicción del siguiente token para cada token en la salida simultáneamente. Esto es redundante durante la inferencia, y solo se utiliza la última predicción. Este modelo puede hacerse más eficiente para la inferencia si solo se calcula la última predicción cuando se ejecuta en modo de inferencia (training=False).\nDefine la clase Translator extendiendo tf.Module:\n\nclass Translator(tf.Module):\n  def __init__(self, tokenizers, transformer):\n    self.tokenizers = tokenizers\n    self.transformer = transformer\n\n  def __call__(self, sentence, max_length=MAX_TOKENS):\n    # La frase de entrada es portugués, por lo que se añaden los tokens `[START]` y `[END]`.\n    assert isinstance(sentence, tf.Tensor)\n    if len(sentence.shape) == 0:\n      sentence = sentence[tf.newaxis]\n\n    sentence = self.tokenizers.pt.tokenize(sentence).to_tensor()\n\n    encoder_input = sentence\n\n    # Como el lenguaje de sálida es inglés\n    # Inicializar con el token `[START]`\n    start_end = self.tokenizers.en.tokenize([''])[0]\n    start = start_end[0][tf.newaxis]\n    end = start_end[1][tf.newaxis]\n\n    # con el TensorArray es posible hacer seguimiento al blucle dinámico\n    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n    output_array = output_array.write(0, start)\n\n    for i in tf.range(max_length):\n      # convierte la lista de tokens generados secuencialmente\n      # (almacenada en el TensorArray) en un tensor con la forma (1, secuencia_de_tokens),\n      # donde la secuencia de tokens representa la traducción generada hasta ese punto.\n      output = tf.transpose(output_array.stack())\n      predictions = self.transformer([encoder_input, output], training=False)\n\n      # Seleccionar las predicciones para el último token de `seq_len`.\n      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n      # Encontrar la pos del token id con la mayor probabilidad\n      predicted_id = tf.argmax(predictions, axis=-1)\n\n      # Concatenar la predicción con los anteriores tokens del decoder.\n      output_array = output_array.write(i+1, predicted_id[0])\n\n      if predicted_id == end:\n        break\n\n    output = tf.transpose(output_array.stack())\n    # La dimensión de sálida es `(1, tokens)`.\n    text = tokenizers.en.detokenize(output)[0]  # Shape: `()`.\n\n    tokens = tokenizers.en.lookup(output)[0]\n\n    # `@tf.function` optimiza la función, dificultando el acceso\n    # a valores dinámicos en bucles. Recalculamos la atención\n    # final fuera del bucle para obtener `attention_weights`.\n    self.transformer([encoder_input, output[:,:-1]], training=False)\n    attention_weights = self.transformer.decoder.last_attn_scores\n\n    return text, tokens, attention_weights\n\nNota: Esta función utiliza un bucle unrolled, no un bucle dinámico. Genera MAX_TOKENS en cada llamada. Consulta el tutorial de NMT con atención para ver un ejemplo de implementación con un bucle dinámico, que puede ser mucho más eficiente.\nProbemos la traducción:\n\ntranslator = Translator(tokenizers, transformer)\n\nEjemplo 1:\n\nsentence = 'este é um problema que temos que resolver.'\nground_truth = 'this is a problem we have to solve .'\n\ntranslated_text, translated_tokens, attention_weights = translator(\n    tf.constant(sentence))\nplot_traduccion(sentence, translated_text, ground_truth)\n\nInput:         : este é um problema que temos que resolver.\nPredicción     : this is a problem that we have to solve .\nGround truth   : this is a problem we have to solve .\n\n\nEjemplo 2:\n\nsentence = 'os meus vizinhos ouviram sobre esta ideia.'\nground_truth = 'and my neighboring homes heard about this idea .'\n\ntranslated_text, translated_tokens, attention_weights = translator(\n    tf.constant(sentence))\nplot_traduccion(sentence, translated_text, ground_truth)\n\nInput:         : os meus vizinhos ouviram sobre esta ideia.\nPredicción     : my friends heard about this idea .\nGround truth   : and my neighboring homes heard about this idea .\n\n\nEjemplo 3:\n\nsentence = 'vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.'\nground_truth = \"so i'll just share with you some stories very quickly of some magical things that have happened.\"\n\ntranslated_text, translated_tokens, attention_weights = translator(\n    tf.constant(sentence))\nplot_traduccion(sentence, translated_text, ground_truth)\n\nInput:         : vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.\nPredicción     : i ' m going very quickly quickly quickly to share with some stories of things , things that will be going to be going to be going to be going to be going to be going to be able to be able to be able to be very good .\nGround truth   : so i'll just share with you some stories very quickly of some magical things that have happened."
  },
  {
    "objectID": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#crear-los-plots-de-atención",
    "href": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#crear-los-plots-de-atención",
    "title": "Explicando la Mejora de los Transformers sobre las RNNs",
    "section": "Crear los plots de atención",
    "text": "Crear los plots de atención\nUsando la clase traductor Translator que almacena los scores de atención, podemos usarlos para ver su relevancia:\nPor ejemplo:\n\nsentence = 'este é o primeiro livro que eu fiz.'\nground_truth = \"this is the first book i've ever done.\"\n\ntranslated_text, translated_tokens, attention_weights = translator(\n    tf.constant(sentence))\nplot_traduccion(sentence, translated_text, ground_truth)\n\nInput:         : este é o primeiro livro que eu fiz.\nPredicción     : this is the first book that i did i did .\nGround truth   : this is the first book i've ever done.\n\n\nCrear una función que grafique la atención cuando se genera un token:\n\nhead = 0\n# Shape: `(batch=1, num_heads, seq_len_q, seq_len_k)`.\nattention_heads = tf.squeeze(attention_weights, 0)\nattention = attention_heads[head]\nattention.shape\n\nTensorShape([12, 11])\n\n\nSon las inputs tokens en portugués:\n\nin_tokens = tf.convert_to_tensor([sentence])\nin_tokens = tokenizers.pt.tokenize(in_tokens).to_tensor()\nin_tokens = tokenizers.pt.lookup(in_tokens)[0]\nin_tokens\n\n&lt;tf.Tensor: shape=(11,), dtype=string, numpy=\narray([b'[START]', b'este', b'e', b'o', b'primeiro', b'livro', b'que',\n       b'eu', b'fiz', b'.', b'[END]'], dtype=object)&gt;\n\n\nEstas son las sálidas (tokens en inglés, traducción)\n\ntranslated_tokens\n\n&lt;tf.Tensor: shape=(13,), dtype=string, numpy=\narray([b'[START]', b'this', b'is', b'the', b'first', b'book', b'that',\n       b'i', b'did', b'i', b'did', b'.', b'[END]'], dtype=object)&gt;\n\n\n\nplot_attention_weights(sentence,\n                       translated_tokens,\n                       attention_weights[0])"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#exportar-el-modelo",
    "href": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#exportar-el-modelo",
    "title": "Explicando la Mejora de los Transformers sobre las RNNs",
    "section": "Exportar el modelo",
    "text": "Exportar el modelo\nHemos probado el modelo y la inferencia funciona. A continuación, puedes exportarlo como un tf.saved_model. Para aprender cómo guardar y cargar un modelo en formato SavedModel, consulta esta guía.\nCrea una clase llamada ExportTranslator extendiendo la subclase tf.Module con un @tf.function en el método __call__:\n\nclass ExportTranslator(tf.Module):\n  def __init__(self, translator):\n    self.translator = translator\n\n  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n  def __call__(self, sentence):\n    (result,\n     tokens,\n     attention_weights) = self.translator(sentence, max_length=MAX_TOKENS)\n\n    return result\n\n\n# Empaqueta el objeto `translator` en la nueva clase `ExportTranslator` creada:\ntranslator = ExportTranslator(translator)\n\nDado que el modelo está decodificando las predicciones utilizando tf.argmax, las predicciones son deterministas. El modelo original y uno recargado desde su SavedModel deberían dar predicciones idénticas:\n\ntranslator('este é o primeiro livro que eu fiz.').numpy()\n\nb'this is the first book that i did i did .'\n\n\n\ntf.saved_model.save(translator, export_dir='translator')\n\n\nreloaded = tf.saved_model.load('translator')\n\n\nreloaded('este é o primeiro livro que eu fiz.').numpy()\n\nb'this is the first book that i did i did .'"
  },
  {
    "objectID": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#conclusión",
    "href": "semana_2/notebooks/Nb_2c_Traduccion_usando_transformers_keras.html#conclusión",
    "title": "Explicando la Mejora de los Transformers sobre las RNNs",
    "section": "Conclusión",
    "text": "Conclusión\nEn este tutorial aprendiste sobre:\n\nLos Transformers y su importancia en el aprendizaje automático\nAtención, auto-atención y atención multi-cabeza\nCodificación posicional con embeddings\nLa arquitectura codificador-decodificador del Transformer original\nEnmascaramiento en la auto-atención\nCómo juntar todo para traducir texto\n\nLas desventajas de esta arquitectura son:\n\nPara una serie temporal, la salida para un paso de tiempo se calcula a partir de la historia completa en lugar de solo las entradas y el estado oculto actual. Esto podría ser menos eficiente.\nSi la entrada tiene una relación temporal/espacial, como texto o imágenes, debe añadirse alguna codificación posicional o el modelo efectivamente verá una bolsa de palabras.\n\nEste notebook se basó en el notebook de Neural Machine Translation with a Transformer and Keras para el curso de Deep Learning práctico en 3 semanas."
  },
  {
    "objectID": "semana_3/notebooks/Nb_3a_Introduccion_transfer_learning_finetuning.html",
    "href": "semana_3/notebooks/Nb_3a_Introduccion_transfer_learning_finetuning.html",
    "title": "Introducción a pre-trained models, transfer learning and fine tuning",
    "section": "",
    "text": "Open In Colab\nEn este notebook encontrarás material introductorio para entender los conceptos de modelos preentrenados, transferencia de aprendizaje y ajuste fino. Y cómo estos, empleados de forman correcta, pueden aumentar el rendimiento de tus modelos."
  },
  {
    "objectID": "semana_3/notebooks/Nb_3a_Introduccion_transfer_learning_finetuning.html#transfer-learning-y-fine-tuning-en-tareas-de-visión-por-computadora",
    "href": "semana_3/notebooks/Nb_3a_Introduccion_transfer_learning_finetuning.html#transfer-learning-y-fine-tuning-en-tareas-de-visión-por-computadora",
    "title": "Introducción a pre-trained models, transfer learning and fine tuning",
    "section": "Transfer Learning y fine tuning en tareas de Visión por Computadora",
    "text": "Transfer Learning y fine tuning en tareas de Visión por Computadora\nModelos preentrenados\nEn esta sección, utilizaremos el modelo pre-entrenado VGG16, el cual es una red convolucional que es usada para reconocimiento de imágenes. Este modelo fue entrenado con ImageNet, por lo cual es un modelo bastante robusto.\n\nObjetivo:\n\nEl objetivo será cargar dicho modelo pre-entrenado y utilizarlo sobre el dataset MNIST, el cual contiene 70.000 imagenes de digitos escritos a mano. Así, poder clasificar dichos números con nuestro modelo entrenado previamente.\n\n\n#@title Importar librerías\n# Importamos las librerias a utilizar\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom tensorflow.keras import layers, models, optimizers, callbacks\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.utils import to_categorical\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import backend as K\n\nfrom gensim.models import KeyedVectors\n\n\n%matplotlib inline\n\n\n#@title Definir funciones complementarias\ndef plot_graphs(history, metric):\n  plt.plot(history.history[metric])\n  plt.plot(history.history['val_'+metric], '')\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(metric)\n  plt.legend([metric, 'val_'+metric])\n\ndef cosine_similarity(vec_a, vec_b):\n  \"\"\"Compute cosine similarity between vec_a and vec_b\"\"\"\n  return np.dot(vec_a, vec_b) / (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))\n\n\n# Cargamos el dataset MNIST\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\nprint(\"Train shape: \", train_images.shape)\nprint(\"Test shape: \", test_images.shape)\n\nTrain shape:  (60000, 28, 28)\nTest shape:  (10000, 28, 28)\n\n\n\n# dibujamos ciertos ejemplos de entrenamiento\nplt.figure(figsize=(10, 4))\nfor i in range(10):\n    plt.subplot(2, 5, i + 1)\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(train_images[i], cmap=plt.cm.gray)\n    plt.xlabel(train_labels[i])\nplt.show()\n\n\n\n\n\n\n\n\n\n# Redimensionamos las imagenes a 32x32 (el minimo tamaño que soporta vgg16)\ntrain_images = tf.image.grayscale_to_rgb(tf.expand_dims(train_images, axis=-1))\ntest_images = tf.image.grayscale_to_rgb(tf.expand_dims(test_images, axis=-1))\n# Agregamos 3 canales de color (RGB) debido a que la red tambien lo necesita\ntrain_images = tf.image.resize(train_images, [32, 32])\ntest_images = tf.image.resize(test_images, [32, 32])\n# Normalizamos las imagenes entre [0, 1] para que el aprendizaje sea mas suave\ntrain_images = train_images / 255.0\ntest_images = test_images / 255.0\nprint(\"Train shape: \", train_images.shape)\nprint(\"Test shape: \", test_images.shape)\n\nTrain shape:  (60000, 32, 32, 3)\nTest shape:  (10000, 32, 32, 3)\n\n\n\n# Convertimos las etiquetas a formato one-hot encoding\ntrain_labels_ohc = to_categorical(train_labels, 10)\ntest_labels_ohc = to_categorical(test_labels, 10)\nprint(train_labels_ohc[:,1])\n\n[0. 0. 0. ... 0. 0. 0.]\n\n\n\n# Cargamos el modelo VGG16 preentrenado, excluyendo las capas superiores (top=False)\n# Recuerde que las capas superiores son las que definen el tipo de problema a solucionar\n# Como nuestro problema es de 10 categorias (10 digitos), agregaremos nuestras propias capas superiores\nvgg16_base = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\nvgg16_base.summary()\n\nModel: \"vgg16\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer (InputLayer)             │ (None, 32, 32, 3)           │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ block1_conv1 (Conv2D)                │ (None, 32, 32, 64)          │           1,792 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ block1_conv2 (Conv2D)                │ (None, 32, 32, 64)          │          36,928 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ block1_pool (MaxPooling2D)           │ (None, 16, 16, 64)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ block2_conv1 (Conv2D)                │ (None, 16, 16, 128)         │          73,856 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ block2_conv2 (Conv2D)                │ (None, 16, 16, 128)         │         147,584 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ block2_pool (MaxPooling2D)           │ (None, 8, 8, 128)           │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ block3_conv1 (Conv2D)                │ (None, 8, 8, 256)           │         295,168 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ block3_conv2 (Conv2D)                │ (None, 8, 8, 256)           │         590,080 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ block3_conv3 (Conv2D)                │ (None, 8, 8, 256)           │         590,080 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ block3_pool (MaxPooling2D)           │ (None, 4, 4, 256)           │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ block4_conv1 (Conv2D)                │ (None, 4, 4, 512)           │       1,180,160 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ block4_conv2 (Conv2D)                │ (None, 4, 4, 512)           │       2,359,808 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ block4_conv3 (Conv2D)                │ (None, 4, 4, 512)           │       2,359,808 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ block4_pool (MaxPooling2D)           │ (None, 2, 2, 512)           │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ block5_conv1 (Conv2D)                │ (None, 2, 2, 512)           │       2,359,808 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ block5_conv2 (Conv2D)                │ (None, 2, 2, 512)           │       2,359,808 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ block5_conv3 (Conv2D)                │ (None, 2, 2, 512)           │       2,359,808 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ block5_pool (MaxPooling2D)           │ (None, 1, 1, 512)           │               0 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 14,714,688 (56.13 MB)\n\n\n\n Trainable params: 14,714,688 (56.13 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# Ahora congelamos los pesos del modelo, pues solo queremos agrega una nueva capa\n# con 10 neuronas, donde cada una representará el digito que queremos predecir\nvgg16_base.trainable = False\nmodel = models.Sequential([\n    vgg16_base,\n    layers.Flatten(),\n    layers.Dropout(0.5),\n    layers.Dense(10, activation='softmax')  # 10 clases de salida\n])\n\n\n# Compilamos y entrenamos los pesos de nuestra última capa\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model.fit(train_images, train_labels_ohc, epochs=5,\n                    batch_size=64, validation_data=(test_images, test_labels_ohc))\n\n\nEpoch 1/5\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 23s 16ms/step - accuracy: 0.5699 - loss: 1.3459 - val_accuracy: 0.8679 - val_loss: 0.5477\n\nEpoch 2/5\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 30s 10ms/step - accuracy: 0.8064 - loss: 0.6443 - val_accuracy: 0.8992 - val_loss: 0.4113\n\nEpoch 3/5\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 10s 10ms/step - accuracy: 0.8276 - loss: 0.5497 - val_accuracy: 0.9120 - val_loss: 0.3532\n\nEpoch 4/5\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 10s 11ms/step - accuracy: 0.8366 - loss: 0.5071 - val_accuracy: 0.9121 - val_loss: 0.3253\n\nEpoch 5/5\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 12s 13ms/step - accuracy: 0.8482 - loss: 0.4773 - val_accuracy: 0.9237 - val_loss: 0.3011\n\n\n\n\n\n# Medimos la precisión del modelo en el conjunto de prueba\ntest_loss, test_acc = model.evaluate(test_images, test_labels_ohc)\n\nprint(f\"Precisión en el conjunto de prueba: {test_acc}\")\n\n\n313/313 ━━━━━━━━━━━━━━━━━━━━ 3s 7ms/step - accuracy: 0.9149 - loss: 0.3224\n\nPrecisión en el conjunto de prueba: 0.9236999750137329\n\n\n\n\n\n# Mostramos el modelo\nmodel.summary()\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ vgg16 (Functional)                   │ (None, 1, 1, 512)           │      14,714,688 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten (Flatten)                    │ (None, 512)                 │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (Dropout)                    │ (None, 512)                 │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (Dense)                        │ (None, 10)                  │           5,130 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 14,730,080 (56.19 MB)\n\n\n\n Trainable params: 5,130 (20.04 KB)\n\n\n\n Non-trainable params: 14,714,688 (56.13 MB)\n\n\n\n Optimizer params: 10,262 (40.09 KB)\n\n\n\n\n# Dibujamos ciertas imágenes con sus predicciones\nplt.figure(figsize=(10, 4))\nfor i in range(10):\n    plt.subplot(2, 5, i + 1)\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(test_images[i], cmap=plt.cm.gray)\n    pred = np.argmax(model.predict(np.expand_dims(test_images[i], axis=0), verbose=False))\n    plt.xlabel(f\"Pred: {pred}\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nObservaciones:\n\nNote que al cargar un modelo pre-entrenado, logramos tener unos pesos que ya saben encontrar ciertos tipos de características dentro de las imágenes. Es por ello que cuando entrenamos nuestra capa superior (10 neuronas), solo hacen falta 5 épocas para alcanzar un accuracy del 92.14% en el conjunto de prueba.\nCabe resaltar que utilizamos un modelo pre-entrenado y agregamos una capa superior para adaptarlo a nuestro problema. Esto se podría considerar transfer learning tambien.\n\nModelos preentrenados\nPara dejar mas claro el concepto de transfer learning lo que haremos es coger el mismo modelo definido anteriormente, solo que esta vez si entrenaremos los pesos del modelo pre-entrenado, para así alcanzar un mejor rendimiento.\n\n# Reiniciar el backend para que las ejecuciones anteriores no interfieran\nK.clear_session()\n\n\n# Definimos el modelo, especificando que queremos entrenar el modelo VGG16\nvgg16_base.trainable = True\nmodel_2 = models.Sequential([\n    vgg16_base,\n    layers.Flatten(),\n    layers.Dropout(0.5),\n    layers.Dense(10, activation='softmax')  # 10 clases de salida\n])\n\n\n# Compilamos y entrenamos los pesos de nuestra última capa\n\nmodel_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model_2.fit(train_images, train_labels_ohc, epochs=5,\n                    batch_size=64, validation_data=(test_images, test_labels_ohc))\n\n\nEpoch 1/5\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 62s 53ms/step - accuracy: 0.1045 - loss: 2.3221 - val_accuracy: 0.1135 - val_loss: 2.3011\n\nEpoch 2/5\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 63s 42ms/step - accuracy: 0.1117 - loss: 2.3013 - val_accuracy: 0.1135 - val_loss: 2.3011\n\nEpoch 3/5\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 41s 42ms/step - accuracy: 0.1110 - loss: 2.3015 - val_accuracy: 0.1135 - val_loss: 2.3011\n\nEpoch 4/5\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 41s 43ms/step - accuracy: 0.1111 - loss: 2.3017 - val_accuracy: 0.1135 - val_loss: 2.3010\n\nEpoch 5/5\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 42s 45ms/step - accuracy: 0.1114 - loss: 2.3014 - val_accuracy: 0.1135 - val_loss: 2.3010\n\n\n\n\n\n# Medimos la precisión del modelo 2 en el conjunto de prueba\ntest_loss, test_acc = model_2.evaluate(test_images, test_labels_ohc)\n\nprint(f\"Precisión en el conjunto de prueba: {test_acc}\")\n\n\n313/313 ━━━━━━━━━━━━━━━━━━━━ 3s 6ms/step - accuracy: 0.1160 - loss: 2.3009\n\nPrecisión en el conjunto de prueba: 0.11349999904632568\n\n\n\n\n\n# Imprimamos la estructura del modelo 2\nmodel_2.summary()\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ vgg16 (Functional)                   │ (None, 1, 1, 512)           │      14,714,688 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten (Flatten)                    │ (None, 512)                 │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (Dropout)                    │ (None, 512)                 │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (Dense)                        │ (None, 10)                  │           5,130 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 44,159,456 (168.45 MB)\n\n\n\n Trainable params: 14,719,818 (56.15 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n Optimizer params: 29,439,638 (112.30 MB)\n\n\n\n\n\nObservaciones:\n\nAntes de hablar del mal rendimiento del modelo (un 9.7% de accuracy en el conjunto de prueba). Hay que hablar de que ahora demoró mas entrenandose. Esto se debe a que ahora, se ajustaron todos los parámeros posibles, no como en el modelo anterior que solo ajustamos los parámetros de la capa superior.\nUna de las razones por las cuales se obtuvo un accuracy muy bajo, es debido a que empezamos a ajustar el modelo pre-entrenado, pero pasamos de tener 512 neuronas como salida del modelo pre-entrenado, a solo tener 10. Entonces ese error se propagó y ajsuto erróneamente los pesos ya entrenados. Lo cual llevó a que el modelo no mejorara.\nPara mitigar este error, utilizaremos fine tunning ajustando mas la capa superior. Así podremos tener un mejor rendimiento de nuestro modelo de transfer learning\n\nFine tunning\n\nPara el ajuste fino, lo que haremos es lo siguiente:\n\nCongelaremos las primeras capas del modelo pre-entrenado\nAgregaremos unas capas superiores al modelo.\nEntrenaremos el modelo así.\nDespués, descongelaremos capas superiores del modelo pre-entrenado y hacemos ese ajuste fino (entrenamos) para aumentar el acierto del modelo.\n\n\n\n# Reiniciar el backend para que las ejecuciones anteriores no interfieran\nK.clear_session()\n\n\nvgg16_base = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n\nfor layer in vgg16_base.layers[:15]:  # Congelar las primeras 15 capas\n    layer.trainable = False\n\n# Agregamos mas neuronas después de nuestro modelo pre-entrenado, para hacer un ajuste mas fino\nmodel_3 = models.Sequential([\n    vgg16_base,\n    layers.Flatten(),\n    layers.Dense(512, activation='relu'),  # Incrementamos el número de unidades para mayor capacidad de representación\n    layers.Dropout(0.5),                   # Aumentamos el Dropout para evitar el sobreajuste\n    layers.Dense(10, activation='softmax') # Capa final con 10 clases\n])\n\n\n# compilamos el modelo y definimos una parada temprana para mitigar el sobreajuste\nmodel_3.compile(optimizer=optimizers.Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\nearly_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\n\n# Entrenamos nuestro modelo\nhistory = model_3.fit(train_images, train_labels_ohc, batch_size=64,\n                      epochs=20,\n                      validation_data=(test_images, test_labels_ohc),\n                      callbacks=[early_stopping])\n\n\nEpoch 1/20\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 22s 20ms/step - accuracy: 0.9012 - loss: 0.3114 - val_accuracy: 0.9808 - val_loss: 0.0569\n\nEpoch 2/20\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 17s 18ms/step - accuracy: 0.9852 - loss: 0.0505 - val_accuracy: 0.9728 - val_loss: 0.0906\n\nEpoch 3/20\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 20s 18ms/step - accuracy: 0.9894 - loss: 0.0353 - val_accuracy: 0.9902 - val_loss: 0.0321\n\nEpoch 4/20\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 19s 17ms/step - accuracy: 0.9910 - loss: 0.0302 - val_accuracy: 0.9902 - val_loss: 0.0332\n\nEpoch 5/20\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 21s 17ms/step - accuracy: 0.9925 - loss: 0.0255 - val_accuracy: 0.9874 - val_loss: 0.0449\n\nEpoch 6/20\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 15s 16ms/step - accuracy: 0.9936 - loss: 0.0217 - val_accuracy: 0.9911 - val_loss: 0.0281\n\nEpoch 7/20\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 20s 16ms/step - accuracy: 0.9946 - loss: 0.0190 - val_accuracy: 0.9896 - val_loss: 0.0363\n\nEpoch 8/20\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 15s 16ms/step - accuracy: 0.9940 - loss: 0.0200 - val_accuracy: 0.9902 - val_loss: 0.0370\n\nEpoch 9/20\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 21s 17ms/step - accuracy: 0.9944 - loss: 0.0195 - val_accuracy: 0.9921 - val_loss: 0.0256\n\nEpoch 10/20\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 17s 18ms/step - accuracy: 0.9956 - loss: 0.0147 - val_accuracy: 0.9905 - val_loss: 0.0364\n\nEpoch 11/20\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 19s 16ms/step - accuracy: 0.9955 - loss: 0.0141 - val_accuracy: 0.9912 - val_loss: 0.0325\n\nEpoch 12/20\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 21s 16ms/step - accuracy: 0.9961 - loss: 0.0131 - val_accuracy: 0.9900 - val_loss: 0.0378\n\nEpoch 13/20\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 17s 18ms/step - accuracy: 0.9959 - loss: 0.0134 - val_accuracy: 0.9917 - val_loss: 0.0350\n\nEpoch 14/20\n\n938/938 ━━━━━━━━━━━━━━━━━━━━ 20s 18ms/step - accuracy: 0.9969 - loss: 0.0110 - val_accuracy: 0.9920 - val_loss: 0.0294\n\n\n\n\n\n# Evaluamos el accuracy del modelo en los datos de prueba\ntest_loss, test_acc = model_3.evaluate(test_images, test_labels_ohc)\nprint(f\"Precisión después del fine-tuning avanzado: {test_acc}\")\n\n\n313/313 ━━━━━━━━━━━━━━━━━━━━ 3s 7ms/step - accuracy: 0.9882 - loss: 0.0374\n\nPrecisión después del fine-tuning avanzado: 0.9921000003814697"
  },
  {
    "objectID": "semana_3/notebooks/Nb_3a_Introduccion_transfer_learning_finetuning.html#transfer-learning-y-fine-tuning-en-tareas-de-nlp",
    "href": "semana_3/notebooks/Nb_3a_Introduccion_transfer_learning_finetuning.html#transfer-learning-y-fine-tuning-en-tareas-de-nlp",
    "title": "Introducción a pre-trained models, transfer learning and fine tuning",
    "section": "Transfer learning y fine tuning en tareas de NLP",
    "text": "Transfer learning y fine tuning en tareas de NLP\n\n#@title Descargar vectores embebidos en inglés y español\n\n# Descargar los vectores FastText de inglés y español\n!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.vec.gz\n\n# Descomprimir los archivos\n!gunzip cc.en.300.vec.gz\n!gunzip cc.es.300.vec.gz\n\n--2024-10-18 13:11:42--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\nResolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.96, 3.163.189.108, 3.163.189.51, ...\nConnecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.96|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1325960915 (1.2G) [binary/octet-stream]\nSaving to: ‘cc.en.300.vec.gz’\n\ncc.en.300.vec.gz    100%[===================&gt;]   1.23G  85.8MB/s    in 8.9s    \n\n2024-10-18 13:11:51 (142 MB/s) - ‘cc.en.300.vec.gz’ saved [1325960915/1325960915]\n\n--2024-10-18 13:11:51--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.vec.gz\nResolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.96, 3.163.189.108, 3.163.189.51, ...\nConnecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.96|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1285580896 (1.2G) [binary/octet-stream]\nSaving to: ‘cc.es.300.vec.gz’\n\ncc.es.300.vec.gz    100%[===================&gt;]   1.20G  28.5MB/s    in 45s     \n\n2024-10-18 13:12:36 (27.3 MB/s) - ‘cc.es.300.vec.gz’ saved [1285580896/1285580896]\n\n\n\n\n# Cargar los embeddings preentrenados de FastText (esto puede tardar un poco)\nembedding_en = KeyedVectors.load_word2vec_format('cc.en.300.vec', binary=False)\n\n# Probar cargando una palabra\nprint(embedding_en['hello'])\n\n[ 1.576e-01  4.380e-02 -4.500e-03  6.660e-02  7.700e-02  4.900e-03\n  8.200e-03  6.500e-03  9.300e-03  3.540e-02 -2.310e-02 -4.920e-02\n -8.330e-02  1.560e-02  2.549e-01  3.450e-02 -1.070e-02 -7.800e-02\n -7.080e-02  7.620e-02 -6.100e-02  4.490e-02 -7.300e-02  1.310e-02\n  3.150e-02 -3.100e-02  1.660e-02  1.740e-02 -7.360e-02  1.182e-01\n -1.213e-01 -4.090e-02  2.940e-02  4.840e-02 -1.340e-02 -1.750e-02\n  7.510e-02  9.970e-02 -4.000e-02  4.100e-03 -7.220e-02 -4.430e-02\n -1.200e-03  7.570e-02  3.980e-02  3.230e-02  1.960e-02  4.680e-02\n -1.460e-02  1.130e-01  3.150e-02 -1.023e-01  1.581e-01 -2.760e-02\n -3.400e-02 -1.770e-02 -6.000e-04  1.108e-01 -1.650e-02 -3.100e-03\n -4.230e-02  1.114e-01 -5.310e-02  4.910e-02  9.100e-02  6.570e-02\n -3.710e-02  3.820e-02  7.250e-02 -5.320e-02  3.060e-02 -5.770e-02\n -8.070e-02 -9.060e-02 -8.050e-02 -6.030e-02 -9.730e-02  4.830e-02\n  6.800e-02 -2.600e-03 -8.600e-03 -5.100e-03  3.160e-02  6.670e-02\n  3.000e-04 -8.350e-02  4.450e-02  3.600e-02 -2.070e-02 -6.210e-02\n -9.080e-02 -4.880e-02  1.328e-01  1.260e-02  4.610e-02 -5.540e-02\n  2.300e-03  4.920e-02  3.360e-02  6.640e-02 -8.930e-02 -5.370e-02\n  1.322e-01 -9.100e-03  3.300e-03 -4.370e-02  7.520e-02 -4.370e-02\n -3.930e-02  4.900e-02  8.060e-02 -3.940e-02 -7.600e-02  7.170e-02\n -1.890e-02 -4.210e-02  3.300e-03 -2.140e-02 -1.301e-01  1.370e-02\n -5.150e-02  3.870e-02  4.930e-02 -6.180e-02 -3.400e-02  3.520e-02\n  2.590e-02 -1.028e-01  6.010e-02 -7.140e-02 -2.240e-02 -1.034e-01\n -6.350e-02  1.200e-03 -8.400e-03 -7.100e-02 -1.390e-02  9.300e-02\n -7.620e-02 -1.800e-01  4.980e-02  5.600e-02  4.370e-02  1.690e-02\n -3.520e-02  5.500e-03 -1.517e-01  8.300e-03  1.339e-01  1.184e-01\n -2.550e-02 -5.900e-02 -1.155e-01 -9.120e-02 -3.260e-02  9.600e-03\n  7.080e-02 -1.196e-01 -2.450e-02  4.670e-02 -1.058e-01  8.400e-03\n -3.590e-02 -7.120e-02  1.491e-01 -9.410e-02  3.880e-02  4.800e-02\n  2.000e-02  5.700e-02 -5.090e-02 -1.550e-02 -3.210e-02  6.400e-02\n  4.460e-02 -5.420e-02  2.390e-02  3.990e-02  4.950e-02 -8.130e-02\n  8.680e-02  2.790e-02  2.230e-02  6.880e-02  5.800e-02  1.240e-02\n  9.180e-02  1.700e-02 -2.210e-02 -5.550e-02  3.200e-03 -8.950e-02\n -6.000e-04 -4.810e-02 -4.110e-02 -3.470e-02 -4.230e-02  1.011e-01\n  4.350e-02  6.750e-02 -7.330e-02  2.330e-02  3.770e-02  9.000e-03\n -8.250e-02 -9.680e-02  5.900e-03  2.620e-02 -2.230e-02  7.390e-02\n -1.900e-03 -9.780e-02 -5.380e-02 -4.770e-02 -1.300e-02  8.000e-04\n  2.900e-02 -3.100e-03 -9.290e-02  6.740e-02 -1.855e-01  4.010e-02\n -5.630e-02  6.190e-02  8.940e-02 -6.910e-02 -3.220e-02 -1.354e-01\n -7.460e-02  1.015e-01 -2.700e-03  6.070e-02  2.430e-02 -1.519e-01\n -2.940e-02 -4.200e-03  5.160e-02  1.860e-01 -2.560e-02  8.120e-02\n  3.200e-03 -3.360e-02  3.900e-02 -7.380e-02  1.146e-01 -1.000e-04\n -3.690e-02  9.310e-02 -2.930e-02  5.210e-02  8.000e-03 -2.930e-02\n  1.312e-01 -8.320e-02 -3.400e-02  1.213e-01  3.510e-02  4.200e-03\n  5.030e-02  2.060e-02  7.900e-02 -4.950e-02  2.540e-02 -2.960e-02\n -2.650e-02  5.430e-02 -5.530e-02  1.070e-02 -3.000e-02 -6.050e-02\n  8.540e-02 -6.660e-02 -6.780e-02  3.520e-02  6.200e-02  4.810e-02\n -3.450e-02 -2.870e-02 -5.910e-02 -5.100e-03 -9.740e-02  1.900e-03\n -9.060e-02  1.480e-02 -9.780e-02  3.960e-02  2.830e-02 -9.280e-02\n -8.200e-03 -4.570e-02  1.123e-01  8.600e-02 -1.475e-01  8.330e-02\n  9.950e-02 -3.670e-02  6.850e-02  8.070e-02 -4.500e-02 -3.110e-02]\n\n\n\nMismo ejercicio Clasificación de texto usando RNN\n\ndataset, info = tfds.load('imdb_reviews', with_info=True,\n                          as_supervised=True)\ntrain_dataset, test_dataset = dataset['train'], dataset['test']\n\nDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\n\n\n\nBUFFER_SIZE = 10000\nBATCH_SIZE = 32\n\n# optimización para train\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\n# optimización para test\ntest_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\n\nVOCAB_SIZE = 1000\nencoder = tf.keras.layers.TextVectorization(\n    max_tokens=VOCAB_SIZE)\n\n# Crea la capa y pasa el texto del conjunto de datos al método .adapt de la capa\nencoder.adapt(train_dataset.map(lambda text, label: text))\n\n\n# Obtener el vocabulario del encoder\nvocab = encoder.get_vocabulary()\n\nprint(vocab[:10])\n\n# Dimensiones de los embeddings preentrenados\nembedding_dim = 300  # Dimensión de los embeddings de FastText\n\n# Crear una matriz de embeddings aleatoria (en caso de que alguna palabra no esté en FastText)\nembedding_matrix = np.random.uniform(-0.25, 0.25, (len(vocab), embedding_dim))\n\n# Llenar la matriz con los vectores de FastText para las palabras del vocabulario\nfor i, word in enumerate(vocab):\n    if word in embedding_en:\n        embedding_matrix[i] = embedding_en[word]\n    else:\n        # Dejar la fila con los valores aleatorios (o también puedes poner ceros)\n        embedding_matrix[i] = np.random.uniform(-0.25, 0.25, embedding_dim)\n\n['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it']\n\n\n\nembedding_matrix.shape\n\n(1000, 300)\n\n\n\ndef rnn(pretrained_vector_matrix):\n    # Ahora utilizamos la API funcional de Keras\n    inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)  # El input será una cadena de texto\n    x = encoder(inputs)  # Aplicamos el encoder\n\n    x = tf.keras.layers.Embedding(\n        input_dim=len(encoder.get_vocabulary()),  # Tamaño del vocabulario\n        output_dim=pretrained_vector_matrix.shape[1],  # Dimensión de los embeddings (300)\n        embeddings_initializer=tf.keras.initializers.Constant(pretrained_vector_matrix),\n        trainable=True,  # Congelar los embeddings\n        mask_zero=True)(x)  # Capa de Embedding\n\n    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False), merge_mode='concat')(x)  # Capa LSTM Bidireccional\n\n    x = tf.keras.layers.Dense(64, activation='relu')(x)  # Capa densa\n    outputs = tf.keras.layers.Dense(1)(x)  # Capa de salida\n\n    # Definimos el modelo\n    model = tf.keras.Model(inputs, outputs)\n\n    return model\n\n# crear la red RNN\nmodel = rnn(embedding_matrix)\n\n# Compilamos el modelo\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              optimizer=tf.keras.optimizers.Adam(1e-4),\n              metrics=['accuracy'])\n\n# Verificamos la estructura del modelo\nmodel.summary()\n\nModel: \"functional_1\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)              ┃ Output Shape           ┃        Param # ┃ Connected to           ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_1             │ (None, 1)              │              0 │ -                      │\n│ (InputLayer)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text_vectorization        │ (None, None)           │              0 │ input_layer_1[0][0]    │\n│ (TextVectorization)       │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ embedding_1 (Embedding)   │ (None, None, 300)      │        300,000 │ text_vectorization[1]… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ not_equal_1 (NotEqual)    │ (None, None)           │              0 │ text_vectorization[1]… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ bidirectional_1           │ (None, 128)            │        186,880 │ embedding_1[0][0],     │\n│ (Bidirectional)           │                        │                │ not_equal_1[0][0]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_2 (Dense)           │ (None, 64)             │          8,256 │ bidirectional_1[0][0]  │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_3 (Dense)           │ (None, 1)              │             65 │ dense_2[0][0]          │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n\n\n\n Total params: 495,201 (1.89 MB)\n\n\n\n Trainable params: 495,201 (1.89 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# hacer una prueba sin usar padding\n# El texto crudo que quieres predecir\nsample_text = ('The movie was cool. The animation and the graphics were out of this world.')\n\n# No es necesario hacer la vectorización manual aquí, simplemente pasa el texto crudo al modelo\npredictions = model.predict(tf.constant([sample_text]))\n\n# Imprime la predicción\nprint(predictions[0])\n\n\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 379ms/step\n\n[-0.06903996]\n\n\n\n\n\nhistory = model.fit(train_dataset, epochs=10,\n                    validation_data=test_dataset,\n                    validation_steps=30)\n\n\nEpoch 1/10\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 43s 52ms/step - accuracy: 0.5433 - loss: 0.6593 - val_accuracy: 0.8250 - val_loss: 0.4475\n\nEpoch 2/10\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 42s 54ms/step - accuracy: 0.8240 - loss: 0.3888 - val_accuracy: 0.8573 - val_loss: 0.3350\n\nEpoch 3/10\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 80s 52ms/step - accuracy: 0.8420 - loss: 0.3497 - val_accuracy: 0.8635 - val_loss: 0.3453\n\nEpoch 4/10\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 40s 50ms/step - accuracy: 0.8608 - loss: 0.3217 - val_accuracy: 0.8365 - val_loss: 0.3172\n\nEpoch 5/10\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 39s 50ms/step - accuracy: 0.8639 - loss: 0.3106 - val_accuracy: 0.8656 - val_loss: 0.3094\n\nEpoch 6/10\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 40s 50ms/step - accuracy: 0.8689 - loss: 0.2969 - val_accuracy: 0.8844 - val_loss: 0.2817\n\nEpoch 7/10\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 39s 50ms/step - accuracy: 0.8743 - loss: 0.2935 - val_accuracy: 0.8615 - val_loss: 0.3293\n\nEpoch 8/10\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 41s 50ms/step - accuracy: 0.8788 - loss: 0.2812 - val_accuracy: 0.8427 - val_loss: 0.3435\n\nEpoch 9/10\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 39s 50ms/step - accuracy: 0.8821 - loss: 0.2770 - val_accuracy: 0.8396 - val_loss: 0.3193\n\nEpoch 10/10\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 42s 51ms/step - accuracy: 0.8780 - loss: 0.2756 - val_accuracy: 0.8656 - val_loss: 0.3164\n\n\n\n\n\ntest_loss, test_acc = model.evaluate(test_dataset)\n\nprint('Test Loss:', test_loss)\nprint('Test Accuracy:', test_acc)\n\n\n782/782 ━━━━━━━━━━━━━━━━━━━━ 19s 24ms/step - accuracy: 0.8578 - loss: 0.3114\n\nTest Loss: 0.311646044254303\n\nTest Accuracy: 0.8579199910163879\n\n\n\n\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplot_graphs(history, 'accuracy')\nplt.ylim(None, 1)\nplt.subplot(1, 2, 2)\nplot_graphs(history, 'loss')\nplt.ylim(0, None)"
  },
  {
    "objectID": "semana_3/notebooks/Nb_3a_Introduccion_transfer_learning_finetuning.html#conclusiones",
    "href": "semana_3/notebooks/Nb_3a_Introduccion_transfer_learning_finetuning.html#conclusiones",
    "title": "Introducción a pre-trained models, transfer learning and fine tuning",
    "section": "Conclusiones",
    "text": "Conclusiones\n\nNote que al agregarle al modelo mas capas superiores, logramos mitigar el problema que presentamos en nuestro modelo 2, logrando un accuracy del 99.30% en nuestro datos de prueba.\nComo conlusión, cuando hagamos uso de modelo pre-entrenados. Tenemos que hacer uso de todas las herramientas que disponemos, como lo son el transfer learning y el fine tunning, una caracteristica muy importante que siempre hay que aplicar.\nEn el modelo de NLP se logró una ligera mejora, sin embargo es necesario hacer más cambios en los parámetros para llegar a mejores resultados."
  }
]